<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>NLP入门（六）pyltp的介绍与使用</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AD%EF%BC%89pyltp%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AD%EF%BC%89pyltp%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h3 id="pyltp的简介">pyltp的简介</h3><p>语言技术平台(LTP)经过哈工大社会计算与信息检索研究中心 11年的持续研发和推广，是国内外最具影响力的中文处理基础平台。它提供的功能包括中文分词、词性标注、命名实体识别、依存句法分析、语义角色标注等。</p><figure><img src="/img/nlp6_1.png" alt="语言技术平台架构" /><figcaption aria-hidden="true">语言技术平台架构</figcaption></figure><p>pyltp 是 LTP 的 Python封装，同时支持Python2和Python3版本。Python3的安装方法为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip3 install pyltp<br></code></pre></td></tr></table></figure><ul><li><p>官网下载网址：https://pypi.org/project/pyltp/0.1.7/</p></li><li><p>官方使用说明文档：https://pyltp.readthedocs.io/zh_CN/develop/api.html</p><p>在使用该模块前，需要下载完整的模型文件，文件下载地址为：<ahref="https://pan.baidu.com/share/link?shareid=1988562907&amp;uk=2738088569#list/path=%2F">https://pan.baidu.com/share/link?shareid=1988562907&amp;uk=2738088569#list/path=%2F</a>。pyltp 的所有输入的分析文本和输出的结果的编码均为UTF-8。模型的数据文件如下：</p></li></ul><figure><img src="/img/nlp6_2.png" alt="模型数据" /><figcaption aria-hidden="true">模型数据</figcaption></figure><p>其中，cws.model用于分词模型，lexicon.txt为分词时添加的用户字典，ner.model为命名实体识别模型，parser.model为依存句法分析模型，pisrl.model为语义角色标注模型，pos为词性标注模型。</p><h3 id="pyltp的使用">pyltp的使用</h3><p>pyltp的使用示例项目结构如下：</p><figure><img src="/img/nlp6_3.png" alt="示例项目" /><figcaption aria-hidden="true">示例项目</figcaption></figure><h5 id="分句">分句</h5><p>分句指的是将一段话或一片文章中的文字按句子分开，按句子形成独立的单元。示例的Python代码sentenct_split.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> SentenceSplitter<br><br><span class="hljs-comment"># 分句</span><br>doc = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span> \<br>      <span class="hljs-string">&#x27;盖茨原计划从明年1月9日至14日陆续访问中国和日本，目前，他决定在行程中增加对韩国的访问。莫莱尔表示，&#x27;</span> \<br>      <span class="hljs-string">&#x27;盖茨在访韩期间将会晤韩国国防部长官金宽镇，就朝鲜近日的行动交换意见，同时商讨加强韩美两军同盟关系等问题，&#x27;</span> \<br>      <span class="hljs-string">&#x27;拟定共同应对朝鲜挑衅和核计划的方案。&#x27;</span><br>sents = SentenceSplitter.split(doc)  <span class="hljs-comment"># 分句</span><br><br><br><span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>    <span class="hljs-built_in">print</span>(sent)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs dns">据韩联社<span class="hljs-number">12</span>月<span class="hljs-number">28</span>日反映，美国防部发言人杰夫·莫莱尔<span class="hljs-number">27</span>日表示，美国防部长盖茨将于<span class="hljs-number">2011年1月14</span>日访问韩国。<br>盖茨原计划从明年<span class="hljs-number">1</span>月<span class="hljs-number">9</span>日至<span class="hljs-number">14</span>日陆续访问中国和日本，目前，他决定在行程中增加对韩国的访问。<br>莫莱尔表示，盖茨在访韩期间将会晤韩国国防部长官金宽镇，就朝鲜近日的行动交换意见，同时商讨加强韩美两军同盟关系等问题，拟定共同应对朝鲜挑衅和核计划的方案。<br></code></pre></td></tr></table></figure><h5 id="分词">分词</h5><p>分词指的是将一句话按词语分开，按词语形成独立的单元。示例的Python代码words_split.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor<br><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;/&#x27;</span>.join(words))<br><br>segmentor.release()<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">据<span class="hljs-regexp">/韩联社/</span><span class="hljs-number">12</span>月<span class="hljs-regexp">/28日/</span>反映<span class="hljs-regexp">/，/</span>美<span class="hljs-regexp">/国防部/</span>发言人<span class="hljs-regexp">/杰夫·莫莱尔/</span><span class="hljs-number">27</span>日<span class="hljs-regexp">/表示/</span>，<span class="hljs-regexp">/美/</span>国防部长<span class="hljs-regexp">/盖茨/</span>将<span class="hljs-regexp">/于/</span><span class="hljs-number">2011</span>年<span class="hljs-regexp">/1月/</span><span class="hljs-number">14</span>日<span class="hljs-regexp">/访问/</span>韩国/。<br></code></pre></td></tr></table></figure><h5 id="词性标注">词性标注</h5><p>词性标注指的是一句话分完词后，制定每个词语的词性。示例的Python代码postagger.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor, Postagger<br><br><span class="hljs-comment"># 分词</span><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br><br>postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br>postags = postagger.postag(words)  <span class="hljs-comment"># 词性标注</span><br><br><span class="hljs-keyword">for</span> word, postag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(words, postags):<br>    <span class="hljs-built_in">print</span>(word, postag)<br><br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">词性标注结果说明</span><br><span class="hljs-string">https://ltp.readthedocs.io/zh_CN/latest/appendix.html#id3</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs excel">据 p<br>韩联社 ni<br><span class="hljs-number">12</span>月 nt<br><span class="hljs-number">28</span>日 nt<br>反映 v<br>， wp<br>美 j<br>国防部 <span class="hljs-built_in">n</span><br>发言人 <span class="hljs-built_in">n</span><br>杰夫·莫莱尔 nh<br><span class="hljs-number">27</span>日 nt<br>表示 v<br>， wp<br>美 j<br>国防部长 <span class="hljs-built_in">n</span><br>盖茨 nh<br>将 d<br>于 p<br><span class="hljs-number">2011</span>年 nt<br><span class="hljs-number">1</span>月 nt<br><span class="hljs-number">14</span>日 nt<br>访问 v<br>韩国 ns<br>。 wp<br></code></pre></td></tr></table></figure><p>词性标注结果可参考网址：<ahref="https://ltp.readthedocs.io/zh_CN/latest/appendix.html">https://ltp.readthedocs.io/zh_CN/latest/appendix.html</a>。</p><h5 id="命名实体识别">命名实体识别</h5><p>命名实体识别（NER）指的是识别出一句话或一段话或一片文章中的命名实体，比如人名，地名，组织机构名。示例的Python代码ner.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor, Postagger<br><br><span class="hljs-comment"># 分词</span><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br><br>postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br>postags = postagger.postag(words)  <span class="hljs-comment"># 词性标注</span><br><br><br>ner_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/ner.model&#x27;</span>)   <span class="hljs-comment"># 命名实体识别模型路径，模型名称为`pos.model`</span><br><br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> NamedEntityRecognizer<br>recognizer = NamedEntityRecognizer() <span class="hljs-comment"># 初始化实例</span><br>recognizer.load(ner_model_path)  <span class="hljs-comment"># 加载模型</span><br><span class="hljs-comment"># netags = recognizer.recognize(words, postags)  # 命名实体识别</span><br><br><br><span class="hljs-comment"># 提取识别结果中的人名，地名，组织机构名</span><br><br>persons, places, orgs = <span class="hljs-built_in">set</span>(), <span class="hljs-built_in">set</span>(), <span class="hljs-built_in">set</span>()<br><br><br>netags = <span class="hljs-built_in">list</span>(recognizer.recognize(words, postags))  <span class="hljs-comment"># 命名实体识别</span><br><span class="hljs-built_in">print</span>(netags)<br><span class="hljs-comment"># print(netags)</span><br>i = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> tag, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(netags, words):<br>    j = i<br>    <span class="hljs-comment"># 人名</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;Nh&#x27;</span> <span class="hljs-keyword">in</span> tag:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;S&#x27;</span>):<br>            persons.add(word)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>            union_person = word<br>            <span class="hljs-keyword">while</span> netags[j] != <span class="hljs-string">&#x27;E-Nh&#x27;</span>:<br>                j += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> j &lt; <span class="hljs-built_in">len</span>(words):<br>                    union_person += words[j]<br>            persons.add(union_person)<br>    <span class="hljs-comment"># 地名</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;Ns&#x27;</span> <span class="hljs-keyword">in</span> tag:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;S&#x27;</span>):<br>            places.add(word)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>            union_place = word<br>            <span class="hljs-keyword">while</span> netags[j] != <span class="hljs-string">&#x27;E-Ns&#x27;</span>:<br>                j += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> j &lt; <span class="hljs-built_in">len</span>(words):<br>                    union_place += words[j]<br>            places.add(union_place)<br>    <span class="hljs-comment"># 机构名</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;Ni&#x27;</span> <span class="hljs-keyword">in</span> tag:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;S&#x27;</span>):<br>            orgs.add(word)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>            union_org = word<br>            <span class="hljs-keyword">while</span> netags[j] != <span class="hljs-string">&#x27;E-Ni&#x27;</span>:<br>                j += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> j &lt; <span class="hljs-built_in">len</span>(words):<br>                    union_org += words[j]<br>            orgs.add(union_org)<br><br>    i += <span class="hljs-number">1</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;人名：&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>.join(persons))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;地名：&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>.join(places))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;组织机构：&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>.join(orgs))<br><br><br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br>recognizer.release()<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Ni&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-Ni&#x27;</span>, <span class="hljs-string">&#x27;E-Ni&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Nh&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Ns&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Nh&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Ns&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>人名： 杰夫·莫莱尔，盖茨<br>地名： 美，韩国<br>组织机构： 韩联社，美国防部<br></code></pre></td></tr></table></figure><p>命名实体识别结果可参考网址：<ahref="https://ltp.readthedocs.io/zh_CN/latest/appendix.html">https://ltp.readthedocs.io/zh_CN/latest/appendix.html</a>。</p><h4 id="依存句法分析">依存句法分析</h4><p>依存语法 (Dependency Parsing, DP)通过分析语言单位内成分之间的依存关系揭示其句法结构。直观来讲，依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关系。示例的Python代码parser.py代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor, Postagger, Parser<br><br><span class="hljs-comment"># 分词</span><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br><br>postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br>postags = postagger.postag(words)  <span class="hljs-comment"># 词性标注</span><br><br><br><span class="hljs-comment"># 依存句法分析</span><br>par_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/parser.model&#x27;</span>)  <span class="hljs-comment"># 模型路径，模型名称为`parser.model`</span><br><br>parser = Parser() <span class="hljs-comment"># 初始化实例</span><br>parser.load(par_model_path)  <span class="hljs-comment"># 加载模型</span><br>arcs = parser.parse(words, postags)  <span class="hljs-comment"># 句法分析</span><br><br>rely_id = [arc.head <span class="hljs-keyword">for</span> arc <span class="hljs-keyword">in</span> arcs]  <span class="hljs-comment"># 提取依存父节点id</span><br>relation = [arc.relation <span class="hljs-keyword">for</span> arc <span class="hljs-keyword">in</span> arcs]  <span class="hljs-comment"># 提取依存关系</span><br>heads = [<span class="hljs-string">&#x27;Root&#x27;</span> <span class="hljs-keyword">if</span> <span class="hljs-built_in">id</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> words[<span class="hljs-built_in">id</span>-<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> rely_id]  <span class="hljs-comment"># 匹配依存父节点词语</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(words)):<br>    <span class="hljs-built_in">print</span>(relation[i] + <span class="hljs-string">&#x27;(&#x27;</span> + words[i] + <span class="hljs-string">&#x27;, &#x27;</span> + heads[i] + <span class="hljs-string">&#x27;)&#x27;</span>)<br><br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br>parser.release()<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(据, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">SBV</span><span class="hljs-params">(韩联社, 反映)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">12</span>月, <span class="hljs-number">28</span>日)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(<span class="hljs-number">28</span>日, 反映)</span></span><br><span class="hljs-function"><span class="hljs-title">POB</span><span class="hljs-params">(反映, 据)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(，, 据)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(美, 国防部)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(国防部, 发言人)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(发言人, 杰夫·莫莱尔)</span></span><br><span class="hljs-function"><span class="hljs-title">SBV</span><span class="hljs-params">(杰夫·莫莱尔, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(<span class="hljs-number">27</span>日, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">HED</span><span class="hljs-params">(表示, Root)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(，, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(美, 国防部长)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(国防部长, 盖茨)</span></span><br><span class="hljs-function"><span class="hljs-title">SBV</span><span class="hljs-params">(盖茨, 访问)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(将, 访问)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(于, 访问)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">2011</span>年, <span class="hljs-number">14</span>日)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">1</span>月, <span class="hljs-number">14</span>日)</span></span><br><span class="hljs-function"><span class="hljs-title">POB</span><span class="hljs-params">(<span class="hljs-number">14</span>日, 于)</span></span><br><span class="hljs-function"><span class="hljs-title">VOB</span><span class="hljs-params">(访问, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">VOB</span><span class="hljs-params">(韩国, 访问)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(。, 表示)</span></span><br></code></pre></td></tr></table></figure><p>依存句法分析结果可参考网址：<ahref="https://ltp.readthedocs.io/zh_CN/latest/appendix.html">https://ltp.readthedocs.io/zh_CN/latest/appendix.html</a>。</p><h5 id="语义角色标注">语义角色标注</h5><p>语义角色标注是实现浅层语义分析的一种方式。在一个句子中，谓词是对主语的陈述或说明，指出“做什么”、“是什么”或“怎么样，代表了一个事件的核心，跟谓词搭配的名词称为论元。语义角色是指论元在动词所指事件中担任的角色。主要有：施事者（Agent）、受事者（Patient）、客体（Theme）、经验者（Experiencer）、受益者（Beneficiary）、工具（Instrument）、处所（Location）、目标（Goal）和来源（Source）等。示例的Python代码rolelabel.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor, Postagger, Parser, SementicRoleLabeller<br><br><span class="hljs-comment"># 分词</span><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br><br>postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br>postags = postagger.postag(words)  <span class="hljs-comment"># 词性标注</span><br><br><span class="hljs-comment"># 依存句法分析</span><br>par_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/parser.model&#x27;</span>)  <span class="hljs-comment"># 模型路径，模型名称为`parser.model`</span><br><br>parser = Parser() <span class="hljs-comment"># 初始化实例</span><br>parser.load(par_model_path)  <span class="hljs-comment"># 加载模型</span><br>arcs = parser.parse(words, postags)  <span class="hljs-comment"># 句法分析</span><br><br><span class="hljs-comment"># 语义角色标注</span><br>srl_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pisrl.model&#x27;</span>)  <span class="hljs-comment"># 语义角色标注模型目录路径</span><br>labeller = SementicRoleLabeller() <span class="hljs-comment"># 初始化实例</span><br>labeller.load(srl_model_path)  <span class="hljs-comment"># 加载模型</span><br>roles = labeller.label(words, postags, arcs)  <span class="hljs-comment"># 语义角色标注</span><br><br><span class="hljs-comment"># 打印结果</span><br><span class="hljs-keyword">for</span> role <span class="hljs-keyword">in</span> roles:<br>    <span class="hljs-built_in">print</span>(words[role.index], end=<span class="hljs-string">&#x27; &#x27;</span>)<br>    <span class="hljs-built_in">print</span>(role.index, <span class="hljs-string">&quot;&quot;</span>.join([<span class="hljs-string">&quot;%s:(%d,%d)&quot;</span> % (arg.name, arg.<span class="hljs-built_in">range</span>.start, arg.<span class="hljs-built_in">range</span>.end) <span class="hljs-keyword">for</span> arg <span class="hljs-keyword">in</span> role.arguments]))<br><br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br>parser.release()<br>labeller.release()<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">反映 <span class="hljs-number">4</span> <span class="hljs-built_in">A0</span>:(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<span class="hljs-built_in">A0</span>:(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<br>表示 <span class="hljs-number">11</span> MNR:(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>)<span class="hljs-built_in">A0</span>:(<span class="hljs-number">6</span>,<span class="hljs-number">9</span>)TMP:(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>)<span class="hljs-built_in">A1</span>:(<span class="hljs-number">13</span>,<span class="hljs-number">22</span>)<br>访问 <span class="hljs-number">21</span> <span class="hljs-built_in">A0</span>:(<span class="hljs-number">13</span>,<span class="hljs-number">15</span>)ADV:(<span class="hljs-number">16</span>,<span class="hljs-number">16</span>)TMP:(<span class="hljs-number">17</span>,<span class="hljs-number">20</span>)<span class="hljs-built_in">A1</span>:(<span class="hljs-number">22</span>,<span class="hljs-number">22</span>)<br></code></pre></td></tr></table></figure><h3 id="总结">总结</h3><p>本文介绍了中文NLP的一个杰出工具pyltp，并给出了该模块的各个功能的一个示例，希望能给读者一些思考与启示。本文到此结束，感谢大家阅读~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>pyltp</tag>
      
      <tag>NLP工具</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（五）用深度学习实现命名实体识别（NER）</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%94%EF%BC%89%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%94%EF%BC%89%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>在文章：<ahref="https://percent4.github.io/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/">NLP入门（四）命名实体识别（NER）</a>中，笔者介绍了两个实现命名实体识别的工具——NLTK和StanfordNLP。在本文中，我们将会学习到如何使用深度学习工具来自己一步步地实现NER，只要你坚持看完，就一定会很有收获的。OK，话不多说，让我们进入正题。几乎所有的NLP都依赖一个强大的语料库，本项目实现NER的语料库如下(文件名为train.txt，一共42000行，这里只展示前15行，可以在文章最后的Github地址下载该语料库)：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">playedonMonday(hometeam<span class="hljs-keyword">in</span>CAPS):<br>VBDINNNP(NNNNINNNP):<br>OOOOOOOOOO<br>AmericanLeague<br>NNPNNP<br>B-MISCI-MISC<br>Cleveland2DETROIT1<br>NNPCDNNPCD<br>B-ORGOB-ORGO<br>BALTIMORE12Oakland11(10innings)<br>VBCDNNPCD(CDNN)<br>B-ORGOB-ORGOOOOO<br>TORONTO5Minnesota3<br>TOCDNNPCD<br>B-ORGOB-ORGO<br>......<br></code></pre></td></tr></table></figure><p>简单介绍下该语料库的结构：该语料库一共42000行，每三行为一组，其中，第一行为英语句子，第二行为每个句子的词性（关于英语单词的词性，可参考文章：<ahref="https://www.jianshu.com/p/79255fe0c5b5">NLP入门（三）词形还原（Lemmatization）</a>），第三行为NER系统的标注，具体的含义会在之后介绍。我们的NER项目的名称为DL_4_NER，结构如下：</p><figure><img src="/img/nlp5_1.png" alt="NER项目名称" /><figcaption aria-hidden="true">NER项目名称</figcaption></figure><p>项目中每个文件的功能如下：</p><ul><li><p>utils.py: 项目配置及数据导入</p></li><li><p>data_processing.py: 数据探索</p></li><li><p>Bi_LSTM_Model_training.py: 模型创建及训练</p></li><li><p>Bi_LSTM_Model_predict.py: 对新句子进行NER预测</p><p>接下来，笔者将结合代码文件，分部介绍该项目的步骤，当所有步骤介绍完毕后，我们的项目就结束了，而你，也就知道了如何用深度学习实现命名实体识别（NER）。Let's begin!</p></li></ul><h3 id="项目配置">项目配置</h3><p>第一步，是项目的配置及数据导入，在utils.py文件中实现，完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><span class="hljs-comment"># basic settings for DL_4_NER Project</span><br>BASE_DIR = <span class="hljs-string">&quot;F://NERSystem&quot;</span><br>CORPUS_PATH = <span class="hljs-string">&quot;%s/train.txt&quot;</span> % BASE_DIR<br><br>KERAS_MODEL_SAVE_PATH = <span class="hljs-string">&#x27;%s/Bi-LSTM-4-NER.h5&#x27;</span> % BASE_DIR<br>WORD_DICTIONARY_PATH = <span class="hljs-string">&#x27;%s/word_dictionary.pk&#x27;</span> % BASE_DIR<br>InVERSE_WORD_DICTIONARY_PATH = <span class="hljs-string">&#x27;%s/inverse_word_dictionary.pk&#x27;</span> % BASE_DIR<br>LABEL_DICTIONARY_PATH = <span class="hljs-string">&#x27;%s/label_dictionary.pk&#x27;</span> % BASE_DIR<br>OUTPUT_DICTIONARY_PATH = <span class="hljs-string">&#x27;%s/output_dictionary.pk&#x27;</span> % BASE_DIR<br><br>CONSTANTS = [<br>             KERAS_MODEL_SAVE_PATH,<br>             InVERSE_WORD_DICTIONARY_PATH,<br>             WORD_DICTIONARY_PATH,<br>             LABEL_DICTIONARY_PATH,<br>             OUTPUT_DICTIONARY_PATH<br>             ]<br><br><span class="hljs-comment"># load data from corpus to from pandas DataFrame</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>():<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CORPUS_PATH, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        text_data = [text.strip() <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> f.readlines()]<br>    text_data = [text_data[k].split(<span class="hljs-string">&#x27;\t&#x27;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(text_data))]<br>    index = <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(text_data), <span class="hljs-number">3</span>)<br><br>    <span class="hljs-comment"># Transforming data to matrix format for neural network</span><br>    input_data = <span class="hljs-built_in">list</span>()<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(index) - <span class="hljs-number">1</span>):<br>        rows = text_data[index[i-<span class="hljs-number">1</span>]:index[i]]<br>        sentence_no = np.array([i]*<span class="hljs-built_in">len</span>(rows[<span class="hljs-number">0</span>]), dtype=<span class="hljs-built_in">str</span>)<br>        rows.append(sentence_no)<br>        rows = np.array(rows).T<br>        input_data.append(rows)<br><br>    input_data = pd.DataFrame(np.concatenate([item <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> input_data]),\<br>                               columns=[<span class="hljs-string">&#x27;word&#x27;</span>, <span class="hljs-string">&#x27;pos&#x27;</span>, <span class="hljs-string">&#x27;tag&#x27;</span>, <span class="hljs-string">&#x27;sent_no&#x27;</span>])<br><br>    <span class="hljs-keyword">return</span> input_data<br></code></pre></td></tr></table></figure><p>在该代码中，先是设置了语料库文件的路径CORPUS_PATH，KERAS模型保存路径KERAS_MODEL_SAVE_PATH，以及在项目过程中会用到的三个字典的保存路径（以pickle文件形式保存）WORD_DICTIONARY_PATH，LABEL_DICTIONARY_PATH，OUTPUT_DICTIONARY_PATH。然后是load_data()函数，它将语料库中的文本以Pandas中的DataFrame结构展示出来，该数据框的前30行如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs apache">         <span class="hljs-attribute">word</span>  pos     tag sent_no<br><span class="hljs-attribute">0</span>      played  VBD       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">1</span>          <span class="hljs-literal">on</span>   IN       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">2</span>      Monday  NNP       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">3</span>           (    (       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">4</span>        home   NN       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">5</span>        team   NN       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">6</span>          in   IN       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">7</span>        CAPS  NNP       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">8</span>           )    )       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">9</span>           :    :       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">10</span>   American  NNP  B-MISC       <span class="hljs-number">2</span><br><span class="hljs-attribute">11</span>     League  NNP  I-MISC       <span class="hljs-number">2</span><br><span class="hljs-attribute">12</span>  Cleveland  NNP   B-ORG       <span class="hljs-number">3</span><br><span class="hljs-attribute">13</span>          <span class="hljs-number">2</span>   CD       O       <span class="hljs-number">3</span><br><span class="hljs-attribute">14</span>    DETROIT  NNP   B-ORG       <span class="hljs-number">3</span><br><span class="hljs-attribute">15</span>          <span class="hljs-number">1</span>   CD       O       <span class="hljs-number">3</span><br><span class="hljs-attribute">16</span>  BALTIMORE   VB   B-ORG       <span class="hljs-number">4</span><br><span class="hljs-attribute">17</span>         <span class="hljs-number">12</span>   CD       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">18</span>    Oakland  NNP   B-ORG       <span class="hljs-number">4</span><br><span class="hljs-attribute">19</span>         <span class="hljs-number">11</span>   CD       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">20</span>          (    (       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">21</span>         <span class="hljs-number">10</span>   CD       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">22</span>    innings   NN       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">23</span>          )    )       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">24</span>    TORONTO   TO   B-ORG       <span class="hljs-number">5</span><br><span class="hljs-attribute">25</span>          <span class="hljs-number">5</span>   CD       O       <span class="hljs-number">5</span><br><span class="hljs-attribute">26</span>  Minnesota  NNP   B-ORG       <span class="hljs-number">5</span><br><span class="hljs-attribute">27</span>          <span class="hljs-number">3</span>   CD       O       <span class="hljs-number">5</span><br><span class="hljs-attribute">28</span>  Milwaukee  NNP   B-ORG       <span class="hljs-number">6</span><br><span class="hljs-attribute">29</span>          <span class="hljs-number">3</span>   CD       O       <span class="hljs-number">6</span><br></code></pre></td></tr></table></figure><p>在该数据框中，word这一列表示文本语料库中的单词，pos这一列表示该单词的词性，tag这一列表示NER的标注，sent_no这一列表示该单词在第几个句子中。</p><h3 id="数据探索">数据探索</h3><p>接着，第二步是数据探索，即对输入的数据（input_data）进行一些数据review，完整的代码（data_processing.py）如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> accumulate<br><span class="hljs-keyword">from</span> operator <span class="hljs-keyword">import</span> itemgetter<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib <span class="hljs-keyword">as</span> mpl<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> BASE_DIR, CONSTANTS, load_data<br><br><span class="hljs-comment"># 设置matplotlib绘图时的字体</span><br>mpl.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>]=[<span class="hljs-string">&#x27;SimHei&#x27;</span>]<br><br><span class="hljs-comment"># 数据查看</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_review</span>():<br><br>    <span class="hljs-comment"># 数据导入</span><br>    input_data = load_data()<br><br>    <span class="hljs-comment"># 基本的数据review</span><br>    sent_num = input_data[<span class="hljs-string">&#x27;sent_no&#x27;</span>].astype(np.<span class="hljs-built_in">int</span>).<span class="hljs-built_in">max</span>()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;一共有%s个句子。\n&quot;</span>%sent_num)<br><br>    vocabulary = input_data[<span class="hljs-string">&#x27;word&#x27;</span>].unique()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;一共有%d个单词。&quot;</span>%<span class="hljs-built_in">len</span>(vocabulary))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;前10个单词为：%s.\n&quot;</span>%vocabulary[:<span class="hljs-number">11</span>])<br><br>    pos_arr = input_data[<span class="hljs-string">&#x27;pos&#x27;</span>].unique()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;单词的词性列表：%s.\n&quot;</span>%pos_arr)<br><br>    ner_tag_arr = input_data[<span class="hljs-string">&#x27;tag&#x27;</span>].unique()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;NER的标注列表：%s.\n&quot;</span> % ner_tag_arr)<br><br>    df = input_data[[<span class="hljs-string">&#x27;word&#x27;</span>, <span class="hljs-string">&#x27;sent_no&#x27;</span>]].groupby(<span class="hljs-string">&#x27;sent_no&#x27;</span>).count()<br>    sent_len_list = df[<span class="hljs-string">&#x27;word&#x27;</span>].tolist()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;句子长度及出现频数字典：\n%s.&quot;</span> % <span class="hljs-built_in">dict</span>(Counter(sent_len_list)))<br><br>    <span class="hljs-comment"># 绘制句子长度及出现频数统计图</span><br>    sort_sent_len_dist = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">dict</span>(Counter(sent_len_list)).items(), key=itemgetter(<span class="hljs-number">0</span>))<br>    sent_no_data = [item[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sort_sent_len_dist]<br>    sent_count_data = [item[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sort_sent_len_dist]<br>    plt.bar(sent_no_data, sent_count_data)<br>    plt.title(<span class="hljs-string">&quot;句子长度及出现频数统计图&quot;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;句子长度&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;句子长度出现的频数&quot;</span>)<br>    plt.savefig(<span class="hljs-string">&quot;%s/句子长度及出现频数统计图.png&quot;</span> % BASE_DIR)<br>    plt.close()<br><br>    <span class="hljs-comment"># 绘制句子长度累积分布函数(CDF)</span><br>    sent_pentage_list = [(count/sent_num) <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> accumulate(sent_count_data)]<br><br>    <span class="hljs-comment"># 寻找分位点为quantile的句子长度</span><br>    quantile = <span class="hljs-number">0.9992</span><br>    <span class="hljs-comment">#print(list(sent_pentage_list))</span><br>    <span class="hljs-keyword">for</span> length, per <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(sent_no_data, sent_pentage_list):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">round</span>(per, <span class="hljs-number">4</span>) == quantile:<br>            index = length<br>            <span class="hljs-keyword">break</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n分位点为%s的句子长度:%d.&quot;</span> % (quantile, index))<br><br>    <span class="hljs-comment"># 绘制CDF</span><br>    plt.plot(sent_no_data, sent_pentage_list)<br>    plt.hlines(quantile, <span class="hljs-number">0</span>, index, colors=<span class="hljs-string">&quot;c&quot;</span>, linestyles=<span class="hljs-string">&quot;dashed&quot;</span>)<br>    plt.vlines(index, <span class="hljs-number">0</span>, quantile, colors=<span class="hljs-string">&quot;c&quot;</span>, linestyles=<span class="hljs-string">&quot;dashed&quot;</span>)<br>    plt.text(<span class="hljs-number">0</span>, quantile, <span class="hljs-built_in">str</span>(quantile))<br>    plt.text(index, <span class="hljs-number">0</span>, <span class="hljs-built_in">str</span>(index))<br>    plt.title(<span class="hljs-string">&quot;句子长度累积分布函数图&quot;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;句子长度&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;句子长度累积频率&quot;</span>)<br>    plt.savefig(<span class="hljs-string">&quot;%s/句子长度累积分布函数图.png&quot;</span> % BASE_DIR)<br>    plt.close()<br><br><span class="hljs-comment"># 数据处理</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_processing</span>():<br>    <span class="hljs-comment"># 数据导入</span><br>    input_data = load_data()<br><br>    <span class="hljs-comment"># 标签及词汇表</span><br>    labels, vocabulary = <span class="hljs-built_in">list</span>(input_data[<span class="hljs-string">&#x27;tag&#x27;</span>].unique()), <span class="hljs-built_in">list</span>(input_data[<span class="hljs-string">&#x27;word&#x27;</span>].unique())<br><br>    <span class="hljs-comment"># 字典列表</span><br>    word_dictionary = &#123;word: i+<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocabulary)&#125;<br>    inverse_word_dictionary = &#123;i+<span class="hljs-number">1</span>: word <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocabulary)&#125;<br>    label_dictionary = &#123;label: i+<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels)&#125;<br>    output_dictionary = &#123;i+<span class="hljs-number">1</span>: labels <span class="hljs-keyword">for</span> i, labels <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels)&#125;<br><br>    dict_list = [word_dictionary, inverse_word_dictionary,label_dictionary, output_dictionary]<br><br>    <span class="hljs-comment"># 保存为pickle形式</span><br>    <span class="hljs-keyword">for</span> dict_item, path <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(dict_list, CONSTANTS[<span class="hljs-number">1</span>:]):<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(path, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            pickle.dump(dict_item, f)<br><br><span class="hljs-comment">#data_review()</span><br></code></pre></td></tr></table></figure><p>调用data_review()函数，输出的结果如下：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs prolog">一共有<span class="hljs-number">13998</span>个句子。<br><br>一共有<span class="hljs-number">24339</span>个单词。<br>前<span class="hljs-number">10</span>个单词为：[<span class="hljs-string">&#x27;played&#x27;</span> <span class="hljs-string">&#x27;on&#x27;</span> <span class="hljs-string">&#x27;Monday&#x27;</span> <span class="hljs-string">&#x27;(&#x27;</span> <span class="hljs-string">&#x27;home&#x27;</span> <span class="hljs-string">&#x27;team&#x27;</span> <span class="hljs-string">&#x27;in&#x27;</span> <span class="hljs-string">&#x27;CAPS&#x27;</span> <span class="hljs-string">&#x27;)&#x27;</span> <span class="hljs-string">&#x27;:&#x27;</span> <span class="hljs-string">&#x27;American&#x27;</span>].<br><br>单词的词性列表：[<span class="hljs-string">&#x27;VBD&#x27;</span> <span class="hljs-string">&#x27;IN&#x27;</span> <span class="hljs-string">&#x27;NNP&#x27;</span> <span class="hljs-string">&#x27;(&#x27;</span> <span class="hljs-string">&#x27;NN&#x27;</span> <span class="hljs-string">&#x27;)&#x27;</span> <span class="hljs-string">&#x27;:&#x27;</span> <span class="hljs-string">&#x27;CD&#x27;</span> <span class="hljs-string">&#x27;VB&#x27;</span> <span class="hljs-string">&#x27;TO&#x27;</span> <span class="hljs-string">&#x27;NNS&#x27;</span> <span class="hljs-string">&#x27;,&#x27;</span> <span class="hljs-string">&#x27;VBP&#x27;</span> <span class="hljs-string">&#x27;VBZ&#x27;</span><br> <span class="hljs-string">&#x27;.&#x27;</span> <span class="hljs-string">&#x27;VBG&#x27;</span> <span class="hljs-string">&#x27;PRP$&#x27;</span> <span class="hljs-string">&#x27;JJ&#x27;</span> <span class="hljs-string">&#x27;CC&#x27;</span> <span class="hljs-string">&#x27;JJS&#x27;</span> <span class="hljs-string">&#x27;RB&#x27;</span> <span class="hljs-string">&#x27;DT&#x27;</span> <span class="hljs-string">&#x27;VBN&#x27;</span> <span class="hljs-string">&#x27;&quot;&#x27;</span> <span class="hljs-string">&#x27;PRP&#x27;</span> <span class="hljs-string">&#x27;WDT&#x27;</span> <span class="hljs-string">&#x27;WRB&#x27;</span><br> <span class="hljs-string">&#x27;MD&#x27;</span> <span class="hljs-string">&#x27;WP&#x27;</span> <span class="hljs-string">&#x27;POS&#x27;</span> <span class="hljs-string">&#x27;JJR&#x27;</span> <span class="hljs-string">&#x27;WP$&#x27;</span> <span class="hljs-string">&#x27;RP&#x27;</span> <span class="hljs-string">&#x27;NNPS&#x27;</span> <span class="hljs-string">&#x27;RBS&#x27;</span> <span class="hljs-string">&#x27;FW&#x27;</span> <span class="hljs-string">&#x27;$&#x27;</span> <span class="hljs-string">&#x27;RBR&#x27;</span> <span class="hljs-string">&#x27;EX&#x27;</span> <span class="hljs-string">&quot;&#x27;&#x27;&quot;</span><br> <span class="hljs-string">&#x27;PDT&#x27;</span> <span class="hljs-string">&#x27;UH&#x27;</span> <span class="hljs-string">&#x27;SYM&#x27;</span> <span class="hljs-string">&#x27;LS&#x27;</span> <span class="hljs-string">&#x27;NN|SYM&#x27;</span>].<br><br><span class="hljs-symbol">NER</span>的标注列表：[<span class="hljs-string">&#x27;O&#x27;</span> <span class="hljs-string">&#x27;B-MISC&#x27;</span> <span class="hljs-string">&#x27;I-MISC&#x27;</span> <span class="hljs-string">&#x27;B-ORG&#x27;</span> <span class="hljs-string">&#x27;I-ORG&#x27;</span> <span class="hljs-string">&#x27;B-PER&#x27;</span> <span class="hljs-string">&#x27;B-LOC&#x27;</span> <span class="hljs-string">&#x27;I-PER&#x27;</span> <span class="hljs-string">&#x27;I-LOC&#x27;</span><br> <span class="hljs-string">&#x27;sO&#x27;</span>].<br><br>句子长度及出现频数字典：<br>&#123;<span class="hljs-number">1</span>: <span class="hljs-number">177</span>, <span class="hljs-number">2</span>: <span class="hljs-number">1141</span>, <span class="hljs-number">3</span>: <span class="hljs-number">620</span>, <span class="hljs-number">4</span>: <span class="hljs-number">794</span>, <span class="hljs-number">5</span>: <span class="hljs-number">769</span>, <span class="hljs-number">6</span>: <span class="hljs-number">639</span>, <span class="hljs-number">7</span>: <span class="hljs-number">999</span>, <span class="hljs-number">8</span>: <span class="hljs-number">977</span>, <span class="hljs-number">9</span>: <span class="hljs-number">841</span>, <span class="hljs-number">10</span>: <span class="hljs-number">501</span>, <span class="hljs-number">11</span>: <span class="hljs-number">395</span>, <span class="hljs-number">12</span>: <span class="hljs-number">316</span>, <span class="hljs-number">13</span>: <span class="hljs-number">339</span>, <span class="hljs-number">14</span>: <span class="hljs-number">291</span>, <span class="hljs-number">15</span>: <span class="hljs-number">275</span>, <span class="hljs-number">16</span>: <span class="hljs-number">225</span>, <span class="hljs-number">17</span>: <span class="hljs-number">229</span>, <span class="hljs-number">18</span>: <span class="hljs-number">212</span>, <span class="hljs-number">19</span>: <span class="hljs-number">197</span>, <span class="hljs-number">20</span>: <span class="hljs-number">221</span>, <span class="hljs-number">21</span>: <span class="hljs-number">228</span>, <span class="hljs-number">22</span>: <span class="hljs-number">221</span>, <span class="hljs-number">23</span>: <span class="hljs-number">230</span>, <span class="hljs-number">24</span>: <span class="hljs-number">210</span>, <span class="hljs-number">25</span>: <span class="hljs-number">207</span>, <span class="hljs-number">26</span>: <span class="hljs-number">224</span>, <span class="hljs-number">27</span>: <span class="hljs-number">188</span>, <span class="hljs-number">28</span>: <span class="hljs-number">199</span>, <span class="hljs-number">29</span>: <span class="hljs-number">214</span>, <span class="hljs-number">30</span>: <span class="hljs-number">183</span>, <span class="hljs-number">31</span>: <span class="hljs-number">202</span>, <span class="hljs-number">32</span>: <span class="hljs-number">167</span>, <span class="hljs-number">33</span>: <span class="hljs-number">167</span>, <span class="hljs-number">34</span>: <span class="hljs-number">141</span>, <span class="hljs-number">35</span>: <span class="hljs-number">130</span>, <span class="hljs-number">36</span>: <span class="hljs-number">119</span>, <span class="hljs-number">37</span>: <span class="hljs-number">105</span>, <span class="hljs-number">38</span>: <span class="hljs-number">112</span>, <span class="hljs-number">39</span>: <span class="hljs-number">98</span>, <span class="hljs-number">40</span>: <span class="hljs-number">78</span>, <span class="hljs-number">41</span>: <span class="hljs-number">74</span>, <span class="hljs-number">42</span>: <span class="hljs-number">63</span>, <span class="hljs-number">43</span>: <span class="hljs-number">51</span>, <span class="hljs-number">44</span>: <span class="hljs-number">42</span>, <span class="hljs-number">45</span>: <span class="hljs-number">39</span>, <span class="hljs-number">46</span>: <span class="hljs-number">19</span>, <span class="hljs-number">47</span>: <span class="hljs-number">22</span>, <span class="hljs-number">48</span>: <span class="hljs-number">19</span>, <span class="hljs-number">49</span>: <span class="hljs-number">15</span>, <span class="hljs-number">50</span>: <span class="hljs-number">16</span>, <span class="hljs-number">51</span>: <span class="hljs-number">8</span>, <span class="hljs-number">52</span>: <span class="hljs-number">9</span>, <span class="hljs-number">53</span>: <span class="hljs-number">5</span>, <span class="hljs-number">54</span>: <span class="hljs-number">4</span>, <span class="hljs-number">55</span>: <span class="hljs-number">9</span>, <span class="hljs-number">56</span>: <span class="hljs-number">2</span>, <span class="hljs-number">57</span>: <span class="hljs-number">2</span>, <span class="hljs-number">58</span>: <span class="hljs-number">2</span>, <span class="hljs-number">59</span>: <span class="hljs-number">2</span>, <span class="hljs-number">60</span>: <span class="hljs-number">3</span>, <span class="hljs-number">62</span>: <span class="hljs-number">2</span>, <span class="hljs-number">66</span>: <span class="hljs-number">1</span>, <span class="hljs-number">67</span>: <span class="hljs-number">1</span>, <span class="hljs-number">69</span>: <span class="hljs-number">1</span>, <span class="hljs-number">71</span>: <span class="hljs-number">1</span>, <span class="hljs-number">72</span>: <span class="hljs-number">1</span>, <span class="hljs-number">78</span>: <span class="hljs-number">1</span>, <span class="hljs-number">80</span>: <span class="hljs-number">1</span>, <span class="hljs-number">113</span>: <span class="hljs-number">1</span>, <span class="hljs-number">124</span>: <span class="hljs-number">1</span>&#125;.<br><br>分位点为<span class="hljs-number">0.9992</span>的句子长度:<span class="hljs-number">60.</span><br></code></pre></td></tr></table></figure><p>在该语料库中，一共有13998个句子，比预期的42000/3=14000个句子少两个。一个有24339个单词，单词量还是蛮大的，当然，这里对单词没有做任何处理，直接保留了语料库中的形式（后期可以继续优化）。单词的词性可以参考文章：<ahref="https://www.jianshu.com/p/79255fe0c5b5">NLP入门（三）词形还原（Lemmatization）</a>。我们需要注意的是，NER的标注列表为['O','B-MISC', 'I-MISC', 'B-ORG' ,'I-ORG', 'B-PER' ,'B-LOC' ,'I-PER','I-LOC','sO']，因此，本项目的NER一共分为四类：PER（人名），LOC（位置），ORG（组织）以及MISC，其中B表示开始，I表示中间，O表示单字词，不计入NER，sO表示特殊单字词。接下来，让我们考虑下句子的长度，这对后面的建模时填充的句子长度有有参考作用。句子长度及出现频数的统计图如下：</p><p><img src="/img/nlp5_2.png" alt="句子长度及出现频数统计图" />可以看到，句子长度基本在60以下，当然，这也可以在输出的句子长度及出现频数字典中看到。那么，我们是否可以选在一个标准作为后面模型的句子填充的长度呢？答案是，利用出现频数的累计分布函数的分位点，在这里，我们选择分位点为0.9992,对应的句子长度为60，如下图：</p><figure><img src="/img/nlp5_3.png" alt="句子长度累积分布函数图" /><figcaption aria-hidden="true">句子长度累积分布函数图</figcaption></figure><p>接着是数据处理函数data_processing()，它的功能主要是实现单词、标签字典，并保存为pickle文件形式，便于后续直接调用。</p><h3 id="建模">建模</h3><p>在第三步中，我们建立Bi-LSTM模型来训练训练，完整的Python代码（Bi_LSTM_Model_training.py）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> BASE_DIR, CONSTANTS, load_data<br><span class="hljs-keyword">from</span> data_processing <span class="hljs-keyword">import</span> data_processing<br><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> np_utils, plot_model<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Bidirectional, LSTM, Dense, Embedding, TimeDistributed<br><br><br><span class="hljs-comment"># 模型输入数据</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">input_data_for_model</span>(<span class="hljs-params">input_shape</span>):<br><br>    <span class="hljs-comment"># 数据导入</span><br>    input_data = load_data()<br>    <span class="hljs-comment"># 数据处理</span><br>    data_processing()<br>    <span class="hljs-comment"># 导入字典</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">1</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        word_dictionary = pickle.load(f)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">2</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        inverse_word_dictionary = pickle.load(f)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">3</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        label_dictionary = pickle.load(f)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">4</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        output_dictionary = pickle.load(f)<br>    vocab_size = <span class="hljs-built_in">len</span>(word_dictionary.keys())<br>    label_size = <span class="hljs-built_in">len</span>(label_dictionary.keys())<br><br>    <span class="hljs-comment"># 处理输入数据</span><br>    aggregate_function = <span class="hljs-keyword">lambda</span> <span class="hljs-built_in">input</span>: [(word, pos, label) <span class="hljs-keyword">for</span> word, pos, label <span class="hljs-keyword">in</span><br>                                            <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;word&#x27;</span>].values.tolist(),<br>                                                <span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;pos&#x27;</span>].values.tolist(),<br>                                                <span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;tag&#x27;</span>].values.tolist())]<br><br>    grouped_input_data = input_data.groupby(<span class="hljs-string">&#x27;sent_no&#x27;</span>).apply(aggregate_function)<br>    sentences = [sentence <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> grouped_input_data]<br><br>    x = [[word_dictionary[word[<span class="hljs-number">0</span>]] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sentences]<br>    x = pad_sequences(maxlen=input_shape, sequences=x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br>    y = [[label_dictionary[word[<span class="hljs-number">2</span>]] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sentences]<br>    y = pad_sequences(maxlen=input_shape, sequences=y, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br>    y = [np_utils.to_categorical(label, num_classes=label_size + <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> y]<br><br>    <span class="hljs-keyword">return</span> x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary<br><br><br><span class="hljs-comment"># 定义深度学习模型：Bi-LSTM</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_Bi_LSTM</span>(<span class="hljs-params">vocab_size, label_size, input_shape, output_dim, n_units, out_act, activation</span>):<br>    model = Sequential()<br>    model.add(Embedding(input_dim=vocab_size + <span class="hljs-number">1</span>, output_dim=output_dim,<br>                        input_length=input_shape, mask_zero=<span class="hljs-literal">True</span>))<br>    model.add(Bidirectional(LSTM(units=n_units, activation=activation,<br>                                 return_sequences=<span class="hljs-literal">True</span>)))<br>    model.add(TimeDistributed(Dense(label_size + <span class="hljs-number">1</span>, activation=out_act)))<br>    model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>, loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-comment"># 模型训练</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_train</span>():<br><br>    <span class="hljs-comment"># 将数据集分为训练集和测试集，占比为9:1</span><br>    input_shape = <span class="hljs-number">60</span><br>    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = input_data_for_model(input_shape)<br>    train_end = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(x)*<span class="hljs-number">0.9</span>)<br>    train_x, train_y = x[<span class="hljs-number">0</span>:train_end], np.array(y[<span class="hljs-number">0</span>:train_end])<br>    test_x, test_y = x[train_end:], np.array(y[train_end:])<br><br>    <span class="hljs-comment"># 模型输入参数</span><br>    activation = <span class="hljs-string">&#x27;selu&#x27;</span><br>    out_act = <span class="hljs-string">&#x27;softmax&#x27;</span><br>    n_units = <span class="hljs-number">100</span><br>    batch_size = <span class="hljs-number">32</span><br>    epochs = <span class="hljs-number">10</span><br>    output_dim = <span class="hljs-number">20</span><br><br>    <span class="hljs-comment"># 模型训练</span><br>    lstm_model = create_Bi_LSTM(vocab_size, label_size, input_shape, output_dim, n_units, out_act, activation)<br>    lstm_model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 模型保存</span><br>    model_save_path = CONSTANTS[<span class="hljs-number">0</span>]<br>    lstm_model.save(model_save_path)<br>    plot_model(lstm_model, to_file=<span class="hljs-string">&#x27;%s/LSTM_model.png&#x27;</span> % BASE_DIR)<br><br>    <span class="hljs-comment"># 在测试集上的效果</span><br>    N = test_x.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 测试的条数</span><br>    avg_accuracy = <span class="hljs-number">0</span>  <span class="hljs-comment"># 预测的平均准确率</span><br>    <span class="hljs-keyword">for</span> start, end <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, N, <span class="hljs-number">1</span>), <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, N+<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)):<br>        sentence = [inverse_word_dictionary[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> test_x[start] <span class="hljs-keyword">if</span> i != <span class="hljs-number">0</span>]<br>        y_predict = lstm_model.predict(test_x[start:end])<br>        input_sequences, output_sequences = [], []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(y_predict[<span class="hljs-number">0</span>])):<br>            output_sequences.append(np.argmax(y_predict[<span class="hljs-number">0</span>][i]))<br>            input_sequences.append(np.argmax(test_y[start][i]))<br><br>        <span class="hljs-built_in">eval</span> = lstm_model.evaluate(test_x[start:end], test_y[start:end])<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test Accuracy: loss = %0.6f accuracy = %0.2f%%&#x27;</span> % (<span class="hljs-built_in">eval</span>[<span class="hljs-number">0</span>], <span class="hljs-built_in">eval</span>[<span class="hljs-number">1</span>] * <span class="hljs-number">100</span>))<br>        avg_accuracy += <span class="hljs-built_in">eval</span>[<span class="hljs-number">1</span>]<br>        output_sequences = <span class="hljs-string">&#x27; &#x27;</span>.join([output_dictionary[key] <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> output_sequences <span class="hljs-keyword">if</span> key != <span class="hljs-number">0</span>]).split()<br>        input_sequences = <span class="hljs-string">&#x27; &#x27;</span>.join([output_dictionary[key] <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> input_sequences <span class="hljs-keyword">if</span> key != <span class="hljs-number">0</span>]).split()<br>        output_input_comparison = pd.DataFrame([sentence, output_sequences, input_sequences]).T<br>        <span class="hljs-built_in">print</span>(output_input_comparison.dropna())<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;#&#x27;</span> * <span class="hljs-number">80</span>)<br><br>    avg_accuracy /= N<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;测试样本的平均预测准确率：%.2f%%.&quot;</span> % (avg_accuracy * <span class="hljs-number">100</span>))<br><br>model_train()<br></code></pre></td></tr></table></figure><p>在上面的代码中，先是通过input_data_for_model()函数来处理好进入模型的数据，其参数为input_shape，即填充句子时的长度。然后是创建Bi-LSTM模型create_Bi_LSTM()，模型的示意图如下：</p><figure><img src="/img/nlp5_4.png" alt="模型示意图" /><figcaption aria-hidden="true">模型示意图</figcaption></figure><p>最后，是在输入的数据上进行模型训练，将原始的数据分为训练集和测试集，占比为9:1，训练的周期为10次。</p><h3 id="模型训练">模型训练</h3><p>运行上述模型训练代码，一共训练10个周期，训练时间大概为500s，在训练集上的准确率达99%以上，在测试集上的平均准确率为95%以上。以下是最后几个测试集上的预测结果：</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs clean">......(前面的输出已忽略)<br>Test Accuracy: loss = <span class="hljs-number">0.000986</span> accuracy = <span class="hljs-number">100.00</span>%<br>          <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>   Cardiff  B-ORG  B-ORG<br><span class="hljs-number">1</span>         <span class="hljs-number">1</span>      O      O<br><span class="hljs-number">2</span>  Brighton  B-ORG  B-ORG<br><span class="hljs-number">3</span>         <span class="hljs-number">0</span>      O      O<br>################################################################################<br><br><span class="hljs-number">1</span>/<span class="hljs-number">1</span> [==============================] - <span class="hljs-number">0</span>s <span class="hljs-number">10</span>ms/step<br>Test Accuracy: loss = <span class="hljs-number">0.000274</span> accuracy = <span class="hljs-number">100.00</span>%<br>          <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>  Carlisle  B-ORG  B-ORG<br><span class="hljs-number">1</span>         <span class="hljs-number">0</span>      O      O<br><span class="hljs-number">2</span>      Hull  B-ORG  B-ORG<br><span class="hljs-number">3</span>         <span class="hljs-number">0</span>      O      O<br>################################################################################<br><br><span class="hljs-number">1</span>/<span class="hljs-number">1</span> [==============================] - <span class="hljs-number">0</span>s <span class="hljs-number">9</span>ms/step<br>Test Accuracy: loss = <span class="hljs-number">0.000479</span> accuracy = <span class="hljs-number">100.00</span>%<br>           <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>    Chester  B-ORG  B-ORG<br><span class="hljs-number">1</span>          <span class="hljs-number">1</span>      O      O<br><span class="hljs-number">2</span>  Cambridge  B-ORG  B-ORG<br><span class="hljs-number">3</span>          <span class="hljs-number">1</span>      O      O<br>################################################################################<br><br><span class="hljs-number">1</span>/<span class="hljs-number">1</span> [==============================] - <span class="hljs-number">0</span>s <span class="hljs-number">9</span>ms/step<br>Test Accuracy: loss = <span class="hljs-number">0.003092</span> accuracy = <span class="hljs-number">100.00</span>%<br>            <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>  Darlington  B-ORG  B-ORG<br><span class="hljs-number">1</span>           <span class="hljs-number">4</span>      O      O<br><span class="hljs-number">2</span>     Swansea  B-ORG  B-ORG<br><span class="hljs-number">3</span>           <span class="hljs-number">1</span>      O      O<br>################################################################################<br><br><span class="hljs-number">1</span>/<span class="hljs-number">1</span> [==============================] - <span class="hljs-number">0</span>s <span class="hljs-number">8</span>ms/step<br>Test Accuracy: loss = <span class="hljs-number">0.000705</span> accuracy = <span class="hljs-number">100.00</span>%<br>             <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>       Exeter  B-ORG  B-ORG<br><span class="hljs-number">1</span>            <span class="hljs-number">2</span>      O      O<br><span class="hljs-number">2</span>  Scarborough  B-ORG  B-ORG<br><span class="hljs-number">3</span>            <span class="hljs-number">2</span>      O      O<br>################################################################################<br>测试样本的平均预测准确率：<span class="hljs-number">95.55</span>%.<br></code></pre></td></tr></table></figure><p>该模型在原始数据上的识别效果还是可以的。训练完模型后，BASE_DIR中的所有文件如下：</p><figure><img src="/img/nlp5_5.png" alt="模型训练完后的所有文件截图" /><figcaption aria-hidden="true">模型训练完后的所有文件截图</figcaption></figure><h3 id="模型预测">模型预测</h3><p>最后，也许是整个项目最为激动人心的时刻，因为，我们要在新数据集上测试模型的识别效果。预测新数据的识别结果的完整Python代码（Bi_LSTM_Model_predict.py）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># Name entity recognition for new data</span><br><br><span class="hljs-comment"># Import the necessary modules</span><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> CONSTANTS<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<br><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize<br><br><span class="hljs-comment"># 导入字典</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">1</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    word_dictionary = pickle.load(f)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">4</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    output_dictionary = pickle.load(f)<br><br><span class="hljs-keyword">try</span>:<br>    <span class="hljs-comment"># 数据预处理</span><br>    input_shape = <span class="hljs-number">60</span><br>    sent = <span class="hljs-string">&#x27;New York is the biggest city in America.&#x27;</span><br>    new_sent = word_tokenize(sent)<br>    new_x = [[word_dictionary[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> new_sent]]<br>    x = pad_sequences(maxlen=input_shape, sequences=new_x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># 载入模型</span><br>    model_save_path = CONSTANTS[<span class="hljs-number">0</span>]<br>    lstm_model = load_model(model_save_path)<br><br>    <span class="hljs-comment"># 模型预测</span><br>    y_predict = lstm_model.predict(x)<br><br>    ner_tag = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(new_sent)):<br>        ner_tag.append(np.argmax(y_predict[<span class="hljs-number">0</span>][i]))<br><br>    ner = [output_dictionary[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ner_tag]<br>    <span class="hljs-built_in">print</span>(new_sent)<br>    <span class="hljs-built_in">print</span>(ner)<br><br>    <span class="hljs-comment"># 去掉NER标注为O的元素</span><br>    ner_reg_list = []<br>    <span class="hljs-keyword">for</span> word, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(new_sent, ner):<br>        <span class="hljs-keyword">if</span> tag != <span class="hljs-string">&#x27;O&#x27;</span>:<br>            ner_reg_list.append((word, tag))<br><br>    <span class="hljs-comment"># 输出模型的NER识别结果</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;NER识别结果：&quot;</span>)<br>    <span class="hljs-keyword">if</span> ner_reg_list:<br>        <span class="hljs-keyword">for</span> i, item <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(ner_reg_list):<br>            <span class="hljs-keyword">if</span> item[<span class="hljs-number">1</span>].startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>                end = i+<span class="hljs-number">1</span><br>                <span class="hljs-keyword">while</span> end &lt;= <span class="hljs-built_in">len</span>(ner_reg_list)-<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> ner_reg_list[end][<span class="hljs-number">1</span>].startswith(<span class="hljs-string">&#x27;I&#x27;</span>):<br>                    end += <span class="hljs-number">1</span><br><br>                ner_type = item[<span class="hljs-number">1</span>].split(<span class="hljs-string">&#x27;-&#x27;</span>)[<span class="hljs-number">1</span>]<br>                ner_type_dict = &#123;<span class="hljs-string">&#x27;PER&#x27;</span>: <span class="hljs-string">&#x27;PERSON: &#x27;</span>,<br>                                <span class="hljs-string">&#x27;LOC&#x27;</span>: <span class="hljs-string">&#x27;LOCATION: &#x27;</span>,<br>                                <span class="hljs-string">&#x27;ORG&#x27;</span>: <span class="hljs-string">&#x27;ORGANIZATION: &#x27;</span>,<br>                                <span class="hljs-string">&#x27;MISC&#x27;</span>: <span class="hljs-string">&#x27;MISC: &#x27;</span><br>                                &#125;<br>                <span class="hljs-built_in">print</span>(ner_type_dict[ner_type],\<br>                    <span class="hljs-string">&#x27; &#x27;</span>.join([item[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> ner_reg_list[i:end]]))<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型并未识别任何有效命名实体。&quot;</span>)<br><br><span class="hljs-keyword">except</span> KeyError <span class="hljs-keyword">as</span> err:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;您输入的句子有单词不在词汇表中，请重新输入！&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;不在词汇表中的单词为：%s.&quot;</span> % err)<br></code></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">[<span class="hljs-string">&#x27;New&#x27;</span>, <span class="hljs-string">&#x27;York&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;biggest&#x27;</span>, <span class="hljs-string">&#x27;city&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;America&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]<br>[<span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]<br>NER识别结果：<br><span class="hljs-keyword">LOCATION</span>:  <span class="hljs-built_in">New</span> York<br><span class="hljs-keyword">LOCATION</span>:  America<br></code></pre></td></tr></table></figure><p>接下来，再测试三个笔者自己想的句子：</p><p>输入为： <figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">sent = <span class="hljs-symbol">&#x27;James</span> <span class="hljs-keyword">is</span> a world famous actor, whose home <span class="hljs-keyword">is</span> <span class="hljs-keyword">in</span> London.&#x27;<br></code></pre></td></tr></table></figure> 输出结果为：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;James&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;world&#x27;</span>, <span class="hljs-string">&#x27;famous&#x27;</span>, <span class="hljs-string">&#x27;actor&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;whose&#x27;</span>, <span class="hljs-string">&#x27;home&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;London&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>PERSON:  James<br>LOCATION:  London<br></code></pre></td></tr></table></figure><p>输入为： <figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">sent = <span class="hljs-symbol">&#x27;Oxford</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">in</span> England, Jack <span class="hljs-keyword">is</span> from here.&#x27;<br></code></pre></td></tr></table></figure> 输出为： <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;Oxford&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;England&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;Jack&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;from&#x27;</span>, <span class="hljs-string">&#x27;here&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>PERSON:  Oxford<br>LOCATION:  England<br>PERSON:  Jack<br></code></pre></td></tr></table></figure></p><p>输入为： <figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">sent</span> = <span class="hljs-string">&#x27;I love Shanghai.&#x27;</span><br></code></pre></td></tr></table></figure> 输出为： <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;Shanghai&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>LOCATION:  Shanghai<br></code></pre></td></tr></table></figure></p><p>在上面的例子中，只有Oxford的识别效果不理想，模型将它识别为PERSON，其实应该是ORGANIZATION。</p><p>接下来是三个来自CNN和wikipedia的句子：</p><p>输入为： <figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">sent</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;the US runs the risk of a military defeat by China or Russia&quot;</span><br></code></pre></td></tr></table></figure> 输出为： <figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">[<span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;US&#x27;</span>, <span class="hljs-string">&#x27;runs&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;risk&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;military&#x27;</span>, <span class="hljs-string">&#x27;defeat&#x27;</span>, <span class="hljs-string">&#x27;by&#x27;</span>, <span class="hljs-string">&#x27;China&#x27;</span>, <span class="hljs-string">&#x27;or&#x27;</span>, <span class="hljs-string">&#x27;Russia&#x27;</span>]<br>[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>]<br>NER识别结果：<br><span class="hljs-keyword">LOCATION</span>:  US<br><span class="hljs-keyword">LOCATION</span>:  China<br><span class="hljs-keyword">LOCATION</span>:  Russia<br></code></pre></td></tr></table></figure> 输入为：<figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk">sent = <span class="hljs-comment">&quot;Home to the headquarters of the United Nations, New York is an important center for international diplomacy.&quot;</span><br></code></pre></td></tr></table></figure> 输出为： <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;Home&#x27;</span>, <span class="hljs-string">&#x27;to&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;headquarters&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;United&#x27;</span>, <span class="hljs-string">&#x27;Nations&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;New&#x27;</span>, <span class="hljs-string">&#x27;York&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;an&#x27;</span>, <span class="hljs-string">&#x27;important&#x27;</span>, <span class="hljs-string">&#x27;center&#x27;</span>, <span class="hljs-string">&#x27;for&#x27;</span>, <span class="hljs-string">&#x27;international&#x27;</span>, <span class="hljs-string">&#x27;diplomacy&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>ORGANIZATION:  United Nations<br>LOCATION:  New York<br></code></pre></td></tr></table></figure></p><p>输入为： <figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">sent</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;The United States is a founding member of the United Nations, World Bank, International Monetary Fund.&quot;</span><br></code></pre></td></tr></table></figure> 输出为: <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;The&#x27;</span>, <span class="hljs-string">&#x27;United&#x27;</span>, <span class="hljs-string">&#x27;States&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;founding&#x27;</span>, <span class="hljs-string">&#x27;member&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;United&#x27;</span>, <span class="hljs-string">&#x27;Nations&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;World&#x27;</span>, <span class="hljs-string">&#x27;Bank&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;International&#x27;</span>, <span class="hljs-string">&#x27;Monetary&#x27;</span>, <span class="hljs-string">&#x27;Fund&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>LOCATION:  United States<br>ORGANIZATION:  United Nations<br>ORGANIZATION:  World Bank<br>ORGANIZATION:  International Monetary Fund<br></code></pre></td></tr></table></figure></p><p>这三个例子识别全部正确。</p><h3 id="总结">总结</h3><p>到这儿，笔者的这个项目就差不多了。我们有必要对这个项目做个总结。首先是这个项目的优点。它的优点在于能够让你一步步地实现NER，而且除了语料库，你基本熟悉了如何创建一个识别NER系统的步骤，同时，对深度学习模型及其应用也有了深刻理解。因此，好处是显而易见的。当然，在实际工作中，语料库的整理才是最耗费时间的，能够占到90%或者更多的时间，因此，有一个好的语料库你才能展开工作。接着讲讲这个项目的缺点。第一个，是语料库不够大，当然，约14000条句子也够了，但本项目没有对句子进行文本预处理，所以，有些单词的变形可能无法进入词汇表。第二个，缺少对新词的处理，一旦句子中出现一个新的单词，这个模型便无法处理，这是后期需要完善的地方。第三个，句子的填充长度为60，如果输入的句子长度大于60，则后面的部分将无法有效识别。因此，后续还有更多的工作需要去做，当然，做一个中文NER也是可以考虑的。本项目已上传Github,地址为 <ahref="https://github.com/percent4/DL_4_NER">https://github.com/percent4/DL_4_NER</a>。：欢迎大家参考~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p><h3 id="参考文献">参考文献</h3><ol type="1"><li>BOOK： Applied Natural Language Processing with Python， TawehBeysolow II</li><li>WEBSITE：https://github.com/Apress/applied-natural-language-processing-w-python</li><li>WEBSITE: NLP入门（四）命名实体识别（NER）:https://www.jianshu.com/p/16e1f6a7aaef</li></ol>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>NER</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（四）命名实体识别（NER）</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>本文将会简单介绍自然语言处理（NLP）中的命名实体识别（NER）。命名实体识别（Named EntityRecognition，简称NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。举个简单的例子，在句子“小明早上8点去学校上课。”中，对其进行命名实体识别，应该能提取信息</p><blockquote><p>人名：小明，时间：早上8点，地点：学校。</p></blockquote><p>本文将会介绍几个工具用来进行命名实体识别，后续有机会的话，我们将会尝试着用HMM、CRF或深度学习来实现命名实体识别。首先我们来看一下NLTK和Stanford NLP中对命名实体识别的分类，如下图：</p><figure><img src="/img/nlp4_1.png"alt="NLTK和Stanford NLP中对命名实体识别的分类" /><figcaption aria-hidden="true">NLTK和StanfordNLP中对命名实体识别的分类</figcaption></figure><p>在上图中，LOCATION和GPE有重合。GPE通常表示地理—政治条目，比如城市，州，国家，洲等。LOCATION除了上述内容外，还能表示名山大川等。FACILITY通常表示知名的纪念碑或人工制品等。下面介绍两个工具来进行NER的任务：NLTK和Stanford NLP。首先是NLTK，我们的示例文档（介绍FIFA，来源于维基百科）如下：</p><blockquote><p>FIFA was founded in 1904 to oversee international competition amongthe national associations of Belgium, Denmark, France, Germany, theNetherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich,its membership now comprises 211 national associations. Member countriesmust each also be members of one of the six regional confederations intowhich the world is divided: Africa, Asia, Europe, North &amp; CentralAmerica and the Caribbean, Oceania, and South America.</p></blockquote><p>实现NER的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> nltk<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_document</span>(<span class="hljs-params">document</span>):<br>   document = re.sub(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, document)<br>   <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(document, <span class="hljs-built_in">str</span>):<br>       document = document<br>   <span class="hljs-keyword">else</span>:<br>       <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Document is not string!&#x27;</span>)<br>   document = document.strip()<br>   sentences = nltk.sent_tokenize(document)<br>   sentences = [sentence.strip() <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences]<br>   <span class="hljs-keyword">return</span> sentences<br><br><span class="hljs-comment"># sample document</span><br>text = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">FIFA was founded in 1904 to oversee international competition among the national associations of Belgium, </span><br><span class="hljs-string">Denmark, France, Germany, the Netherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich, its </span><br><span class="hljs-string">membership now comprises 211 national associations. Member countries must each also be members of one of </span><br><span class="hljs-string">the six regional confederations into which the world is divided: Africa, Asia, Europe, North &amp; Central America </span><br><span class="hljs-string">and the Caribbean, Oceania, and South America.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># tokenize sentences</span><br>sentences = parse_document(text)<br>tokenized_sentences = [nltk.word_tokenize(sentence) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences]<br><span class="hljs-comment"># tag sentences and use nltk&#x27;s Named Entity Chunker</span><br>tagged_sentences = [nltk.pos_tag(sentence) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> tokenized_sentences]<br>ne_chunked_sents = [nltk.ne_chunk(tagged) <span class="hljs-keyword">for</span> tagged <span class="hljs-keyword">in</span> tagged_sentences]<br><span class="hljs-comment"># extract all named entities</span><br>named_entities = []<br><span class="hljs-keyword">for</span> ne_tagged_sentence <span class="hljs-keyword">in</span> ne_chunked_sents:<br>   <span class="hljs-keyword">for</span> tagged_tree <span class="hljs-keyword">in</span> ne_tagged_sentence:<br>       <span class="hljs-comment"># extract only chunks having NE labels</span><br>       <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(tagged_tree, <span class="hljs-string">&#x27;label&#x27;</span>):<br>           entity_name = <span class="hljs-string">&#x27; &#x27;</span>.join(c[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> tagged_tree.leaves()) <span class="hljs-comment">#get NE name</span><br>           entity_type = tagged_tree.label() <span class="hljs-comment"># get NE category</span><br>           named_entities.append((entity_name, entity_type))<br>           <span class="hljs-comment"># get unique named entities</span><br>           named_entities = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(named_entities))<br><br><span class="hljs-comment"># store named entities in a data frame</span><br>entity_frame = pd.DataFrame(named_entities, columns=[<span class="hljs-string">&#x27;Entity Name&#x27;</span>, <span class="hljs-string">&#x27;Entity Type&#x27;</span>])<br><span class="hljs-comment"># display results</span><br><span class="hljs-built_in">print</span>(entity_frame)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">        Entity Name   Entity Type<br><span class="hljs-number">0</span>              FIFA  <span class="hljs-keyword">ORGANIZATION</span><br><span class="hljs-keyword"></span><span class="hljs-number">1</span>   Central America  <span class="hljs-keyword">ORGANIZATION</span><br><span class="hljs-keyword"></span><span class="hljs-number">2</span>           <span class="hljs-keyword">Belgium </span>          GPE<br><span class="hljs-number">3</span>         Caribbean      LOCATION<br><span class="hljs-number">4</span>              Asia           GPE<br><span class="hljs-number">5</span>            France           GPE<br><span class="hljs-number">6</span>           Oceania           GPE<br><span class="hljs-number">7</span>           Germany           GPE<br><span class="hljs-number">8</span>     South America           GPE<br><span class="hljs-number">9</span>           Denmark           GPE<br><span class="hljs-number">10</span>           Zürich           GPE<br><span class="hljs-number">11</span>           Africa        PERSON<br><span class="hljs-number">12</span>           <span class="hljs-keyword">Sweden </span>          GPE<br><span class="hljs-number">13</span>      Netherlands           GPE<br><span class="hljs-number">14</span>            Spain           GPE<br><span class="hljs-number">15</span>      <span class="hljs-keyword">Switzerland </span>          GPE<br><span class="hljs-number">16</span>            <span class="hljs-keyword">North </span>          GPE<br><span class="hljs-number">17</span>           Europe           GPE<br></code></pre></td></tr></table></figure><p>可以看到，NLTK中的NER任务大体上完成得还是不错的，能够识别FIFA为组织（ORGANIZATION），Belgium,Asia为GPE,但是也有一些不太如人意的地方，比如，它将CentralAmerica识别为ORGANIZATION，而实际上它应该为GPE；将Africa识别为PERSON，实际上应该为GPE。</p><p>接下来，我们尝试着用StanfordNLP工具。关于该工具，我们主要使用Stanford NER标注工具。在使用这个工具之前，你需要在自己的电脑上安装Java（一般是JDK），并将Java添加到系统路径中，同时下载英语NER的文件包：stanford-ner-2018-10-16.zip（大小为172MB），下载地址为：https://nlp.stanford.edu/software/CRF-NER.shtml。以笔者的电脑为例，Java所在的路径为：C:Files.0_161.exe， 下载StanfordNER的zip文件解压后的文件夹的路径为：E://stanford-ner-2018-10-16，如下图所示：</p><p><img src="/img/nlp4_2.png" /></p><p>在classifer文件夹中有如下文件：</p><p><img src="/img/nlp4_3.png" /></p><p>它们代表的含义如下：</p><blockquote><p>3 class: Location, Person, Organization 4 class: Location, Person,Organization, Misc 7 class: Location, Person, Organization, Money,Percent, Date, Time</p></blockquote><p>可以使用Python实现Stanford NER，完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">from</span> nltk.tag <span class="hljs-keyword">import</span> StanfordNERTagger<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> nltk<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_document</span>(<span class="hljs-params">document</span>):<br>   document = re.sub(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, document)<br>   <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(document, <span class="hljs-built_in">str</span>):<br>       document = document<br>   <span class="hljs-keyword">else</span>:<br>       <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Document is not string!&#x27;</span>)<br>   document = document.strip()<br>   sentences = nltk.sent_tokenize(document)<br>   sentences = [sentence.strip() <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences]<br>   <span class="hljs-keyword">return</span> sentences<br><br><span class="hljs-comment"># sample document</span><br>text = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">FIFA was founded in 1904 to oversee international competition among the national associations of Belgium, </span><br><span class="hljs-string">Denmark, France, Germany, the Netherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich, its </span><br><span class="hljs-string">membership now comprises 211 national associations. Member countries must each also be members of one of </span><br><span class="hljs-string">the six regional confederations into which the world is divided: Africa, Asia, Europe, North &amp; Central America </span><br><span class="hljs-string">and the Caribbean, Oceania, and South America.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>sentences = parse_document(text)<br>tokenized_sentences = [nltk.word_tokenize(sentence) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences]<br><br><span class="hljs-comment"># set java path in environment variables</span><br>java_path = <span class="hljs-string">r&#x27;C:\Program Files\Java\jdk1.8.0_161\bin\java.exe&#x27;</span><br>os.environ[<span class="hljs-string">&#x27;JAVAHOME&#x27;</span>] = java_path<br><span class="hljs-comment"># load stanford NER</span><br>sn = StanfordNERTagger(<span class="hljs-string">&#x27;E://stanford-ner-2018-10-16/classifiers/english.muc.7class.distsim.crf.ser.gz&#x27;</span>,<br>                       path_to_jar=<span class="hljs-string">&#x27;E://stanford-ner-2018-10-16/stanford-ner.jar&#x27;</span>)<br><br><span class="hljs-comment"># tag sentences</span><br>ne_annotated_sentences = [sn.tag(sent) <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> tokenized_sentences]<br><span class="hljs-comment"># extract named entities</span><br>named_entities = []<br><span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> ne_annotated_sentences:<br>   temp_entity_name = <span class="hljs-string">&#x27;&#x27;</span><br>   temp_named_entity = <span class="hljs-literal">None</span><br>   <span class="hljs-keyword">for</span> term, tag <span class="hljs-keyword">in</span> sentence:<br>       <span class="hljs-comment"># get terms with NE tags</span><br>       <span class="hljs-keyword">if</span> tag != <span class="hljs-string">&#x27;O&#x27;</span>:<br>           temp_entity_name = <span class="hljs-string">&#x27; &#x27;</span>.join([temp_entity_name, term]).strip() <span class="hljs-comment">#get NE name</span><br>           temp_named_entity = (temp_entity_name, tag) <span class="hljs-comment"># get NE and its category</span><br>       <span class="hljs-keyword">else</span>:<br>           <span class="hljs-keyword">if</span> temp_named_entity:<br>               named_entities.append(temp_named_entity)<br>               temp_entity_name = <span class="hljs-string">&#x27;&#x27;</span><br>               temp_named_entity = <span class="hljs-literal">None</span><br><br><span class="hljs-comment"># get unique named entities</span><br>named_entities = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(named_entities))<br><span class="hljs-comment"># store named entities in a data frame</span><br>entity_frame = pd.DataFrame(named_entities, columns=[<span class="hljs-string">&#x27;Entity Name&#x27;</span>, <span class="hljs-string">&#x27;Entity Type&#x27;</span>])<br><span class="hljs-comment"># display results</span><br><span class="hljs-built_in">print</span>(entity_frame)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">                Entity Name   Entity <span class="hljs-keyword">Type</span><br><span class="hljs-number">0</span>                      <span class="hljs-number">1904</span>          <span class="hljs-keyword">DATE</span><br><span class="hljs-number">1</span>                   Denmark      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">2</span>                     Spain      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">3</span>   North &amp; Central America  ORGANIZATION<br><span class="hljs-number">4</span>             South America      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">5</span>                   Belgium      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">6</span>                    Zürich      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">7</span>           the Netherlands      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">8</span>                    France      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">9</span>                 Caribbean      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">10</span>                   Sweden      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">11</span>                  Oceania      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">12</span>                     Asia      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">13</span>                     FIFA  ORGANIZATION<br><span class="hljs-number">14</span>                   Europe      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">15</span>                   Africa      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">16</span>              Switzerland      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">17</span>                  Germany      LOCATION<br></code></pre></td></tr></table></figure><p>可以看到，在StanfordNER的帮助下，NER的实现效果较好，将Africa识别为LOCATION，将1904识别为时间（这在NLTK中没有识别出来），但还是对North&amp; Central America识别有误，将其识别为ORGANIZATION。值得注意的是，并不是说Stanford NER一定会比NLTKNER的效果好，两者针对的对象，预料，算法可能有差异，因此，需要根据自己的需求决定使用什么工具。本次分享到此结束，以后有机会的话，将会尝试着用HMM、CRF或深度学习来实现命名实体识别。</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>NER</tag>
      
      <tag>NLP工具</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（三）词形还原（Lemmatization）</title>
    <link href="/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F%EF%BC%88Lemmatization%EF%BC%89/"/>
    <url>/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F%EF%BC%88Lemmatization%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>词形还原（Lemmatization）是文本预处理中的重要部分，与词干提取（stemming）很相似。简单说来，词形还原就是去掉单词的词缀，提取单词的主干部分，通常提取后的单词会是字典中的单词，不同于词干提取（stemming），提取后的单词不一定会出现在单词中。比如，单词“cars”词形还原后的单词为“car”，单词“ate”词形还原后的单词为“eat”。在Python的nltk模块中，使用WordNet为我们提供了稳健的词形还原的函数。如以下示例Python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.stem <span class="hljs-keyword">import</span> WordNetLemmatizer<br><br>wnl = WordNetLemmatizer()<br><span class="hljs-comment"># lemmatize nouns</span><br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;cars&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>))<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;men&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>))<br><br><span class="hljs-comment"># lemmatize verbs</span><br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;running&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>))<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;ate&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>))<br><br><span class="hljs-comment"># lemmatize adjectives</span><br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;saddest&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>))<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;fancier&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>car men run eat sad fancy</p></blockquote><p>在以上代码中，wnl.lemmatize()函数可以进行词形还原，第一个参数为单词，第二个参数为该单词的词性，如名词，动词，形容词等，返回的结果为输入单词的词形还原后的结果。词形还原一般是简单的，但具体我们在使用时，指定单词的词性很重要，不然词形还原可能效果不好，如以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.stem <span class="hljs-keyword">import</span> WordNetLemmatizer<br><br>wnl = WordNetLemmatizer()<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;ate&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>))<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;fancier&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>ate fancier</p></blockquote><p>那么，如何获取单词的词性呢？在NLP中，使用Parts ofspeech（POS）技术实现。在nltk中，可以使用nltk.pos_tag()获取单词在句子中的词性，如以下Python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">sentence = <span class="hljs-string">&#x27;The brown fox is quick and he is jumping over the lazy dog&#x27;</span><br><span class="hljs-keyword">import</span> nltk<br>tokens = nltk.word_tokenize(sentence)<br>tagged_sent = nltk.pos_tag(tokens)<br><span class="hljs-built_in">print</span>(tagged_sent)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'),('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'),('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'),('dog', 'NN')]</p></blockquote><p>关于上述词性的说明，可以参考下表：</p><figure><img src="/img/nlp3_1.webp" alt="词性说明表1" /><figcaption aria-hidden="true">词性说明表1</figcaption></figure><figure><img src="/img/nlp3_2.webp" alt="词性说明表2" /><figcaption aria-hidden="true">词性说明表2</figcaption></figure><p>OK，知道了获取单词在句子中的词性，再结合词形还原，就能很好地完成词形还原功能。示例的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize, pos_tag<br><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> wordnet<br><span class="hljs-keyword">from</span> nltk.stem <span class="hljs-keyword">import</span> WordNetLemmatizer<br><br><span class="hljs-comment"># 获取单词的词性</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_wordnet_pos</span>(<span class="hljs-params">tag</span>):<br>    <span class="hljs-keyword">if</span> tag.startswith(<span class="hljs-string">&#x27;J&#x27;</span>):<br>        <span class="hljs-keyword">return</span> wordnet.ADJ<br>    <span class="hljs-keyword">elif</span> tag.startswith(<span class="hljs-string">&#x27;V&#x27;</span>):<br>        <span class="hljs-keyword">return</span> wordnet.VERB<br>    <span class="hljs-keyword">elif</span> tag.startswith(<span class="hljs-string">&#x27;N&#x27;</span>):<br>        <span class="hljs-keyword">return</span> wordnet.NOUN<br>    <span class="hljs-keyword">elif</span> tag.startswith(<span class="hljs-string">&#x27;R&#x27;</span>):<br>        <span class="hljs-keyword">return</span> wordnet.ADV<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>sentence = <span class="hljs-string">&#x27;football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal.&#x27;</span><br>tokens = word_tokenize(sentence)  <span class="hljs-comment"># 分词</span><br>tagged_sent = pos_tag(tokens)     <span class="hljs-comment"># 获取单词词性</span><br><br>wnl = WordNetLemmatizer()<br>lemmas_sent = []<br><span class="hljs-keyword">for</span> tag <span class="hljs-keyword">in</span> tagged_sent:<br>    wordnet_pos = get_wordnet_pos(tag[<span class="hljs-number">1</span>]) <span class="hljs-keyword">or</span> wordnet.NOUN<br>    lemmas_sent.append(wnl.lemmatize(tag[<span class="hljs-number">0</span>], pos=wordnet_pos)) <span class="hljs-comment"># 词形还原</span><br><br><span class="hljs-built_in">print</span>(lemmas_sent)<br><br></code></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>['football', 'be', 'a', 'family', 'of', 'team', 'sport', 'that','involve', ',', 'to', 'vary', 'degree', ',', 'kick', 'a', 'ball', 'to','score', 'a', 'goal', '.']</p></blockquote><p>输出的结果就是对句子中的单词进行词形还原后的结果。本次分享到此结束，欢迎大家交流~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>词形还原</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（二）探究TF-IDF的原理</title>
    <link href="/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%8C%EF%BC%89%E6%8E%A2%E7%A9%B6TF-IDF%E7%9A%84%E5%8E%9F%E7%90%86/"/>
    <url>/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%8C%EF%BC%89%E6%8E%A2%E7%A9%B6TF-IDF%E7%9A%84%E5%8E%9F%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h3 id="tf-idf介绍">TF-IDF介绍</h3><p>TF-IDF是NLP中一种常用的统计方法，用以评估一个字词对于一个文件集或一个语料库中的其中一份文件的重要程度，通常用于提取文本的特征，即关键词。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。在NLP中，TF-IDF的计算公式如下：</p><p><span class="math display">\[tfidf = tf*idf.\]</span></p><p>其中，tf是词频(Term Frequency)，idf为逆向文件频率(Inverse DocumentFrequency)。tf为词频，即一个词语在文档中的出现频率，假设一个词语在整个文档中出现了i次，而整个文档有N个词语，则tf的值为i/N.idf为逆向文件频率，假设整个文档有n篇文章，而一个词语在k篇文章中出现，则idf值为</p><p><span class="math display">\[idf=\log_{2}(\frac{n}{k}).\]</span></p><p>当然，不同地方的idf值计算公式会有稍微的不同。比如有些地方会在分母的k上加1，防止分母为0，还有些地方会让分子，分母都加上1，这是smoothing技巧。在本文中，还是采用最原始的idf值计算公式，因为这与gensim里面的计算公式一致。假设整个文档有D篇文章，则单词i在第j篇文章中的tfidf值为</p><figure><img src="/img/nlp2_1.webp" alt="gensim中tfidf的计算公式" /><figcaption aria-hidden="true">gensim中tfidf的计算公式</figcaption></figure><p>以上就是TF-IDF的计算方法。</p><h3 id="文本介绍及预处理">文本介绍及预处理</h3><p>我们将采用以下三个示例文本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">text1 =<span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal. </span><br><span class="hljs-string">Unqualified, the word football is understood to refer to whichever form of football is the most popular </span><br><span class="hljs-string">in the regional context in which the word appears. Sports commonly called football in certain places </span><br><span class="hljs-string">include association football (known as soccer in some countries); gridiron football (specifically American </span><br><span class="hljs-string">football or Canadian football); Australian rules football; rugby football (either rugby league or rugby union); </span><br><span class="hljs-string">and Gaelic football. These different variations of football are known as football codes.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>text2 = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Basketball is a team sport in which two teams of five players, opposing one another on a rectangular court, </span><br><span class="hljs-string">compete with the primary objective of shooting a basketball (approximately 9.4 inches (24 cm) in diameter) </span><br><span class="hljs-string">through the defender&#x27;s hoop (a basket 18 inches (46 cm) in diameter mounted 10 feet (3.048 m) high to a backboard </span><br><span class="hljs-string">at each end of the court) while preventing the opposing team from shooting through their own hoop. A field goal is </span><br><span class="hljs-string">worth two points, unless made from behind the three-point line, when it is worth three. After a foul, timed play stops </span><br><span class="hljs-string">and the player fouled or designated to shoot a technical foul is given one or more one-point free throws. The team with </span><br><span class="hljs-string">the most points at the end of the game wins, but if regulation play expires with the score tied, an additional period </span><br><span class="hljs-string">of play (overtime) is mandated.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>text3 = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Volleyball, game played by two teams, usually of six players on a side, in which the players use their hands to bat a </span><br><span class="hljs-string">ball back and forth over a high net, trying to make the ball touch the court within the opponents’ playing area before </span><br><span class="hljs-string">it can be returned. To prevent this a player on the opposing team bats the ball up and toward a teammate before it touches </span><br><span class="hljs-string">the court surface—that teammate may then volley it back across the net or bat it to a third teammate who volleys it across </span><br><span class="hljs-string">the net. A team is allowed only three touches of the ball before it must be returned over the net.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure><p>这三篇文章分别是关于足球，篮球，排球的介绍，它们组成一篇文档。接下来是文本的预处理部分。首先是对文本去掉换行符，然后是分句，分词，再去掉其中的标点，完整的Python代码如下，输入的参数为文章text:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> nltk<br><span class="hljs-keyword">import</span> string<br><br><span class="hljs-comment"># 文本预处理</span><br><span class="hljs-comment"># 函数：text文件分句，分词，并去掉标点</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_tokens</span>(<span class="hljs-params">text</span>):<br>    text = text.replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>    sents = nltk.sent_tokenize(text)  <span class="hljs-comment"># 分句</span><br>    tokens = []<br>    <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> nltk.word_tokenize(sent):  <span class="hljs-comment"># 分词</span><br>            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> string.punctuation: <span class="hljs-comment"># 去掉标点</span><br>                tokens.append(word)<br>    <span class="hljs-keyword">return</span> tokens<br></code></pre></td></tr></table></figure><p>接着，去掉文章中的通用词（stopwords），然后统计每个单词的出现次数，完整的Python代码如下，输入的参数为文章text:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords     <span class="hljs-comment">#停用词</span><br><br><span class="hljs-comment"># 对原始的text文件去掉停用词</span><br><span class="hljs-comment"># 生成count字典，即每个单词的出现次数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_count</span>(<span class="hljs-params">text</span>):<br>    tokens = get_tokens(text)<br>    filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> w <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)]    <span class="hljs-comment">#去掉停用词</span><br>    count = Counter(filtered)<br>    <span class="hljs-keyword">return</span> count<br></code></pre></td></tr></table></figure><p>以text3为例，生成的count字典如下：</p><blockquote><p>Counter({'ball': 4, 'net': 4, 'teammate': 3, 'returned': 2, 'bat': 2,'court': 2, 'team': 2, 'across': 2, 'touches': 2, 'back': 2, 'players':2, 'touch': 1, 'must': 1, 'usually': 1, 'side': 1, 'player': 1, 'area':1, 'Volleyball': 1, 'hands': 1, 'may': 1, 'toward': 1, 'A': 1, 'third':1, 'two': 1, 'six': 1, 'opposing': 1, 'within': 1, 'prevent': 1,'allowed': 1, '’': 1, 'playing': 1, 'played': 1, 'volley': 1,'surface—that': 1, 'volleys': 1, 'opponents': 1, 'use': 1, 'high': 1,'teams': 1, 'bats': 1, 'To': 1, 'game': 1, 'make': 1, 'forth': 1,'three': 1, 'trying': 1})</p></blockquote><h3 id="gensim中的tf-idf">Gensim中的TF-IDF</h3><p>对文本进行预处理后，对于以上三个示例文本，我们都会得到一个count字典，里面是每个文本中单词的出现次数。下面，我们将用gensim中的已实现的TF-IDF模型，来输出每篇文章中TF-IDF排名前三的单词及它们的tfidf值，完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords     <span class="hljs-comment">#停用词</span><br><span class="hljs-keyword">from</span> gensim <span class="hljs-keyword">import</span> corpora, models, matutils<br><br><span class="hljs-comment">#training by gensim&#x27;s Ifidf Model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_words</span>(<span class="hljs-params">text</span>):<br>    tokens = get_tokens(text)<br>    filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> w <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)]<br>    <span class="hljs-keyword">return</span> filtered<br><br><span class="hljs-comment"># get text</span><br>count1, count2, count3 = get_words(text1), get_words(text2), get_words(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-comment"># training by TfidfModel in gensim</span><br>dictionary = corpora.Dictionary(countlist)<br>new_dict = &#123;v:k <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> dictionary.token2id.items()&#125;<br>corpus2 = [dictionary.doc2bow(count) <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> countlist]<br>tfidf2 = models.TfidfModel(corpus2)<br>corpus_tfidf = tfidf2[corpus2]<br><br><span class="hljs-comment"># output</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nTraining by gensim Tfidf Model.......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, doc <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(corpus_tfidf):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    sorted_words = <span class="hljs-built_in">sorted</span>(doc, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    <span class="hljs-keyword">for</span> num, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(new_dict[num], <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Training</span> by gensim Tfidf Model.......<br><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">1</span><br>    <span class="hljs-attribute">Word</span>: football, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">84766</span><br>    <span class="hljs-attribute">Word</span>: rugby, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">21192</span><br>    <span class="hljs-attribute">Word</span>: known, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">14128</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">2</span><br>    <span class="hljs-attribute">Word</span>: play, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">29872</span><br>    <span class="hljs-attribute">Word</span>: cm, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">19915</span><br>    <span class="hljs-attribute">Word</span>: diameter, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">19915</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">3</span><br>    <span class="hljs-attribute">Word</span>: net, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">45775</span><br>    <span class="hljs-attribute">Word</span>: teammate, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">34331</span><br>    <span class="hljs-attribute">Word</span>: across, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">22888</span><br></code></pre></td></tr></table></figure><p>输出的结果还是比较符合我们的预期的，比如关于足球的文章中提取了football,rugby关键词，关于篮球的文章中提取了plat,cm关键词，关于排球的文章中提取了net, teammate关键词。</p><h3 id="自己动手实践tf-idf模型">自己动手实践TF-IDF模型</h3><p>有了以上我们对TF-IDF模型的理解，其实我们自己也可以动手实践一把，这是学习算法的最佳方式！以下是笔者实践TF-IDF的代码（接文本预处理代码）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><br><span class="hljs-comment"># 计算tf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tf</span>(<span class="hljs-params">word, count</span>):<br>    <span class="hljs-keyword">return</span> count[word] / <span class="hljs-built_in">sum</span>(count.values())<br><span class="hljs-comment"># 计算count_list有多少个文件包含word</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">n_containing</span>(<span class="hljs-params">word, count_list</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> count_list <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> count)<br><br><span class="hljs-comment"># 计算idf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">idf</span>(<span class="hljs-params">word, count_list</span>):<br>    <span class="hljs-keyword">return</span> math.log2(<span class="hljs-built_in">len</span>(count_list) / (n_containing(word, count_list)))    <span class="hljs-comment">#对数以2为底</span><br><span class="hljs-comment"># 计算tf-idf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tfidf</span>(<span class="hljs-params">word, count, count_list</span>):<br>    <span class="hljs-keyword">return</span> tf(word, count) * idf(word, count_list)<br><br><span class="hljs-comment"># TF-IDF测试</span><br>count1, count2, count3 = make_count(text1), make_count(text2), make_count(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training by original algorithm......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, count <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(countlist):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    scores = &#123;word: tfidf(word, count, countlist) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> count&#125;<br>    sorted_words = <span class="hljs-built_in">sorted</span>(scores.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    <span class="hljs-comment"># sorted_words = matutils.unitvec(sorted_words)</span><br>    <span class="hljs-keyword">for</span> word, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(word, <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Training</span> by original algorithm......<br><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">1</span><br>    <span class="hljs-attribute">Word</span>: football, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">30677</span><br>    <span class="hljs-attribute">Word</span>: rugby, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">07669</span><br>    <span class="hljs-attribute">Word</span>: known, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">05113</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">2</span><br>    <span class="hljs-attribute">Word</span>: play, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">05283</span><br>    <span class="hljs-attribute">Word</span>: inches, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">03522</span><br>    <span class="hljs-attribute">Word</span>: worth, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">03522</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">3</span><br>    <span class="hljs-attribute">Word</span>: net, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">10226</span><br>    <span class="hljs-attribute">Word</span>: teammate, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">07669</span><br>    <span class="hljs-attribute">Word</span>: across, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">05113</span><br></code></pre></td></tr></table></figure><p>可以看到，笔者自己动手实践的TF-IDF模型提取的关键词与gensim一致，至于篮球中为什么后两个单词不一致，是因为这些单词的tfidf一样，随机选择的结果不同而已。但是有一个问题，那就是计算得到的tfidf值不一样，这是什么原因呢？查阅gensim中计算tf-idf值的源代码（https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/tfidfmodel.py）：</p><figure><img src="/img/nlp2_2.webp" alt="TfidfModel类的参数" /><figcaption aria-hidden="true">TfidfModel类的参数</figcaption></figure><figure><img src="/img/nlp2_3.webp" alt="normalize参数的说明" /><figcaption aria-hidden="true">normalize参数的说明</figcaption></figure><p>也就是说，gensim对得到的tf-idf向量做了规范化（normalize），将其转化为单位向量。因此，我们需要在刚才的代码中加入规范化这一步，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 对向量做规范化, normalize</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">unitvec</span>(<span class="hljs-params">sorted_words</span>):<br>    lst = [item[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sorted_words]<br>    L2Norm = math.sqrt(<span class="hljs-built_in">sum</span>(np.array(lst)*np.array(lst)))<br>    unit_vector = [(item[<span class="hljs-number">0</span>], item[<span class="hljs-number">1</span>]/L2Norm) <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sorted_words]<br>    <span class="hljs-keyword">return</span> unit_vector<br><br><span class="hljs-comment"># TF-IDF测试</span><br>count1, count2, count3 = make_count(text1), make_count(text2), make_count(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training by original algorithm......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, count <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(countlist):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    scores = &#123;word: tfidf(word, count, countlist) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> count&#125;<br>    sorted_words = <span class="hljs-built_in">sorted</span>(scores.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    sorted_words = unitvec(sorted_words)   <span class="hljs-comment"># normalize</span><br>    <span class="hljs-keyword">for</span> word, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(word, <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Training</span> by original algorithm......<br><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">1</span><br>    <span class="hljs-attribute">Word</span>: football, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">84766</span><br>    <span class="hljs-attribute">Word</span>: rugby, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">21192</span><br>    <span class="hljs-attribute">Word</span>: known, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">14128</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">2</span><br>    <span class="hljs-attribute">Word</span>: play, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">29872</span><br>    <span class="hljs-attribute">Word</span>: shooting, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">19915</span><br>    <span class="hljs-attribute">Word</span>: diameter, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">19915</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">3</span><br>    <span class="hljs-attribute">Word</span>: net, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">45775</span><br>    <span class="hljs-attribute">Word</span>: teammate, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">34331</span><br>    <span class="hljs-attribute">Word</span>: back, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">22888</span><br></code></pre></td></tr></table></figure><p>现在的输出结果与gensim得到的结果一致！</p><h3 id="总结">总结</h3><p>Gensim是Python做NLP时鼎鼎大名的模块，有空还是多读读源码吧！以后，我们还会继续介绍TF-IDF在其它方面的应用，欢迎大家交流~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p><p>本文的完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> nltk<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> string<br><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords     <span class="hljs-comment">#停用词</span><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter       <span class="hljs-comment">#计数</span><br><span class="hljs-keyword">from</span> gensim <span class="hljs-keyword">import</span> corpora, models, matutils<br><br>text1 =<span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal. </span><br><span class="hljs-string">Unqualified, the word football is understood to refer to whichever form of football is the most popular </span><br><span class="hljs-string">in the regional context in which the word appears. Sports commonly called football in certain places </span><br><span class="hljs-string">include association football (known as soccer in some countries); gridiron football (specifically American </span><br><span class="hljs-string">football or Canadian football); Australian rules football; rugby football (either rugby league or rugby union); </span><br><span class="hljs-string">and Gaelic football. These different variations of football are known as football codes.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>text2 = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Basketball is a team sport in which two teams of five players, opposing one another on a rectangular court, </span><br><span class="hljs-string">compete with the primary objective of shooting a basketball (approximately 9.4 inches (24 cm) in diameter) </span><br><span class="hljs-string">through the defender&#x27;s hoop (a basket 18 inches (46 cm) in diameter mounted 10 feet (3.048 m) high to a backboard </span><br><span class="hljs-string">at each end of the court) while preventing the opposing team from shooting through their own hoop. A field goal is </span><br><span class="hljs-string">worth two points, unless made from behind the three-point line, when it is worth three. After a foul, timed play stops </span><br><span class="hljs-string">and the player fouled or designated to shoot a technical foul is given one or more one-point free throws. The team with </span><br><span class="hljs-string">the most points at the end of the game wins, but if regulation play expires with the score tied, an additional period </span><br><span class="hljs-string">of play (overtime) is mandated.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>text3 = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Volleyball, game played by two teams, usually of six players on a side, in which the players use their hands to bat a </span><br><span class="hljs-string">ball back and forth over a high net, trying to make the ball touch the court within the opponents’ playing area before </span><br><span class="hljs-string">it can be returned. To prevent this a player on the opposing team bats the ball up and toward a teammate before it touches </span><br><span class="hljs-string">the court surface—that teammate may then volley it back across the net or bat it to a third teammate who volleys it across </span><br><span class="hljs-string">the net. A team is allowed only three touches of the ball before it must be returned over the net.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># 文本预处理</span><br><span class="hljs-comment"># 函数：text文件分句，分词，并去掉标点</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_tokens</span>(<span class="hljs-params">text</span>):<br>    text = text.replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>    sents = nltk.sent_tokenize(text)  <span class="hljs-comment"># 分句</span><br>    tokens = []<br>    <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> nltk.word_tokenize(sent):  <span class="hljs-comment"># 分词</span><br>            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> string.punctuation: <span class="hljs-comment"># 去掉标点</span><br>                tokens.append(word)<br>    <span class="hljs-keyword">return</span> tokens<br><br><span class="hljs-comment"># 对原始的text文件去掉停用词</span><br><span class="hljs-comment"># 生成count字典，即每个单词的出现次数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_count</span>(<span class="hljs-params">text</span>):<br>    tokens = get_tokens(text)<br>    filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> w <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)]    <span class="hljs-comment">#去掉停用词</span><br>    count = Counter(filtered)<br>    <span class="hljs-keyword">return</span> count<br><br><span class="hljs-comment"># 计算tf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tf</span>(<span class="hljs-params">word, count</span>):<br>    <span class="hljs-keyword">return</span> count[word] / <span class="hljs-built_in">sum</span>(count.values())<br><span class="hljs-comment"># 计算count_list有多少个文件包含word</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">n_containing</span>(<span class="hljs-params">word, count_list</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> count_list <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> count)<br><br><span class="hljs-comment"># 计算idf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">idf</span>(<span class="hljs-params">word, count_list</span>):<br>    <span class="hljs-keyword">return</span> math.log2(<span class="hljs-built_in">len</span>(count_list) / (n_containing(word, count_list)))    <span class="hljs-comment">#对数以2为底</span><br><span class="hljs-comment"># 计算tf-idf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tfidf</span>(<span class="hljs-params">word, count, count_list</span>):<br>    <span class="hljs-keyword">return</span> tf(word, count) * idf(word, count_list)<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 对向量做规范化, normalize</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">unitvec</span>(<span class="hljs-params">sorted_words</span>):<br>    lst = [item[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sorted_words]<br>    L2Norm = math.sqrt(<span class="hljs-built_in">sum</span>(np.array(lst)*np.array(lst)))<br>    unit_vector = [(item[<span class="hljs-number">0</span>], item[<span class="hljs-number">1</span>]/L2Norm) <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sorted_words]<br>    <span class="hljs-keyword">return</span> unit_vector<br><br><span class="hljs-comment"># TF-IDF测试</span><br>count1, count2, count3 = make_count(text1), make_count(text2), make_count(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training by original algorithm......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, count <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(countlist):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    scores = &#123;word: tfidf(word, count, countlist) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> count&#125;<br>    sorted_words = <span class="hljs-built_in">sorted</span>(scores.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    sorted_words = unitvec(sorted_words)   <span class="hljs-comment"># normalize</span><br>    <span class="hljs-keyword">for</span> word, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(word, <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br><br><span class="hljs-comment">#training by gensim&#x27;s Ifidf Model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_words</span>(<span class="hljs-params">text</span>):<br>    tokens = get_tokens(text)<br>    filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> w <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)]<br>    <span class="hljs-keyword">return</span> filtered<br><br><span class="hljs-comment"># get text</span><br>count1, count2, count3 = get_words(text1), get_words(text2), get_words(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-comment"># training by TfidfModel in gensim</span><br>dictionary = corpora.Dictionary(countlist)<br>new_dict = &#123;v:k <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> dictionary.token2id.items()&#125;<br>corpus2 = [dictionary.doc2bow(count) <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> countlist]<br>tfidf2 = models.TfidfModel(corpus2)<br>corpus_tfidf = tfidf2[corpus2]<br><br><span class="hljs-comment"># output</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nTraining by gensim Tfidf Model.......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, doc <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(corpus_tfidf):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    sorted_words = <span class="hljs-built_in">sorted</span>(doc, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    <span class="hljs-keyword">for</span> num, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(new_dict[num], <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br>        <br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">输出结果：</span><br><span class="hljs-string"></span><br><span class="hljs-string">Training by original algorithm......</span><br><span class="hljs-string"></span><br><span class="hljs-string">Top words in document 1</span><br><span class="hljs-string">    Word: football, TF-IDF: 0.84766</span><br><span class="hljs-string">    Word: rugby, TF-IDF: 0.21192</span><br><span class="hljs-string">    Word: word, TF-IDF: 0.14128</span><br><span class="hljs-string">Top words in document 2</span><br><span class="hljs-string">    Word: play, TF-IDF: 0.29872</span><br><span class="hljs-string">    Word: inches, TF-IDF: 0.19915</span><br><span class="hljs-string">    Word: points, TF-IDF: 0.19915</span><br><span class="hljs-string">Top words in document 3</span><br><span class="hljs-string">    Word: net, TF-IDF: 0.45775</span><br><span class="hljs-string">    Word: teammate, TF-IDF: 0.34331</span><br><span class="hljs-string">    Word: bat, TF-IDF: 0.22888</span><br><span class="hljs-string"></span><br><span class="hljs-string">Training by gensim Tfidf Model.......</span><br><span class="hljs-string"></span><br><span class="hljs-string">Top words in document 1</span><br><span class="hljs-string">    Word: football, TF-IDF: 0.84766</span><br><span class="hljs-string">    Word: rugby, TF-IDF: 0.21192</span><br><span class="hljs-string">    Word: known, TF-IDF: 0.14128</span><br><span class="hljs-string">Top words in document 2</span><br><span class="hljs-string">    Word: play, TF-IDF: 0.29872</span><br><span class="hljs-string">    Word: cm, TF-IDF: 0.19915</span><br><span class="hljs-string">    Word: diameter, TF-IDF: 0.19915</span><br><span class="hljs-string">Top words in document 3</span><br><span class="hljs-string">    Word: net, TF-IDF: 0.45775</span><br><span class="hljs-string">    Word: teammate, TF-IDF: 0.34331</span><br><span class="hljs-string">    Word: across, TF-IDF: 0.22888</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>TF-IDF</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（一）词袋模型及句子相似度</title>
    <link href="/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    <url>/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6/</url>
    
    <content type="html"><![CDATA[<p>本文作为笔者NLP入门系列文章第一篇，以后我们就要步入NLP时代。本文将会介绍NLP中常见的词袋模型（Bag ofWords）以及如何利用词袋模型来计算句子间的相似度（余弦相似度，cosinesimilarity）。首先，让我们来看一下，什么是词袋模型。我们以下面两个简单句子为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">sent1 = <span class="hljs-string">&quot;I love sky, I love sea.&quot;</span><br>sent2 = <span class="hljs-string">&quot;I like running, I love reading.&quot;</span><br></code></pre></td></tr></table></figure><p>通常，NLP无法一下子处理完整的段落或句子，因此，第一步往往是分句和分词。这里只有句子，因此我们只需要分词即可。对于英语句子，可以使用NLTK中的word_tokenize函数，对于中文句子，则可使用jieba模块。故第一步为分词，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize<br>sents = [sent1, sent2]<br>texts = [[word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_tokenize(sent)] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents]<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scheme">[[<span class="hljs-symbol">&#x27;I</span>&#x27;, <span class="hljs-symbol">&#x27;love</span>&#x27;, <span class="hljs-symbol">&#x27;sky</span>&#x27;, &#x27;,&#x27;, <span class="hljs-symbol">&#x27;I</span>&#x27;, <span class="hljs-symbol">&#x27;love</span>&#x27;, <span class="hljs-symbol">&#x27;sea</span>&#x27;, <span class="hljs-symbol">&#x27;.</span>&#x27;], [<span class="hljs-symbol">&#x27;I</span>&#x27;, <span class="hljs-symbol">&#x27;like</span>&#x27;, <span class="hljs-symbol">&#x27;running</span>&#x27;, &#x27;,&#x27;, <span class="hljs-symbol">&#x27;I</span>&#x27;, <span class="hljs-symbol">&#x27;love</span>&#x27;, <span class="hljs-symbol">&#x27;reading</span>&#x27;, <span class="hljs-symbol">&#x27;.</span>&#x27;]]<br></code></pre></td></tr></table></figure><p>分词完毕。下一步是构建语料库，即所有句子中出现的单词及标点。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">all_list = []<br><span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>    all_list += text<br>corpus = <span class="hljs-built_in">set</span>(all_list)<br><span class="hljs-built_in">print</span>(corpus)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">&#123;&#x27;love&#x27;, &#x27;running&#x27;, &#x27;reading&#x27;, &#x27;sky&#x27;, &#x27;.&#x27;, &#x27;I&#x27;, &#x27;like&#x27;, &#x27;sea&#x27;, &#x27;,&#x27;&#125;<br></code></pre></td></tr></table></figure><p>可以看到，语料库中一共是8个单词及标点。接下来，对语料库中的单词及标点建立数字映射，便于后续的句子的向量表示。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">corpus_dict = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(corpus, <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(corpus))))<br><span class="hljs-built_in">print</span>(corpus_dict)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">&#123;&#x27;running&#x27;: <span class="hljs-number">1</span>, &#x27;reading&#x27;: <span class="hljs-number">2</span>, &#x27;love&#x27;: <span class="hljs-number">0</span>, &#x27;sky&#x27;: <span class="hljs-number">3</span>, &#x27;.&#x27;: <span class="hljs-number">4</span>, &#x27;I&#x27;: <span class="hljs-number">5</span>, &#x27;like&#x27;: <span class="hljs-number">6</span>, &#x27;sea&#x27;: <span class="hljs-number">7</span>, &#x27;,&#x27;: <span class="hljs-number">8</span>&#125;<br></code></pre></td></tr></table></figure><p>虽然单词及标点并没有按照它们出现的顺序来建立数字映射，不过这并不会影响句子的向量表示及后续的句子间的相似度。下一步，也就是词袋模型的关键一步，就是建立句子的向量表示。这个表示向量并不是简单地以单词或标点出现与否来选择0，1数字，而是把单词或标点的出现频数作为其对应的数字表示，结合刚才的语料库字典，句子的向量表示的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 建立句子的向量表示</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">vector_rep</span>(<span class="hljs-params">text, corpus_dict</span>):<br>    vec = []<br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> corpus_dict.keys():<br>        <span class="hljs-keyword">if</span> key <span class="hljs-keyword">in</span> text:<br>            vec.append((corpus_dict[key], text.count(key)))<br>        <span class="hljs-keyword">else</span>:<br>            vec.append((corpus_dict[key], <span class="hljs-number">0</span>))<br><br>    vec = <span class="hljs-built_in">sorted</span>(vec, key= <span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">0</span>])<br><br>    <span class="hljs-keyword">return</span> vec<br><br>vec1 = vector_rep(texts[<span class="hljs-number">0</span>], corpus_dict)<br>vec2 = vector_rep(texts[<span class="hljs-number">1</span>], corpus_dict)<br><span class="hljs-built_in">print</span>(vec1)<br><span class="hljs-built_in">print</span>(vec2)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scheme">[(<span class="hljs-name">0</span>, <span class="hljs-number">2</span>), (<span class="hljs-name">1</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">2</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">3</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">4</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">5</span>, <span class="hljs-number">2</span>), (<span class="hljs-name">6</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">7</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">8</span>, <span class="hljs-number">1</span>)]<br>[(<span class="hljs-name">0</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">1</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">2</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">3</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">4</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">5</span>, <span class="hljs-number">2</span>), (<span class="hljs-name">6</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">7</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">8</span>, <span class="hljs-number">1</span>)]<br></code></pre></td></tr></table></figure><p>让我们稍微逗留一会儿，来看看这个向量。在第一句中I出现了两次，在预料库字典中，I对应的数字为5，因此在第一句中5出现2次，在列表中的元组即为(5,2)，代表单词I在第一句中出现了2次。以上的输出可能并不那么直观，真实的两个句子的代表向量应为：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-string">[2, 0, 0, 1, 1, 2, 0, 1, 1]</span><br><span class="hljs-string">[1, 1, 1, 0, 1, 2, 1, 0, 1]</span><br></code></pre></td></tr></table></figure><p>OK，词袋模型到此结束。接下来，我们会利用刚才得到的词袋模型，即两个句子的向量表示，来计算相似度。在NLP中，如果得到了两个句子的向量表示，那么，一般会选择用余弦相似度作为它们的相似度，而向量的余弦相似度即为两个向量的夹角的余弦值。其计算的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> sqrt<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">similarity_with_2_sents</span>(<span class="hljs-params">vec1, vec2</span>):<br>    inner_product = <span class="hljs-number">0</span><br>    square_length_vec1 = <span class="hljs-number">0</span><br>    square_length_vec2 = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> tup1, tup2 <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(vec1, vec2):<br>        inner_product += tup1[<span class="hljs-number">1</span>]*tup2[<span class="hljs-number">1</span>]<br>        square_length_vec1 += tup1[<span class="hljs-number">1</span>]**<span class="hljs-number">2</span><br>        square_length_vec2 += tup2[<span class="hljs-number">1</span>]**<span class="hljs-number">2</span><br><br>    <span class="hljs-keyword">return</span> (inner_product/sqrt(square_length_vec1*square_length_vec2))<br><br><br>cosine_sim = similarity_with_2_sents(vec1, vec2)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;两个句子的余弦相似度为： %.4f。&#x27;</span>%cosine_sim)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">两个句子的余弦相似度为： 0.7303。<br></code></pre></td></tr></table></figure><p>这样，我们就通过句子的词袋模型，得到了它们间的句子相似度。当然，在实际的NLP项目中，如果需要计算两个句子的相似度，我们只需调用gensim模块即可，它是NLP的利器，能够帮助我们处理很多NLP任务。下面为用gensim计算两个句子的相似度的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">sent1 = <span class="hljs-string">&quot;I love sky, I love sea.&quot;</span><br>sent2 = <span class="hljs-string">&quot;I like running, I love reading.&quot;</span><br><br><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize<br>sents = [sent1, sent2]<br>texts = [[word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_tokenize(sent)] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents]<br><span class="hljs-built_in">print</span>(texts)<br><br><span class="hljs-keyword">from</span> gensim <span class="hljs-keyword">import</span> corpora<br><span class="hljs-keyword">from</span> gensim.similarities <span class="hljs-keyword">import</span> Similarity<br><br><span class="hljs-comment">#  语料库</span><br>dictionary = corpora.Dictionary(texts)<br><br><span class="hljs-comment"># 利用doc2bow作为词袋模型</span><br>corpus = [dictionary.doc2bow(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts]<br>similarity = Similarity(<span class="hljs-string">&#x27;-Similarity-index&#x27;</span>, corpus, num_features=<span class="hljs-built_in">len</span>(dictionary))<br><span class="hljs-built_in">print</span>(similarity)<br><span class="hljs-comment"># 获取句子的相似度</span><br>new_sensence = sent1<br>test_corpus_1 = dictionary.doc2bow(word_tokenize(new_sensence))<br><br>cosine_sim = similarity[test_corpus_1][<span class="hljs-number">1</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;利用gensim计算得到两个句子的相似度： %.4f。&quot;</span>%cosine_sim)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs delphi">[[<span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;sky&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;sea&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>], [<span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;like&#x27;</span>, <span class="hljs-string">&#x27;running&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;reading&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]]<br>Similarity <span class="hljs-keyword">index</span> <span class="hljs-keyword">with</span> <span class="hljs-number">2</span> documents <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> shards (<span class="hljs-keyword">stored</span> under -Similarity-<span class="hljs-keyword">index</span>)<br>利用gensim计算得到两个句子的相似度： <span class="hljs-number">0.7303</span>。<br></code></pre></td></tr></table></figure><p>注意，如果在运行代码时出现以下warning:</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">gensim\utils.py:<span class="hljs-number">1209</span>: UserWarning: detected Windows; aliasing chunkize <span class="hljs-keyword">to</span> chunkize_serial<br>  warnings.warn(&quot;detected Windows; aliasing chunkize to chunkize_serial&quot;)<br><br>gensim\matutils.py:<span class="hljs-number">737</span>: FutureWarning: <span class="hljs-keyword">Conversion</span> <span class="hljs-keyword">of</span> the second argument <span class="hljs-keyword">of</span> issubdtype <span class="hljs-keyword">from</span> `<span class="hljs-type">int</span>` <span class="hljs-keyword">to</span> `np.signedinteger` <span class="hljs-keyword">is</span> deprecated. <span class="hljs-keyword">In</span> future, it will be treated <span class="hljs-keyword">as</span> `np.int32 == np.dtype(<span class="hljs-type">int</span>).<span class="hljs-keyword">type</span>`.<br>  <span class="hljs-keyword">if</span> np.issubdtype(vec.dtype, np.int):<br></code></pre></td></tr></table></figure><p>如果想要去掉这些warning，则在导入gensim模块的代码前添加以下代码即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> warnings<br>warnings.filterwarnings(action=<span class="hljs-string">&#x27;ignore&#x27;</span>,category=UserWarning,module=<span class="hljs-string">&#x27;gensim&#x27;</span>)<br>warnings.filterwarnings(action=<span class="hljs-string">&#x27;ignore&#x27;</span>,category=FutureWarning,module=<span class="hljs-string">&#x27;gensim&#x27;</span>)<br></code></pre></td></tr></table></figure><p>本文到此结束，感谢阅读！如果不当之处，请速联系笔者，欢迎大家交流！祝您好运~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape），欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>词袋模型</tag>
      
      <tag>句子相似度</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>技术文章写作计划</title>
    <link href="/2023/07/06/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0%E5%86%99%E4%BD%9C%E8%AE%A1%E5%88%92/"/>
    <url>/2023/07/06/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0%E5%86%99%E4%BD%9C%E8%AE%A1%E5%88%92/</url>
    
    <content type="html"><![CDATA[<ul class="task-list"><li><label><input type="checkbox"checked="" />滑动验证码的识别</label></li><li><label><input type="checkbox"checked="" />滑动验证码的获取</label></li><li><label><input type="checkbox" />点选验证码的识别</label></li><li><label><input type="checkbox" />ELK简单搭建的demo</label></li><li><label><input type="checkbox" />文本聚类</label></li><li><label><input type="checkbox" />智能问答</label></li><li><label><input type="checkbox" />车牌的识别</label></li><li><label><input type="checkbox"checked="" />个人足迹地图（WEB服务）</label></li><li><label><input type="checkbox" checked="" />别名发现系统</label></li><li><label><input type="checkbox" />读取doc和docx文档</label></li><li><label><input type="checkbox"checked="" />利用celery实现定时任务</label></li><li><label><input type="checkbox"checked="" />文本标注工具Doccano</label></li><li><label><input type="checkbox"checked="" />利用Conda创建Python虚拟环境</label></li><li><label><input type="checkbox"checked="" />利用SFTP连接Linux服务器并上传、下载文件</label></li><li><label><input type="checkbox" checked="" />Flask学习之RESTfulAPI</label></li><li><label><input type="checkbox" />Flask学习之JWT认证</label></li><li><label><input type="checkbox" checked="" />BSON文件读取</label></li><li><label><inputtype="checkbox" />Flask学习之Flask-SQLALCHEMY</label></li><li><label><input type="checkbox"checked="" />设计模式（完成三篇：单例模式、工厂模式、监听模式）</label></li><li><label><input type="checkbox" />Redis</label></li><li><label><input type="checkbox" />supervisor使用</label></li><li><label><input type="checkbox"checked="" />tornado之文件下载（包含中文文件下载）</label></li><li><label><input type="checkbox"checked="" />利用CRF实现中文分词</label></li><li><label><input type="checkbox"checked="" />利用CRF实现模型预测</label></li><li><label><input type="checkbox"checked="" />protobuf的初次使用</label></li><li><label><inputtype="checkbox" />更新tensorflow/serving中的models.config文件中的model_version_policy</label></li><li><label><input type="checkbox"checked="" />tensorflow同时使用多个session</label></li><li><label><input type="checkbox"checked="" />如何离线安装tensorflow模块</label></li><li><label><input type="checkbox"checked="" />tensorboard查看ckpt和pb文件模型</label></li><li><label><input type="checkbox"checked="" />将ckpt转化为pb文件</label></li><li><label><input type="checkbox"checked="" />tensorflow/serving之BERT模型部署和预测</label></li><li><label><input type="checkbox"checked="" />tensorflow/serving实现模型部署</label></li><li><label><input type="checkbox" /><span class="citation"data-cites="property">@property</span></label></li><li><label><input type="checkbox" />tf_record</label></li><li><label><input type="checkbox" />指代关系抽取</label></li><li><label><inputtype="checkbox" />实体链接（百度实体链接比赛、武器装备知识图谱）</label></li><li><label><input type="checkbox"checked="" />文本多分类BERT微调</label></li><li><label><input type="checkbox"checked="" />文本多标签分类BERT微调</label></li><li><label><input type="checkbox"checked="" />文本序列标注BERT微调</label></li><li><label><input type="checkbox" checked="" />keras-bertEnglish系列（3个模型稍微调整即可）</label></li><li><label><input type="checkbox"checked="" />keras-bert调用ALBERT</label></li><li><label><input type="checkbox"checked="" />keras-bert模型部署</label></li><li><label><input type="checkbox"checked="" />h5文件转化为pb文件进行部署</label></li><li><label><input type="checkbox"checked="" />tensorflow/serving高效调用</label></li><li><label><inputtype="checkbox" />tensorflow_hub实现英文文本二分类</label></li><li><label><inputtype="checkbox" />tensorflow2.0和transformers实现文本多分类</label></li><li><label><input type="checkbox" />抽取式问答</label></li><li><label><input type="checkbox"checked="" />完形填空与文本纠错</label></li><li><label><inputtype="checkbox" />transformers实现中文序列标注</label></li><li><label><inputtype="checkbox" />tokenizers中的token使用方法</label></li><li><label><input type="checkbox" />BPE token 算法</label></li><li><label><input type="checkbox" checked="" />Keras:K折交叉验证</label></li><li><label><input type="checkbox"checked="" />使用Prothemus对tensorflow/serving进行服务监控</label></li><li><label><input type="checkbox"checked="" />seqeval获取序列标注实体识别结果</label></li><li><label><input type="checkbox" />ES进阶</label></li><li><label><input type="checkbox"checked="" />从荷兰国旗问题到快速排序</label></li><li><label><input type="checkbox" />中英文大模型调研</label></li><li><label><input type="checkbox" />LLaMA模型的介绍及其使用</label></li><li><label><input type="checkbox" />Fine-tune LLaMA模型</label></li><li><label><input type="checkbox"checked="" />OpenAI的tokenizer调研</label></li><li><label><input type="checkbox" checked="" />Gitlab CI/CD入门</label></li><li><label><input type="checkbox"checked="" />LangChain使用</label></li><li><label><input type="checkbox" checked="" />Flask部署</label></li><li><label><input type="checkbox" />LangChain构建阅读助手</label></li><li><label><inputtype="checkbox" />使用LoRA训练Flan-T5-XXL模型</label></li><li><label><inputtype="checkbox" />VSCode连接远程服务器进行开发</label></li></ul>]]></content>
    
    
    <categories>
      
      <category>写作计划</category>
      
    </categories>
    
    
    <tags>
      
      <tag>写作计划</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用Hexo+Github搭建个人博客网站</title>
    <link href="/2023/07/06/%E4%BD%BF%E7%94%A8Hexo-Github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/"/>
    <url>/2023/07/06/%E4%BD%BF%E7%94%A8Hexo-Github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/</url>
    
    <content type="html"><![CDATA[<p>曾几何时，笔者也幻想过写个项目来搭建属于自己的个人博客。但是，写程序以及维护的成本，不禁让我犹豫再三，最后还是选择了CSDN等博客网站。将近六年的博客生涯，我尝试了不同的博客网站，各有各的利和弊，不变的是广告，这让人很不爽。直到今天，我看到了别人写的利用Hexo+Github来搭建个人博客网站，如获至宝。折腾了一阵以后，轻松完成了个人博客的搭建，这种清爽的界面风格，让人耳目一新，同时它又是免费的，功能繁多的，便于维护的。下面，我将会介绍如何来使用Hexo+Github搭建个人博客网站。</p><h3 id="准备工作">准备工作</h3><p>为了顺利地完成个人博客网站的搭建，需要做以下准备工作：</p><ul><li>安装Git和NodeJs（版本为18.16.1）；</li><li>安装Hexo（命令为<code>npm i -g hexo</code>）;</li><li>Github账号</li></ul><h3 id="搭建博客">搭建博客</h3><p>下面将分步来介绍如何使用Hexo和Github来搭建个人博客网站。</p><h4 id="创建github仓库">创建Github仓库</h4><p>在Github中新建一个名为username.github.io的空仓库，其中username是你在GitHub上的用户名，比如笔者的仓库名为percent.github.io。</p><h4 id="配置ssh">配置SSH</h4><p>如果想要使用远程从你的电脑上传文件至你的github仓库，那么，你就需要配置SSH。点击你个人Github上的Settings选项，在<code>SSH and GPG keys</code>中配置SSH的公钥，一般公钥位于<code>.ssh/id_rsa.pub</code>中，如下图：<img src="/img/hexo1.png" alt="配置SSH" /></p><h4 id="博客初始化">博客初始化</h4><p>新建一个空的文件夹，比如笔者新建了文件夹<code>github_blog</code>，使用<code>hexo init</code>命令初始化博客。初始化后的文件夹结构如下：<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs 1c">.<br>├── _config.yml<br>├── package.json<br>├── scaffolds<br>├── source<br><span class="hljs-string">|   ├── _drafts</span><br><span class="hljs-string">|   └── _posts</span><br>└── themes<br></code></pre></td></tr></table></figure> 上述文件说明如下：</p><ul><li>_config.yml 网站的 配置 信息，您可以在此配置大部分的参数。</li><li>package.json：应用程序的信息。EJS, Stylus 和 Markdown renderer已默认安装，您可以自由移除。</li><li>scaffolds：模版文件夹。当您新建文章时，Hexo会根据 scaffold来建立文件。</li><li>source：资源文件夹是存放用户资源的地方。</li><li>themes：主题文件夹。Hexo 会根据主题来生成静态页面。</li></ul><h4 id="生成个人博客网站">生成个人博客网站</h4><p>配置_config.yml文件，配置信息如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># Deployment</span><br><span class="hljs-comment">## Docs: https://hexo.io/docs/deployment.html</span><br><span class="hljs-attr">deploy:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">git</span><br>  <span class="hljs-attr">repo:</span> <span class="hljs-string">https://github.com/percent4/percent4.github.io.git(第一步创建的Github仓库)</span><br>  <span class="hljs-attr">branch:</span> <span class="hljs-string">master</span><br></code></pre></td></tr></table></figure><p>安装插件<code>npm install hexo-deployer-git --save</code>后，运行如下命令：<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">hexo</span> clean<span class="hljs-comment"># 清除数据</span><br>hexo d -g<span class="hljs-comment"># 生成博客</span><br></code></pre></td></tr></table></figure>这时候，你会看到博客数据会提交至Github的信息，而第一步创建的空仓库也有了提交内容，当然，你的个人博客也搭建搭建完毕，访问网址为：https://username.github.io/，其中username是你在GitHub上的用户名。界面如下： <imgsrc="/img/hexo2.png" alt="Hexo界面" /></p><h3 id="博客维护">博客维护</h3><p>Hexo提供了一套维护博客的优雅的办法。笔者在此仅介绍如何新建一篇博客。新建博客格式为markdown格式，比如我想创建一篇名为<code>利用Tornado搭建文档预览系统</code>的博客，可以使用以下命令：</p><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs haxe">hexo <span class="hljs-keyword">new</span> <span class="hljs-type"></span>利用Tornado搭建文档预览系统<br></code></pre></td></tr></table></figure><p>这时候会在你当前目录下的source/_posts文件夹下生成<code>利用Tornado搭建文档预览系统.md</code>,其中内容如下：<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">利用Tornado搭建文档预览系统</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2020-06-09 18:32:29</span><br><span class="hljs-attr">tags:</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure>其中title为博客标题，date为博客时间，tags为博客标签。在<code>---</code>后面可以写博客正文的内容。写完博客后，使用命令 <figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">hexo</span> clean<span class="hljs-comment"># 清除数据</span><br>hexo d -g<span class="hljs-comment"># 生成博客</span><br></code></pre></td></tr></table></figure> 就会更新个人博客。</p><h3 id="更换主题">更换主题</h3><p>Hexo提供的默认主题为landscape,我们想替换主题为Fluid.Hexo替换主题为Fluid的步骤如下：</p><ol type="1"><li>通过<code>npm</code>直接安装，进入博客目录执行命令：<code>npm install --save hexo-theme-fluid</code></li><li>将node_modules文件夹下的hexo-theme-fluid复制到themes文件夹，并重名为fluid</li><li>在博客目录下创建_config.fluid.yml，将主题的_config.yml内容复制进去，并将<code>theme:</code>后面的主题修改为fluid</li><li>使用<code>hexo s</code>进行本地部署，如无问题，则使用命令<code>hexo d -g</code>进行远程部署</li></ol><h3 id="总结">总结</h3><p>当然，Hexo还提供了许多丰富的功能，比如theme（主题）的个性化定制等，这会使得你的博客内容更加丰富，功能更加完善。</p><p>笔者大家的个人博客网站为：<ahref="https://percent4.github.io/">https://percent4.github.io/</a>，欢迎大家访问。以后，笔者将会逐渐往个人博客网站倾斜，而减少使用公开的博客社区。</p>]]></content>
    
    
    <categories>
      
      <category>Hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
      <tag>Github</tag>
      
      <tag>个人博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何使用Hexo？</title>
    <link href="/2023/07/06/hello-world/"/>
    <url>/2023/07/06/hello-world/</url>
    
    <content type="html"><![CDATA[<p>欢迎来到 <a href="https://hexo.io/">Hexo</a>!这是我的第一篇博客。查阅 <ahref="https://hexo.io/docs/">documentation</a> 获取更多信息。如果你在使用Hexo遇到问题，你可以在这里找到答案 <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a>，或者你可以在这上面提问：<ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="快速开始">快速开始</h2><h3 id="创建一篇新的博客">创建一篇新的博客</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>更新信息: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="运行服务">运行服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>更新信息: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="产生静态文件">产生静态文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>更新信息: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="远程部署">远程部署</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>更新信息: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
