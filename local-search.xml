<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>BERT的几个可能的应用</title>
    <link href="/2023/07/08/BERT%E7%9A%84%E5%87%A0%E4%B8%AA%E5%8F%AF%E8%83%BD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
    <url>/2023/07/08/BERT%E7%9A%84%E5%87%A0%E4%B8%AA%E5%8F%AF%E8%83%BD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>BERT是谷歌公司于2018年11月发布的一款新模型，它一种预训练语言表示的方法，在大量文本语料（维基百科）上训练了一个通用的“语言理解”模型，然后用这个模型去执行想做的NLP任务。一经公布，它便引爆了整个NLP界，其在11个主流NLP任务中都取得优异的结果，因此成为NLP领域最吸引人的一个模型。简单来说，BERT就是在训练了大量的文本语料（无监督）之后，能够在对英语中的单词（或中文的汉字）给出一个向量表示，使得该单词（或汉字）具有一定的语义表示能力，因此，BERT具有一定的先验知识，在NLP任务中表现十分抢眼。</p><p>在文章<ahref="https://blog.csdn.net/Vancl_Wang/article/details/90349047">利用bert-serving-server搭建bert词向量服务(一)</a>中，作者简洁明了地介绍了如何利用bert-serving-server来获取中文汉字的词向量，这大大降低了一般从业者使用BERT的门槛。</p><p>结合笔者这段时间的工作体会以及思考，笔者尝试着给出BERT的几个可能的应用，如下：</p><ul><li>NLP基本任务</li><li>查找相似词语</li><li>提取文本中的实体</li><li>问答中的实体对齐</li></ul><p>由于笔者才疏学浅且撰写文章时间仓促，文章中有不足之处，请读者多多批评指正！</p><h3 id="nlp基本任务">NLP基本任务</h3><p>BERT公布已经半年多了，现在已经成为NLP中的深度学习模型中必不可少的工具，一般会加载在模型中的Embedding层。由于篇幅原因，笔者不再介绍自己的BERT项目，而是介绍几个BERT在基本任务中的Github项目：</p><ul><li>英语文本分类： <strong><ahref="https://github.com/Socialbird-AILab/BERT-Classification-Tutorial">BERT-Classification-Tutorial</a></strong></li><li>中文情感分类： <strong><ahref="https://github.com/renxingkai/BERT_Chinese_Classification">BERT_Chinese_Classification</a></strong></li><li>中文命名实体识别（NER）: <strong><ahref="https://github.com/yumath/bertNER">bertNER</a></strong></li></ul><p>可以看到，BERT已经广泛应用于NLP基本任务中，在开源项目中导出可以见到它的身影，并且这些项目的作者也写了非常细致的代码工程，便于上手。</p><p>在具体讲述下面的三个应用前，我们先了解下BERT应用的项目结构，如下：</p><p><imgsrc="https://img-blog.csdnimg.cn/20190607111211990.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pjbGlhbjkx,size_16,color_FFFFFF,t_70" /></p><p>其中，bert_client_lmj.py为调用BERT词向量服务，具体可参考文章<ahref="https://blog.csdn.net/Vancl_Wang/article/details/90349047">利用bert-serving-server搭建bert词向量服务(一)</a>，完整的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><span class="hljs-keyword">from</span> bert_serving.client <span class="hljs-keyword">import</span> BertClient<br><span class="hljs-keyword">from</span> sklearn.metrics.pairwise <span class="hljs-keyword">import</span> cosine_similarity<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoding</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.server_ip = <span class="hljs-string">&quot;127.0.0.1&quot;</span><br>        self.bert_client = BertClient(ip=self.server_ip)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, query</span>):<br>        tensor = self.bert_client.encode([query])<br>        <span class="hljs-keyword">return</span> tensor<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query_similarity</span>(<span class="hljs-params">self, query_list</span>):<br>        tensors = self.bert_client.encode(query_list)<br>        <span class="hljs-keyword">return</span> cosine_similarity(tensors)[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    ec = Encoding()<br>    <span class="hljs-built_in">print</span>(ec.encode(<span class="hljs-string">&quot;中国&quot;</span>).shape)<br>    <span class="hljs-built_in">print</span>(ec.encode(<span class="hljs-string">&quot;美国&quot;</span>).shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;中国和美国的向量相似度:&quot;</span>, ec.query_similarity([<span class="hljs-string">&quot;中国&quot;</span>, <span class="hljs-string">&quot;美国&quot;</span>]))<br></code></pre></td></tr></table></figure><h3 id="查找相似词语">查找相似词语</h3><p>利用词向量可以查找文章中与指定词语最相近的几个词语。具体的做法为：现将文章分词，对分词后的每个词，查询其与指定词语的相似度，最后按相似度输出词语即可。我们的示例文章为老舍的《养花》，内容如下：</p><blockquote><p>我爱花，所以也爱养花。我可还没成为养花专家，因为没有工夫去研究和试验。我只把养花当做生活中的一种乐趣，花开得大小好坏都不计较，只要开花，我就高兴。在我的小院子里，一到夏天满是花草，小猫只好上房去玩，地上没有它们的运动场。花虽然多，但是没有奇花异草。珍贵的花草不易养活，看着一棵好花生病要死，是件难过的事。北京的气候，对养花来说不算很好，冬天冷，春天多风，夏天不是干旱就是大雨倾盆，秋天最好，可是会忽然闹霜冻。在这种气候里，想把南方的好花养活，我还没有那么大的本事。因此，我只养些好种易活、自己会奋斗的花草。不过，尽管花草自己会奋斗，我若是置之不理，任其自生自灭，大半还是会死的。我得天天照管它们，像好朋友似的关心它们。一来二去，我摸着一些门道：有的喜阴，就别放在太阳地里；有的喜干，就别多浇水。摸着门道，花草养活了，而且三年五载老活着、开花，多么有意思啊！不是乱吹，这就是知识呀！多得些知识决不是坏事。我不是有腿病吗，不但不利于行，也不利于久坐。我不知道花草们受我的照顾，感谢我不感谢；我可得感谢它们。我工作的时候，我总是写一会儿就到院中去看看，浇浇这棵，搬搬那盆，然后回到屋里再写一会儿，然后再出去。如此循环，让脑力劳动和体力劳动得到适当的调节，有益身心，胜于吃药。要是赶上狂风暴雨或天气突变，就得全家动员，抢救花草，十分紧张。几百盆花，都要很快地抢到屋里去，使人腰酸腿疼，热汗直流。第二天，天气好了，又得把花都搬出去，就又一次腰酸腿疼，热汗直流。可是，这多么有意思呀！不劳动，连棵花也养不活，这难道不是真理吗？送牛奶的同志进门就夸“好香”，这使我们全家都感到骄傲。赶到昙花开放的时候，约几位朋友来看看，更有秉烛夜游的味道——昙花总在夜里开放。花分根了，一棵分为几棵，就赠给朋友们一些。看着友人拿走自己的劳动果实，心里自然特别欢喜。当然，也有伤心的时候，今年夏天就有这么一回。三百棵菊秧还在地上（没到移入盆中的时候），下了暴雨，邻家的墙倒了，菊秧被砸死三十多种，一百多棵。全家人几天都没有笑容。有喜有忧，有笑有泪，有花有果，有香有色，既须劳动，又长见识，这就是养花的乐趣。</p></blockquote><p>指定词语为“开心”，查询《养花》一文中与“开心”最为接近的5个词语，完整的Python代码如下：（find_similar_words.py）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">from</span> bert_client_lmj <span class="hljs-keyword">import</span> Encoding<br><span class="hljs-keyword">from</span> operator <span class="hljs-keyword">import</span> itemgetter<br><br><span class="hljs-comment"># 读取文章</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./doc.txt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    content = f.read().replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br><br>ec = Encoding()<br>similar_word_dict = &#123;&#125;<br><br><span class="hljs-comment"># 查找文章中与&#x27;开心&#x27;的最接近的词语</span><br>words = <span class="hljs-built_in">list</span>(jieba.cut(content))<br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>    <span class="hljs-built_in">print</span>(word)<br>    <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> similar_word_dict.keys():<br>        similar_word_dict[word] = ec.query_similarity([word, <span class="hljs-string">&#x27;开心&#x27;</span>])<br><br><span class="hljs-comment"># 按相似度从高到低排序</span><br>sorted_dict = <span class="hljs-built_in">sorted</span>(similar_word_dict.items(), key=itemgetter(<span class="hljs-number">1</span>), reverse=<span class="hljs-literal">True</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;与%s最接近的5个词语及相似度如下：&#x27;</span> % <span class="hljs-string">&#x27;开心&#x27;</span>)<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> sorted_dict[:<span class="hljs-number">5</span>]:<br>    <span class="hljs-built_in">print</span>(_)<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs 1c">与开心最接近的<span class="hljs-number">5</span>个词语及相似度如下：<br>(&#x27;难过&#x27;, <span class="hljs-number">0.9070794</span>)<br>(&#x27;高兴&#x27;, <span class="hljs-number">0.89517105</span>)<br>(&#x27;乐趣&#x27;, <span class="hljs-number">0.89260685</span>)<br>(&#x27;骄傲&#x27;, <span class="hljs-number">0.87363803</span>)<br>(&#x27;我爱花&#x27;, <span class="hljs-number">0.86954254</span>)<br></code></pre></td></tr></table></figure><h3 id="提取文本中的实体">提取文本中的实体</h3><p>在事件抽取中，我们往往需要抽取一些指定的元素，比如在下面的句子中，</p><blockquote><p>巴基斯坦当地时间2014年12月16日早晨，巴基斯坦塔利班运动武装分子袭击了西北部白沙瓦市一所军人子弟学校，打死141人，其中132人为12岁至16岁的学生。</p></blockquote><p>我们需要抽取袭击者，也就是恐怖组织这个元素。</p><p>直接从句法分析，也许可以得到一定的效果，但由于事件描述方式多变，句法分析会显得比较复杂且效果不一定能保证。这时候，我们尝试BERT词向量，它在一定程度上可以作为补充策略，帮助我们定位到事件的元素。具体的想法如下：</p><ul><li>指定事件元素模板</li><li>句子分词，对词语做n-gram</li><li>查询每个n-gram与模板的相似度</li><li>按相似度对n-gram排序，取相似度最高的n-gram</li></ul><p>在这里，我们的事件元素为恐怖组织，指定的模板为“伊斯兰组织”，完整的Python程序如下（find_similar_entity_in_sentence.py）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">from</span> operator <span class="hljs-keyword">import</span> itemgetter<br><span class="hljs-keyword">from</span> bert_client_lmj <span class="hljs-keyword">import</span> Encoding<br><br><span class="hljs-comment"># 创建n-gram</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_ngrams</span>(<span class="hljs-params">sequence, n</span>):<br>    lst = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(*[sequence[index:] <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)]))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(lst)):<br>        lst[i] = <span class="hljs-string">&#x27;&#x27;</span>.join(lst[i])<br>    <span class="hljs-keyword">return</span> lst<br><br><span class="hljs-comment"># 模板</span><br>template = <span class="hljs-string">&#x27;伊斯兰组织&#x27;</span><br><span class="hljs-comment"># 示例句子</span><br>doc = <span class="hljs-string">&quot;巴基斯坦当地时间2014年12月16日早晨，巴基斯坦塔利班运动武装分子袭击了西北部白沙瓦市一所军人子弟学校，打死141人，其中132人为12岁至16岁的学生。&quot;</span><br><br>words = <span class="hljs-built_in">list</span>(jieba.cut(doc))<br>all_lst = []<br><span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>):<br>    all_lst.extend(compute_ngrams(words, j))<br><br>ec = Encoding()<br>similar_word_dict = &#123;&#125;<br><br><span class="hljs-comment"># 查找文章中与template的最接近的词语</span><br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> all_lst:<br>    <span class="hljs-built_in">print</span>(word)<br>    <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> similar_word_dict.keys():<br>        similar_word_dict[word] = ec.query_similarity([word, template])<br><br><span class="hljs-comment"># 按相似度从高到低排序</span><br>sorted_dict = <span class="hljs-built_in">sorted</span>(similar_word_dict.items(), key=itemgetter(<span class="hljs-number">1</span>), reverse=<span class="hljs-literal">True</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;与%s最接近的实体是: %s，相似度为 %s.&#x27;</span> %(template, sorted_dict[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], sorted_dict[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]))<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dns">与伊斯兰组织最接近的实体是: 塔利班运动武装分子，相似度为 <span class="hljs-number">0.8953854</span>.<br></code></pre></td></tr></table></figure><p>可以看到，该算法成功地帮助我们定位到了恐怖组织：塔利班运动武装分子，效果很好，但是由于是无监督产生的词向量，效果不一定可控，而且该算法运行速度较慢，这点可以从工程上加以改进。</p><h3 id="问答中的实体对齐">问答中的实体对齐</h3><p>在智能问答中，我们往往会采用知识图谱或者数据库存储实体，其中一个难点就是实体对齐。举个例子，我们在数据库中储存的实体如下：（entities.txt）</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-number">094</span>型/晋级<br><span class="hljs-number">052</span>C型（旅洋Ⅱ级）<br>辽宁舰<span class="hljs-regexp">/瓦良格/</span>Varyag<br>杰拉尔德·R·福特号航空母舰<br><span class="hljs-number">052</span>D型（旅洋III级）<br><span class="hljs-number">054</span>A型<br>CVN-<span class="hljs-number">72</span><span class="hljs-regexp">/林肯号/</span>Lincoln<br></code></pre></td></tr></table></figure><p>这样的实体名字很复杂，如果用户想查询实体“辽宁舰”，就会碰到困难，但是由于实体以储存在数据库或知识图谱中，实体不好直接修改。一种办法是通过关键字匹配定位实体，在这里，我们可以借助BERT词向量来实现，完整的Python代码如下：（Entity_Alignment.py）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><span class="hljs-keyword">from</span> bert_client_lmj <span class="hljs-keyword">import</span> Encoding<br><span class="hljs-keyword">from</span> operator <span class="hljs-keyword">import</span> itemgetter<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;entities.txt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    entities = [_.strip() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> f.readlines()]<br><br>ec = Encoding()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">entity_alignment</span>(<span class="hljs-params">query</span>):<br><br>    similar_word_dict = &#123;&#125;<br><br>    <span class="hljs-comment"># 查找已有实体中与query最接近的实体</span><br>    <span class="hljs-keyword">for</span> entity <span class="hljs-keyword">in</span> entities:<br>        <span class="hljs-keyword">if</span> entity <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> similar_word_dict.keys():<br>            similar_word_dict[entity] = ec.query_similarity([entity, query])<br><br>    <span class="hljs-comment"># 按相似度从高到低排序</span><br>    sorted_dict = <span class="hljs-built_in">sorted</span>(similar_word_dict.items(), key=itemgetter(<span class="hljs-number">1</span>), reverse=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">return</span> sorted_dict[<span class="hljs-number">0</span>]<br><br>query = <span class="hljs-string">&#x27;辽宁舰&#x27;</span><br>result = entity_alignment(query)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;查询实体：%s，匹配实体：%s 。&#x27;</span> %(query, result))<br><br>query = <span class="hljs-string">&#x27;林肯号&#x27;</span><br>result = entity_alignment(query)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;查询实体：%s，匹配实体：%s 。&#x27;</span> %(query, result))<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs 1c">查询实体：辽宁舰，匹配实体：(&#x27;辽宁舰/瓦良格/Varyag&#x27;, <span class="hljs-number">0.8534695</span>) 。<br>查询实体：林肯号，匹配实体：(&#x27;CVN-72/林肯号/Lincoln&#x27;, <span class="hljs-number">0.8389378</span>) 。<br></code></pre></td></tr></table></figure><p>在这里，查询的速度应该不是困难，因为我们可以将已储存的实体以离线的方式查询其词向量并储存，这样进来一个查询到实体，只查询一次词向量，并计算其与离线的词向量的相似度。这种方法也存在缺陷，主要是由于词向量的无监督，实体对齐有时候不会很准，但作为一种补充策略，也许可以考虑。</p><h3 id="总结">总结</h3><p>本文介绍了笔者这段时间所思考的BERT词向量的几个应用，由于能力有限，文章中会存在考虑不当的地方，还请读者多多批评指正。</p><p>另外，笔者将会持续调研词向量方面的技术，比如腾讯词向量，百度词向量等，欢迎大家关注～</p><blockquote><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BERT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（十）使用LSTM进行文本情感分析</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%8D%81%EF%BC%89%E4%BD%BF%E7%94%A8LSTM%E8%BF%9B%E8%A1%8C%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%8D%81%EF%BC%89%E4%BD%BF%E7%94%A8LSTM%E8%BF%9B%E8%A1%8C%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h4 id="情感分析简介">情感分析简介</h4><p>文本情感分析（SentimentAnalysis）是自然语言处理（NLP）方法中常见的应用，也是一个有趣的基本任务，尤其是以提炼文本情绪内容为目的的分类。它是对带有情感色彩的主观性文本进行分析、处理、归纳和推理的过程。</p><p>本文将介绍情感分析中的情感极性（倾向）分析。所谓情感极性分析，指的是对文本进行褒义、贬义、中性的判断。在大多应用场景下，只分为两类。例如对于“喜爱”和“厌恶”这两个词，就属于不同的情感倾向。</p><p>本文将详细介绍如何使用深度学习模型中的LSTM模型来实现文本的情感分析。</p><h3 id="文本介绍及语料分析">文本介绍及语料分析</h3><p>我们以某电商网站中某个商品的评论作为语料（corpus.csv），该数据集的下载网址为：<ahref="https://github.com/renjunxiang/Text-Classification/blob/master/TextClassification/data/data_single.csv">https://github.com/renjunxiang/Text-Classification/blob/master/TextClassification/data/data_single.csv</a>，该数据集一共有4310条评论数据，文本的情感分为两类：“正面”和“反面”，该数据集的前几行如下：</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey"><span class="hljs-built_in">evaluation,</span>label<br>用了一段时间，感觉还不错，可以,正面<br>电视非常好，已经是家里的第二台了。第一天下单，第二天就到本地了，可是物流的人说车坏了，一直催，客服也帮着催，到第三天下午<span class="hljs-number">5</span>点才送过来。父母年纪大了，买个大电视画面清晰，趁着耳朵还好使，享受几年。,正面<br>电视比想象中的大好多，画面也很清晰，系统很智能，更多功能还在摸索中,正面<br>不错,正面<br>用了这么多天了，感觉还不错。夏普的牌子还是比较可靠。希望以后比较耐用，现在是考量质量的时候。,正面<br>物流速度很快，非常棒，今天就看了电视，非常清晰，非常流畅，一次非常完美的购物体验,正面<br>非常好，客服还特意打电话做回访,正面<br>物流小哥不错，辛苦了，东西还没用,正面<br>送货速度快，质量有保障，活动价格挺好的。希望用的久，不出问题。,正面<br></code></pre></td></tr></table></figure><p>接着我们需要对语料做一个简单的分析：</p><ul><li><p>数据集中的情感分布；</p></li><li><p>数据集中的评论句子长度分布。</p><p>使用以下Python脚本，我们可以统计出数据集中的情感分布以及评论句子长度分布。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> font_manager<br><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> accumulate<br><br><span class="hljs-comment"># 设置matplotlib绘图时的字体</span><br>my_font = font_manager.FontProperties(fname=<span class="hljs-string">&quot;/Library/Fonts/Songti.ttc&quot;</span>)<br><br><span class="hljs-comment"># 统计句子长度及长度出现的频数</span><br>df = pd.read_csv(<span class="hljs-string">&#x27;./corpus.csv&#x27;</span>)<br><span class="hljs-built_in">print</span>(df.groupby(<span class="hljs-string">&#x27;label&#x27;</span>)[<span class="hljs-string">&#x27;label&#x27;</span>].count())<br><br>df[<span class="hljs-string">&#x27;length&#x27;</span>] = df[<span class="hljs-string">&#x27;evaluation&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x))<br>len_df = df.groupby(<span class="hljs-string">&#x27;length&#x27;</span>).count()<br>sent_length = len_df.index.tolist()<br>sent_freq = len_df[<span class="hljs-string">&#x27;evaluation&#x27;</span>].tolist()<br><br><span class="hljs-comment"># 绘制句子长度及出现频数统计图</span><br>plt.bar(sent_length, sent_freq)<br>plt.title(<span class="hljs-string">&quot;句子长度及出现频数统计图&quot;</span>, fontproperties=my_font)<br>plt.xlabel(<span class="hljs-string">&quot;句子长度&quot;</span>, fontproperties=my_font)<br>plt.ylabel(<span class="hljs-string">&quot;句子长度出现的频数&quot;</span>, fontproperties=my_font)<br>plt.savefig(<span class="hljs-string">&quot;./句子长度及出现频数统计图.png&quot;</span>)<br>plt.close()<br><br><span class="hljs-comment"># 绘制句子长度累积分布函数(CDF)</span><br>sent_pentage_list = [(count/<span class="hljs-built_in">sum</span>(sent_freq)) <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> accumulate(sent_freq)]<br><br><span class="hljs-comment"># 绘制CDF</span><br>plt.plot(sent_length, sent_pentage_list)<br><br><span class="hljs-comment"># 寻找分位点为quantile的句子长度</span><br>quantile = <span class="hljs-number">0.91</span><br><span class="hljs-comment">#print(list(sent_pentage_list))</span><br><span class="hljs-keyword">for</span> length, per <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(sent_length, sent_pentage_list):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">round</span>(per, <span class="hljs-number">2</span>) == quantile:<br>        index = length<br>        <span class="hljs-keyword">break</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n分位点为%s的句子长度:%d.&quot;</span> % (quantile, index))<br><br><span class="hljs-comment"># 绘制句子长度累积分布函数图</span><br>plt.plot(sent_length, sent_pentage_list)<br>plt.hlines(quantile, <span class="hljs-number">0</span>, index, colors=<span class="hljs-string">&quot;c&quot;</span>, linestyles=<span class="hljs-string">&quot;dashed&quot;</span>)<br>plt.vlines(index, <span class="hljs-number">0</span>, quantile, colors=<span class="hljs-string">&quot;c&quot;</span>, linestyles=<span class="hljs-string">&quot;dashed&quot;</span>)<br>plt.text(<span class="hljs-number">0</span>, quantile, <span class="hljs-built_in">str</span>(quantile))<br>plt.text(index, <span class="hljs-number">0</span>, <span class="hljs-built_in">str</span>(index))<br>plt.title(<span class="hljs-string">&quot;句子长度累积分布函数图&quot;</span>, fontproperties=my_font)<br>plt.xlabel(<span class="hljs-string">&quot;句子长度&quot;</span>, fontproperties=my_font)<br>plt.ylabel(<span class="hljs-string">&quot;句子长度累积频率&quot;</span>, fontproperties=my_font)<br>plt.savefig(<span class="hljs-string">&quot;./句子长度累积分布函数图.png&quot;</span>)<br>plt.close()<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">label</span><br><span class="hljs-string">正面</span>    <span class="hljs-number">1908</span><br><span class="hljs-string">负面</span>    <span class="hljs-number">2375</span><br><span class="hljs-attr">Name:</span> <span class="hljs-string">label,</span> <span class="hljs-attr">dtype:</span> <span class="hljs-string">int64</span><br><br><span class="hljs-string">分位点为0.91的句子长度:183.</span><br></code></pre></td></tr></table></figure><p>可以看到，正反面两类情感的比例差不多。句子长度及出现频数统计图如下：</p><p><img src="/img/nlp10_1.png" /></p><p>句子长度累积分布函数图如下：</p><p><img src="/img/nlp10_2.png" /></p><p>可以看到，大多数样本的句子长度集中在1-200之间，句子长度累计频率取0.91分位点，则长度为183左右。</p><h3 id="使用lstm模型">使用LSTM模型</h3><p>接着我们使用深度学习中的LSTM模型来对上述数据集做情感分析，笔者实现的模型框架如下：</p><figure><img src="/img/nlp10_3.png" alt="模型结构图" /><figcaption aria-hidden="true">模型结构图</figcaption></figure><p>完整的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> np_utils, plot_model<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> LSTM, Dense, Embedding, Dropout<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><br><span class="hljs-comment"># 导入数据</span><br><span class="hljs-comment"># 文件的数据中，特征为evaluation, 类别为label.</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>(<span class="hljs-params">filepath, input_shape=<span class="hljs-number">20</span></span>):<br>    df = pd.read_csv(filepath)<br><br>    <span class="hljs-comment"># 标签及词汇表</span><br>    labels, vocabulary = <span class="hljs-built_in">list</span>(df[<span class="hljs-string">&#x27;label&#x27;</span>].unique()), <span class="hljs-built_in">list</span>(df[<span class="hljs-string">&#x27;evaluation&#x27;</span>].unique())<br><br>    <span class="hljs-comment"># 构造字符级别的特征</span><br>    string = <span class="hljs-string">&#x27;&#x27;</span><br>    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> vocabulary:<br>        string += word<br><br>    vocabulary = <span class="hljs-built_in">set</span>(string)<br><br>    <span class="hljs-comment"># 字典列表</span><br>    word_dictionary = &#123;word: i+<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocabulary)&#125;<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;word_dict.pk&#x27;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        pickle.dump(word_dictionary, f)<br>    inverse_word_dictionary = &#123;i+<span class="hljs-number">1</span>: word <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocabulary)&#125;<br>    label_dictionary = &#123;label: i <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels)&#125;<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;label_dict.pk&#x27;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        pickle.dump(label_dictionary, f)<br>    output_dictionary = &#123;i: labels <span class="hljs-keyword">for</span> i, labels <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels)&#125;<br><br>    vocab_size = <span class="hljs-built_in">len</span>(word_dictionary.keys()) <span class="hljs-comment"># 词汇表大小</span><br>    label_size = <span class="hljs-built_in">len</span>(label_dictionary.keys()) <span class="hljs-comment"># 标签类别数量</span><br><br>    <span class="hljs-comment"># 序列填充，按input_shape填充，长度不足的按0补充</span><br>    x = [[word_dictionary[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> df[<span class="hljs-string">&#x27;evaluation&#x27;</span>]]<br>    x = pad_sequences(maxlen=input_shape, sequences=x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br>    y = [[label_dictionary[sent]] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> df[<span class="hljs-string">&#x27;label&#x27;</span>]]<br>    y = [np_utils.to_categorical(label, num_classes=label_size) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> y]<br>    y = np.array([<span class="hljs-built_in">list</span>(_[<span class="hljs-number">0</span>]) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> y])<br><br>    <span class="hljs-keyword">return</span> x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary<br><br><span class="hljs-comment"># 创建深度学习模型， Embedding + LSTM + Softmax.</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_LSTM</span>(<span class="hljs-params">n_units, input_shape, output_dim, filepath</span>):<br>    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = load_data(filepath)<br>    model = Sequential()<br>    model.add(Embedding(input_dim=vocab_size + <span class="hljs-number">1</span>, output_dim=output_dim,<br>                        input_length=input_shape, mask_zero=<span class="hljs-literal">True</span>))<br>    model.add(LSTM(n_units, input_shape=(x.shape[<span class="hljs-number">0</span>], x.shape[<span class="hljs-number">1</span>])))<br>    model.add(Dropout(<span class="hljs-number">0.2</span>))<br>    model.add(Dense(label_size, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br>    model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>, optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>, metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br>    plot_model(model, to_file=<span class="hljs-string">&#x27;./model_lstm.png&#x27;</span>, show_shapes=<span class="hljs-literal">True</span>)<br>    model.summary()<br><br>    <span class="hljs-keyword">return</span> model<br><br><span class="hljs-comment"># 模型训练</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_train</span>(<span class="hljs-params">input_shape, filepath, model_save_path</span>):<br><br>    <span class="hljs-comment"># 将数据集分为训练集和测试集，占比为9:1</span><br>    <span class="hljs-comment"># input_shape = 100</span><br>    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = load_data(filepath, input_shape)<br>    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = <span class="hljs-number">0.1</span>, random_state = <span class="hljs-number">42</span>)<br><br>    <span class="hljs-comment"># 模型输入参数，需要自己根据需要调整</span><br>    n_units = <span class="hljs-number">100</span><br>    batch_size = <span class="hljs-number">32</span><br>    epochs = <span class="hljs-number">5</span><br>    output_dim = <span class="hljs-number">20</span><br><br>    <span class="hljs-comment"># 模型训练</span><br>    lstm_model = create_LSTM(n_units, input_shape, output_dim, filepath)<br>    lstm_model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 模型保存</span><br>    lstm_model.save(model_save_path)<br><br>    N = test_x.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 测试的条数</span><br>    predict = []<br>    label = []<br>    <span class="hljs-keyword">for</span> start, end <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, N, <span class="hljs-number">1</span>), <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, N+<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)):<br>        sentence = [inverse_word_dictionary[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> test_x[start] <span class="hljs-keyword">if</span> i != <span class="hljs-number">0</span>]<br>        y_predict = lstm_model.predict(test_x[start:end])<br>        label_predict = output_dictionary[np.argmax(y_predict[<span class="hljs-number">0</span>])]<br>        label_true = output_dictionary[np.argmax(test_y[start:end])]<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#x27;</span>.join(sentence), label_true, label_predict) <span class="hljs-comment"># 输出预测结果</span><br>        predict.append(label_predict)<br>        label.append(label_true)<br><br>    acc = accuracy_score(predict, label) <span class="hljs-comment"># 预测准确率</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;模型在测试集上的准确率为: %s.&#x27;</span> % acc)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    filepath = <span class="hljs-string">&#x27;./corpus.csv&#x27;</span><br>    input_shape = <span class="hljs-number">180</span><br>    model_save_path = <span class="hljs-string">&#x27;./corpus_model.h5&#x27;</span><br>    model_train(input_shape, filepath, model_save_path)<br></code></pre></td></tr></table></figure><p>对上述模型，共训练5次，训练集和测试集比例为9:1，输出的结果为：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">......</span><br><span class="hljs-string">Epoch</span> <span class="hljs-number">5</span><span class="hljs-string">/5</span><br><span class="hljs-string">......</span><br><span class="hljs-number">3424</span><span class="hljs-string">/3854</span> [<span class="hljs-string">=========================&gt;....</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 2s - loss: 0.1280 - acc:</span> <span class="hljs-number">0.9565</span><br><span class="hljs-number">3456</span><span class="hljs-string">/3854</span> [<span class="hljs-string">=========================&gt;....</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1274 - acc:</span> <span class="hljs-number">0.9569</span><br><span class="hljs-number">3488</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==========================&gt;...</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1274 - acc:</span> <span class="hljs-number">0.9570</span><br><span class="hljs-number">3520</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==========================&gt;...</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1287 - acc:</span> <span class="hljs-number">0.9568</span><br><span class="hljs-number">3552</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==========================&gt;...</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1290 - acc:</span> <span class="hljs-number">0.9564</span><br><span class="hljs-number">3584</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==========================&gt;...</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1284 - acc:</span> <span class="hljs-number">0.9568</span><br><span class="hljs-number">3616</span><span class="hljs-string">/3854</span> [<span class="hljs-string">===========================&gt;..</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1284 - acc:</span> <span class="hljs-number">0.9569</span><br><span class="hljs-number">3648</span><span class="hljs-string">/3854</span> [<span class="hljs-string">===========================&gt;..</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1278 - acc:</span> <span class="hljs-number">0.9572</span><br><span class="hljs-number">3680</span><span class="hljs-string">/3854</span> [<span class="hljs-string">===========================&gt;..</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1271 - acc:</span> <span class="hljs-number">0.9576</span><br><span class="hljs-number">3712</span><span class="hljs-string">/3854</span> [<span class="hljs-string">===========================&gt;..</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1268 - acc:</span> <span class="hljs-number">0.9580</span><br><span class="hljs-number">3744</span><span class="hljs-string">/3854</span> [<span class="hljs-string">============================&gt;.</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1279 - acc:</span> <span class="hljs-number">0.9575</span><br><span class="hljs-number">3776</span><span class="hljs-string">/3854</span> [<span class="hljs-string">============================&gt;.</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1272 - acc:</span> <span class="hljs-number">0.9579</span><br><span class="hljs-number">3808</span><span class="hljs-string">/3854</span> [<span class="hljs-string">============================&gt;.</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1279 - acc:</span> <span class="hljs-number">0.9580</span><br><span class="hljs-number">3840</span><span class="hljs-string">/3854</span> [<span class="hljs-string">============================&gt;.</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1281 - acc:</span> <span class="hljs-number">0.9581</span><br><span class="hljs-number">3854</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==============================</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">18s 5ms/step - loss: 0.1298 - acc:</span> <span class="hljs-number">0.9577</span><br><span class="hljs-string">......</span><br><span class="hljs-string">给父母买的，特意用了一段时间再来评价，电视非常好，没有坏点和损坏，界面也很简洁，便于操作，稍微不足就是开机会比普通电视慢一些，这应该是智能电视的通病吧，如果可以希望微鲸大大可以更新系统优化下开机时间~电视真的很棒，性价比爆棚，值得大家考虑购买。</span> <span class="hljs-string">客服很细心，快递小哥很耐心的等我通电验货，态度非常好。</span> <span class="hljs-string">负面</span> <span class="hljs-string">正面</span><br><span class="hljs-string">长须鲸和海狮回答都很及时，虽然物流不够快但是服务不错电视不错，对比了乐视小米和微鲸论性价比还是微鲸好点</span> <span class="hljs-string">负面</span> <span class="hljs-string">负面</span><br><span class="hljs-string">所以看不到4k效果，但是应该可以。</span> <span class="hljs-string">自带音响，中规中矩吧，好像没有别人说的好。而且，到现在没连接上我的漫步者，这个非常不满意，因为看到网上说好像普通3.5mm的连不上或者连上了声音小。希望厂家接下来开发的电视有改进。不知道我要不要换个音响。其他的用用再说。</span> <span class="hljs-string">放在地上的是跟我混了两年的tcl，天气受潮，修了一次，下岗了。</span> <span class="hljs-string">最后，我也觉得底座不算太稳，凑合着用。</span> <span class="hljs-string">负面</span> <span class="hljs-string">负面</span><br><span class="hljs-string">电视机一般，低端机不要求那么高咯。</span> <span class="hljs-string">负面</span> <span class="hljs-string">负面</span><br><span class="hljs-string">很好，两点下单上午就到了，服务很好。</span> <span class="hljs-string">正面</span> <span class="hljs-string">正面</span><br><span class="hljs-string">帮朋友买的，好好好好好好好好</span> <span class="hljs-string">正面</span> <span class="hljs-string">正面</span><br><span class="hljs-string">......</span><br><span class="hljs-string">模型在测试集上的准确率为:</span> <span class="hljs-number">0.9020979020979021</span><span class="hljs-string">.</span><br></code></pre></td></tr></table></figure><p>可以看到，该模型在训练集上的准确率为95%以上，在测试集上的准确率为90%以上，效果还是相当不错的。</p><h3 id="模型预测">模型预测</h3><p>接着，我们利用刚刚训练好的模型，对新的数据进行测试。笔者随机改造上述样本的评论，然后预测其情感倾向。情感预测的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-comment"># Import the necessary modules</span><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><br><br><span class="hljs-comment"># 导入字典</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;word_dict.pk&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    word_dictionary = pickle.load(f)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;label_dict.pk&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    output_dictionary = pickle.load(f)<br><br><span class="hljs-keyword">try</span>:<br>    <span class="hljs-comment"># 数据预处理</span><br>    input_shape = <span class="hljs-number">180</span><br>    sent = <span class="hljs-string">&quot;电视刚安装好，说实话，画质不怎么样，很差！&quot;</span><br>    x = [[word_dictionary[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent]]<br>    x = pad_sequences(maxlen=input_shape, sequences=x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># 载入模型</span><br>    model_save_path = <span class="hljs-string">&#x27;./sentiment_analysis.h5&#x27;</span><br>    lstm_model = load_model(model_save_path)<br><br>    <span class="hljs-comment"># 模型预测</span><br>    y_predict = lstm_model.predict(x)<br>    label_dict = &#123;v:k <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> output_dictionary.items()&#125;<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;输入语句: %s&#x27;</span> % sent)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;情感预测结果: %s&#x27;</span> % label_dict[np.argmax(y_predict)])<br><br><span class="hljs-keyword">except</span> KeyError <span class="hljs-keyword">as</span> err:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;您输入的句子有汉字不在词汇表中，请重新输入！&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;不在词汇表中的单词为：%s.&quot;</span> % err)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><span class="hljs-section">输入语句: 电视刚安装好，说实话，画质不怎么样，很差！</span><br><span class="hljs-section">情感预测结果: 负面</span><br></code></pre></td></tr></table></figure><p>让我们再尝试着测试一些其他的评论：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><span class="hljs-section">输入语句: 物超所值，真心不错</span><br><span class="hljs-section">情感预测结果: 正面</span><br><span class="hljs-section">输入语句: 很大很好，方便安装！</span><br><span class="hljs-section">情感预测结果: 正面</span><br><span class="hljs-section">输入语句: 卡，慢，死机，闪退。</span><br><span class="hljs-section">情感预测结果: 负面</span><br><span class="hljs-section">输入语句: 这种货色就这样吧，别期待怎样。</span><br><span class="hljs-section">情感预测结果: 负面</span><br><span class="hljs-section">输入语句: 啥服务态度码，出了事情一个推一个，送货安装还收我50</span><br><span class="hljs-section">情感预测结果: 负面</span><br><span class="hljs-section">输入语句: 京东服务很好！但我买的这款电视两天后就出现这样的问题，很后悔买了这样的电视</span><br><span class="hljs-section">情感预测结果: 负面</span><br><span class="hljs-section">输入语句: 产品质量不错，就是这位客服的态度十分恶劣，对相关服务不予解释说明，缺乏耐心，</span><br><span class="hljs-section">情感预测结果: 负面</span><br><span class="hljs-section">输入语句: 很满意，电视非常好。护眼模式，很好，也很清晰。</span><br><span class="hljs-section">情感预测结果: 负面</span><br></code></pre></td></tr></table></figure><h3 id="总结">总结</h3><p>当然，该模型并不是对一切该商品的评论都会有好的效果，还是应该针对特定的语料去训练，去预测。</p><p>本文主要介绍了LSTM模型在文本情感分析方面的应用，该项目已上传Github，地址为：<ahref="https://github.com/percent4/Sentiment_Analysis">https://github.com/percent4/Sentiment_Analysis</a>。</p><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注~</p><h3 id="参考文献">参考文献</h3><ol type="1"><li>Python机器学习 -- NLP情感分析：<ahref="https://blog.csdn.net/qq_38328378/article/details/81198322">https://blog.csdn.net/qq_38328378/article/details/81198322</a></li><li>数据集来源：<ahref="https://github.com/renjunxiang/Text-Classification/blob/master/TextClassification/data/data_single.csv">https://github.com/renjunxiang/Text-Classification/blob/master/TextClassification/data/data_single.csv</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>情感分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（九）词义消岐（WSD）的简介与实现</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B9%9D%EF%BC%89%E8%AF%8D%E4%B9%89%E6%B6%88%E5%B2%90%EF%BC%88WSD%EF%BC%89%E7%9A%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%9E%E7%8E%B0/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B9%9D%EF%BC%89%E8%AF%8D%E4%B9%89%E6%B6%88%E5%B2%90%EF%BC%88WSD%EF%BC%89%E7%9A%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h3 id="词义消岐简介">词义消岐简介</h3><p>词义消岐，英文名称为Word SenseDisambiguation，英语缩写为WSD，是自然语言处理（NLP）中一个非常有趣的基本任务。</p><p>那么，什么是词义消岐呢？通常，在我们的自然语言中，不管是英语，还是中文，都有多义词存在。这些多义词的存在，会让人对句子的意思产生混淆，但人通过学习又是可以正确地区分出来的。</p><p>以<strong>“小米”</strong>这个词为例，如果仅仅只是说“小米”这个词语，你并不知道它实际指的到底是小米科技公司还是谷物。但当我们把词语置于某个特定的语境中，我们能很好地区分出这个词语的意思。比如，</p><blockquote><p>雷军是小米的创始人。</p></blockquote><p>在这个句子中，我们知道这个“小米”指的是小米科技公司。比如</p><blockquote><p>我今天早上喝了一碗小米粥。</p></blockquote><p>在这个句子中，“小米”指的是谷物、农作物。</p><p>所谓词义消岐，指的是在特定的语境中，识别出某个歧义词的正确含义。</p><p>那么，词义消岐有什么作用呢？词义消岐可以很好地服务于语言翻译和智能问答领域，当然，还有许多应用有待开发～</p><h3 id="词义消岐实现">词义消岐实现</h3><p>在目前的词义消岐算法中，有不少原创算法，有些实现起来比较简单，有些想法较为复杂，但实现的效果普遍都不是很好。比较经典的词义消岐的算法为Lesk算法，该算法的想法很简单，通过对某个歧义词构建不同含义的语料及待判别句子中该词语与语料的重合程度来实现，具体的算法原理可参考网址：<ahref="https://en.wikipedia.org/wiki/Lesk_algorithm">https://en.wikipedia.org/wiki/Lesk_algorithm</a>.</p><p>在下面的部分中，笔者将会介绍自己想的一种实现词义消岐的算法，仅仅是一个想法，仅供参考。</p><p>我们以词语“火箭”为例，选取其中的两个<strong>义项</strong>（同一个词语的不同含义）：<ahref="https://baike.baidu.com/item/%E7%81%AB%E7%AE%AD/8794081#viewPageContent"title="NBA球队名">NBA球队名</a> 和 <ahref="https://baike.baidu.com/item/%E7%81%AB%E7%AE%AD/6308#viewPageContent"title="燃气推进装置">燃气推进装置</a> ，如下：</p><p><img src="/img/nlp9_1.png" /></p><h4 id="获取语料">获取语料</h4><p>首先，我们利用爬虫爬取这两个义项的百度百科网页，以句子为单位，只要句子中出现该词语，则把这句话加入到这个义项的预料中。爬虫的完整Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> SentenceSplitter<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WebScrape</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, word, url</span>):<br>        self.url = url<br>        self.word = word<br><br>    <span class="hljs-comment"># 爬取百度百科页面</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">web_parse</span>(<span class="hljs-params">self</span>):<br>        headers = &#123;<span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \</span><br><span class="hljs-string">                                             (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36&#x27;</span>&#125;<br>        req = requests.get(url=self.url, headers=headers)<br><br>        <span class="hljs-comment"># 解析网页，定位到main-content部分</span><br>        <span class="hljs-keyword">if</span> req.status_code == <span class="hljs-number">200</span>:<br>            soup = BeautifulSoup(req.text.encode(req.encoding), <span class="hljs-string">&#x27;lxml&#x27;</span>)<br>            <span class="hljs-keyword">return</span> soup<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># 获取该词语的义项</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_gloss</span>(<span class="hljs-params">self</span>):<br>        soup = self.web_parse()<br>        <span class="hljs-keyword">if</span> soup:<br>            lis = soup.find(<span class="hljs-string">&#x27;ul&#x27;</span>, class_=<span class="hljs-string">&quot;polysemantList-wrapper cmn-clearfix&quot;</span>)<br>            <span class="hljs-keyword">if</span> lis:<br>                <span class="hljs-keyword">for</span> li <span class="hljs-keyword">in</span> lis(<span class="hljs-string">&#x27;li&#x27;</span>):<br>                    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;&lt;a&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">str</span>(li):<br>                        gloss = li.text.replace(<span class="hljs-string">&#x27;▪&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>                        <span class="hljs-keyword">return</span> gloss<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># 获取该义项的语料，以句子为单位</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_content</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 发送HTTP请求</span><br>        result = []<br>        soup = self.web_parse()<br>        <span class="hljs-keyword">if</span> soup:<br>            paras = soup.find(<span class="hljs-string">&#x27;div&#x27;</span>, class_=<span class="hljs-string">&#x27;main-content&#x27;</span>).text.split(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>            <span class="hljs-keyword">for</span> para <span class="hljs-keyword">in</span> paras:<br>                <span class="hljs-keyword">if</span> self.word <span class="hljs-keyword">in</span> para:<br>                    sents = <span class="hljs-built_in">list</span>(SentenceSplitter.split(para))<br>                    <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>                        <span class="hljs-keyword">if</span> self.word <span class="hljs-keyword">in</span> sent:<br>                            sent = sent.replace(<span class="hljs-string">&#x27;\xa0&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>).replace(<span class="hljs-string">&#x27;\u3000&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>                            result.append(sent)<br><br>        result = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(result))<br><br>        <span class="hljs-keyword">return</span> result<br><br>    <span class="hljs-comment"># 将该义项的语料写入到txt</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">write_2_file</span>(<span class="hljs-params">self</span>):<br>        gloss = self.get_gloss()<br>        result = self.get_content()<br>        <span class="hljs-built_in">print</span>(gloss)<br>        <span class="hljs-built_in">print</span>(result)<br>        <span class="hljs-keyword">if</span> result <span class="hljs-keyword">and</span> gloss:<br>            <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./%s_%s.txt&#x27;</span>% (self.word, gloss), <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>                f.writelines([_+<span class="hljs-string">&#x27;\n&#x27;</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> result])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>(<span class="hljs-params">self</span>):<br>        self.write_2_file()<br><br><span class="hljs-comment"># NBA球队名</span><br><span class="hljs-comment">#url = &#x27;https://baike.baidu.com/item/%E4%BC%91%E6%96%AF%E6%95%A6%E7%81%AB%E7%AE%AD%E9%98%9F/370758?fromtitle=%E7%81%AB%E7%AE%AD&amp;fromid=8794081#viewPageContent&#x27;</span><br><span class="hljs-comment"># 燃气推进装置</span><br>url = <span class="hljs-string">&#x27;https://baike.baidu.com/item/%E7%81%AB%E7%AE%AD/6308#viewPageContent&#x27;</span><br>WebScrape(<span class="hljs-string">&#x27;火箭&#x27;</span>, url).run()<br></code></pre></td></tr></table></figure><p>利用这个爬虫，我们爬取了“火箭”这个词语的两个义项的语料，生成了火箭_燃气推进装置.txt文件和火箭_NBA球队名.txt文件，这两个文件分别含有361和171个句子。以火箭_燃气推进装置.txt文件为例，前10个句子如下：</p><blockquote><p>火箭技术的飞速发展，不仅可提供更加完善的各类导弹和推动相关科学的发展，还将使开发空间资源、建立空间产业、空间基地及星际航行等成为可能。火箭技术是一项十分复杂的综合性技术，主要包括火箭推进技术、总体设计技术、火箭结构技术、控制和制导技术、计划管理技术、可靠性和质量控制技术、试验技术，对导弹来说还有弹头制导和控制、1903年，俄国的К.E.齐奥尔科夫斯基提出了制造大型液体火箭的设想和设计原理。火箭有很多种，原始的火箭是用引火物附在弓箭头上，然后射到敌人身上引起焚烧的一种箭矢。“长征三号丙”火箭是在 “长征三号乙”火箭的基础上，减少了两个助推器并取消了助推器上的尾翼。 火箭与导弹有什么区别为了能够在未来大规模的将人类送入太空，不可能依赖传统的火箭和飞船。火箭V2火箭探测高层大气的物理特征（如气压、温度、湿度等）和现象的探空火箭。可一次发射一发至数十发火箭弹。</p></blockquote><h4 id="实现算法">实现算法</h4><p>我们以句子为单位进行词义消岐，即输入一句话，识别出该句子中某个歧义词的含义。笔者使用的算法比较简单，是以TF-IDF为权重的频数判别。以句子</p><blockquote><p>赛季初的时候，火箭是众望所归的西部决赛球队。</p></blockquote><p>为例，对该句子分词后，去掉停用词（stopwords），然后分别统计除了“火箭”这个词以外的TF-IDF值，累加起来,比较在两个义项下这个值的大小即可。</p><p>实现这个算法的完整Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> log2<br><br><span class="hljs-comment"># 读取每个义项的语料</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_file</span>(<span class="hljs-params">path</span>):<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(path, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        lines = [_.strip() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> f.readlines()]<br>        <span class="hljs-keyword">return</span> lines<br><br><span class="hljs-comment"># 对示例句子分词</span><br>sent = <span class="hljs-string">&#x27;赛季初的时候，火箭是众望所归的西部决赛球队。&#x27;</span><br>wsd_word = <span class="hljs-string">&#x27;火箭&#x27;</span><br><br>jieba.add_word(wsd_word)<br>sent_words = <span class="hljs-built_in">list</span>(jieba.cut(sent, cut_all=<span class="hljs-literal">False</span>))<br><br><span class="hljs-comment"># 去掉停用词</span><br>stopwords = [wsd_word, <span class="hljs-string">&#x27;我&#x27;</span>, <span class="hljs-string">&#x27;你&#x27;</span>, <span class="hljs-string">&#x27;它&#x27;</span>, <span class="hljs-string">&#x27;他&#x27;</span>, <span class="hljs-string">&#x27;她&#x27;</span>, <span class="hljs-string">&#x27;了&#x27;</span>, <span class="hljs-string">&#x27;是&#x27;</span>, <span class="hljs-string">&#x27;的&#x27;</span>, <span class="hljs-string">&#x27;啊&#x27;</span>, <span class="hljs-string">&#x27;谁&#x27;</span>, <span class="hljs-string">&#x27;什么&#x27;</span>,<span class="hljs-string">&#x27;都&#x27;</span>,\<br>             <span class="hljs-string">&#x27;很&#x27;</span>, <span class="hljs-string">&#x27;个&#x27;</span>, <span class="hljs-string">&#x27;之&#x27;</span>, <span class="hljs-string">&#x27;人&#x27;</span>, <span class="hljs-string">&#x27;在&#x27;</span>, <span class="hljs-string">&#x27;上&#x27;</span>, <span class="hljs-string">&#x27;下&#x27;</span>, <span class="hljs-string">&#x27;左&#x27;</span>, <span class="hljs-string">&#x27;右&#x27;</span>, <span class="hljs-string">&#x27;。&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>, <span class="hljs-string">&#x27;！&#x27;</span>, <span class="hljs-string">&#x27;？&#x27;</span>]<br><br>sent_cut = []<br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent_words:<br>    <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stopwords:<br>        sent_cut.append(word)<br><br><span class="hljs-built_in">print</span>(sent_cut)<br><br><br><span class="hljs-comment"># 计算其他词的TF-IDF以及频数</span><br>wsd_dict = &#123;&#125;<br><span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> os.listdir(<span class="hljs-string">&#x27;.&#x27;</span>):<br>    <span class="hljs-keyword">if</span> wsd_word <span class="hljs-keyword">in</span> file:<br>        wsd_dict[file.replace(<span class="hljs-string">&#x27;.txt&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)] = read_file(file)<br><br><span class="hljs-comment"># 统计每个词语在语料中出现的次数</span><br>tf_dict = &#123;&#125;<br><span class="hljs-keyword">for</span> meaning, sents <span class="hljs-keyword">in</span> wsd_dict.items():<br>    tf_dict[meaning] = []<br>    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent_cut:<br>        word_count = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>            example = <span class="hljs-built_in">list</span>(jieba.cut(sent, cut_all=<span class="hljs-literal">False</span>))<br>            word_count += example.count(word)<br><br>        <span class="hljs-keyword">if</span> word_count:<br>            tf_dict[meaning].append((word, word_count))<br><br>idf_dict = &#123;&#125;<br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent_cut:<br>    document_count = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> meaning, sents <span class="hljs-keyword">in</span> wsd_dict.items():<br>        <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> sent:<br>                document_count += <span class="hljs-number">1</span><br><br>    idf_dict[word] = document_count<br><br><span class="hljs-comment"># 输出值</span><br>total_document = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> meaning, sents <span class="hljs-keyword">in</span> wsd_dict.items():<br>    total_document += <span class="hljs-built_in">len</span>(sents)<br><br><span class="hljs-comment"># 计算tf_idf值</span><br>mean_tf_idf = []<br><span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> tf_dict.items():<br>    <span class="hljs-built_in">print</span>(k+<span class="hljs-string">&#x27;:&#x27;</span>)<br>    tf_idf_sum = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> v:<br>        word = item[<span class="hljs-number">0</span>]<br>        tf = item[<span class="hljs-number">1</span>]<br>        tf_idf = item[<span class="hljs-number">1</span>]*log2(total_document/(<span class="hljs-number">1</span>+idf_dict[word]))<br>        tf_idf_sum += tf_idf<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;%s, 频数为: %s, TF-IDF值为: %s&#x27;</span>% (word, tf, tf_idf))<br><br>    mean_tf_idf.append((k, tf_idf_sum))<br><br>sort_array = <span class="hljs-built_in">sorted</span>(mean_tf_idf, key=<span class="hljs-keyword">lambda</span> x:x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)<br>true_meaning = sort_array[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].split(<span class="hljs-string">&#x27;_&#x27;</span>)[<span class="hljs-number">1</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n经过词义消岐，%s在该句子中的意思为 %s .&#x27;</span> % (wsd_word, true_meaning))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">[&#x27;赛季&#x27;, &#x27;初&#x27;, &#x27;时候&#x27;, &#x27;众望所归&#x27;, &#x27;西部&#x27;, &#x27;决赛&#x27;, &#x27;球队&#x27;]</span><br><span class="hljs-attribute">火箭_燃气推进装置</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">初, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 12.49585502688717</span><br><span class="hljs-attribute">火箭_NBA球队名</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">赛季, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">63, TF-IDF值为: 204.6194333469459</span><br><span class="hljs-attribute">初, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">1, TF-IDF值为: 6.247927513443585</span><br><span class="hljs-attribute">时候, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">1, TF-IDF值为: 8.055282435501189</span><br><span class="hljs-attribute">西部, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">16, TF-IDF值为: 80.88451896801904</span><br><span class="hljs-attribute">决赛, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">7, TF-IDF值为: 33.13348038429679</span><br><span class="hljs-attribute">球队, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">40, TF-IDF值为: 158.712783770034</span><br><br>经过词义消岐，火箭在该句子中的意思为 NBA球队名 .<br></code></pre></td></tr></table></figure><h4 id="测试">测试</h4><p>接着，我们对上面的算法和程序进行更多的测试。</p><p>输入句子为:</p><blockquote><p>三十多年前，战士们在戈壁滩白手起家，建起了我国的火箭发射基地。</p></blockquote><p>输出结果为:</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs prolog">[<span class="hljs-string">&#x27;三十多年&#x27;</span>, <span class="hljs-string">&#x27;前&#x27;</span>, <span class="hljs-string">&#x27;战士&#x27;</span>, <span class="hljs-string">&#x27;们&#x27;</span>, <span class="hljs-string">&#x27;戈壁滩&#x27;</span>, <span class="hljs-string">&#x27;白手起家&#x27;</span>, <span class="hljs-string">&#x27;建起&#x27;</span>, <span class="hljs-string">&#x27;我国&#x27;</span>, <span class="hljs-string">&#x27;发射&#x27;</span>, <span class="hljs-string">&#x27;基地&#x27;</span>]<br>火箭<span class="hljs-symbol">_</span>燃气推进装置:<br>前, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">9.063440958888354</span><br>们, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">6.05528243550119</span><br>我国, 频数为: <span class="hljs-number">3</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">22.410959804340102</span><br>发射, 频数为: <span class="hljs-number">89</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">253.27878721862933</span><br>基地, 频数为: <span class="hljs-number">7</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">42.38697704850833</span><br>火箭<span class="hljs-symbol">_NBA</span>球队名:<br>前, 频数为: <span class="hljs-number">3</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">13.59516143833253</span><br>们, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">6.05528243550119</span><br><br>经过词义消岐，火箭在该句子中的意思为 燃气推进装置 .<br></code></pre></td></tr></table></figure><p>输入句子为：</p><blockquote><p>对于马刺这样级别的球队，常规赛只有屈指可数的几次交锋具有真正的意义，今天对火箭一役是其中之一。</p></blockquote><p>输出结果为：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs prolog">[<span class="hljs-string">&#x27;对于&#x27;</span>, <span class="hljs-string">&#x27;马刺&#x27;</span>, <span class="hljs-string">&#x27;这样&#x27;</span>, <span class="hljs-string">&#x27;级别&#x27;</span>, <span class="hljs-string">&#x27;球队&#x27;</span>, <span class="hljs-string">&#x27;常规赛&#x27;</span>, <span class="hljs-string">&#x27;只有&#x27;</span>, <span class="hljs-string">&#x27;屈指可数&#x27;</span>, <span class="hljs-string">&#x27;几次&#x27;</span>, <span class="hljs-string">&#x27;交锋&#x27;</span>, <span class="hljs-string">&#x27;具有&#x27;</span>, <span class="hljs-string">&#x27;真正&#x27;</span>, <span class="hljs-string">&#x27;意义&#x27;</span>, <span class="hljs-string">&#x27;今天&#x27;</span>, <span class="hljs-string">&#x27;对&#x27;</span>, <span class="hljs-string">&#x27;一役&#x27;</span>, <span class="hljs-string">&#x27;其中&#x27;</span>, <span class="hljs-string">&#x27;之一&#x27;</span>]<br>火箭<span class="hljs-symbol">_</span>燃气推进装置:<br>只有, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">7.470319934780034</span><br>具有, 频数为: <span class="hljs-number">5</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">32.35159967390017</span><br>真正, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">14.940639869560068</span><br>意义, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">8.055282435501189</span><br>对, 频数为: <span class="hljs-number">5</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">24.03677461028802</span><br>其中, 频数为: <span class="hljs-number">3</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">21.16584730650357</span><br>之一, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">14.11056487100238</span><br>火箭<span class="hljs-symbol">_NBA</span>球队名:<br>马刺, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">7.470319934780034</span><br>球队, 频数为: <span class="hljs-number">40</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">158.712783770034</span><br>常规赛, 频数为: <span class="hljs-number">14</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">73.4709851882102</span><br>只有, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">7.470319934780034</span><br>对, 频数为: <span class="hljs-number">10</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">48.07354922057604</span><br>之一, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">7.05528243550119</span><br><br>经过词义消岐，火箭在该句子中的意思为 <span class="hljs-symbol">NBA</span>球队名 .<br></code></pre></td></tr></table></figure><p>输入句子为：</p><blockquote><p>姚明是火箭队的主要得分手之一。</p></blockquote><p>输出结果为：</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">[&#x27;姚明&#x27;, &#x27;火箭队&#x27;, &#x27;主要&#x27;, &#x27;得分手&#x27;, &#x27;之一&#x27;]</span><br><span class="hljs-attribute">火箭_燃气推进装置</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">主要, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">9, TF-IDF值为: 51.60018906552445</span><br><span class="hljs-attribute">之一, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 14.11056487100238</span><br><span class="hljs-attribute">火箭_NBA球队名</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">姚明, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">18, TF-IDF值为: 90.99508383902142</span><br><span class="hljs-attribute">火箭队, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">133, TF-IDF值为: 284.1437533641371</span><br><span class="hljs-attribute">之一, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">1, TF-IDF值为: 7.05528243550119</span><br><br>经过词义消岐，火箭在该句子中的意思为 NBA球队名 .<br></code></pre></td></tr></table></figure><p>输入的句子为:</p><blockquote><p>从1992年开始研制的长征二号F型火箭，是中国航天史上技术最复杂、可靠性和安全性指标最高的运载火箭。</p></blockquote><p>输出结果为：</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">[&#x27;从&#x27;, &#x27;1992&#x27;, &#x27;年&#x27;, &#x27;开始&#x27;, &#x27;研制&#x27;, &#x27;长征二号&#x27;, &#x27;F&#x27;, &#x27;型&#x27;, &#x27;中国&#x27;, &#x27;航天史&#x27;, &#x27;技术&#x27;, &#x27;最&#x27;, &#x27;复杂&#x27;, &#x27;、&#x27;, &#x27;可靠性&#x27;, &#x27;和&#x27;, &#x27;安全性&#x27;, &#x27;指标&#x27;, &#x27;最高&#x27;, &#x27;运载火箭&#x27;]</span><br><span class="hljs-attribute">火箭_燃气推进装置</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">从, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">6, TF-IDF值为: 29.312144604353264</span><br><span class="hljs-attribute">1992, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">1, TF-IDF值为: 6.733354340613827</span><br><span class="hljs-attribute">年, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">43, TF-IDF值为: 107.52982410441274</span><br><span class="hljs-attribute">开始, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">5, TF-IDF值为: 30.27641217750595</span><br><span class="hljs-attribute">研制, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">25, TF-IDF值为: 110.28565614316162</span><br><span class="hljs-attribute">长征二号, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">37, TF-IDF值为: 159.11461253349566</span><br><span class="hljs-attribute">F, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">7, TF-IDF值为: 40.13348038429679</span><br><span class="hljs-attribute">中国, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">45, TF-IDF值为: 153.51418105769093</span><br><span class="hljs-attribute">技术, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">27, TF-IDF值为: 119.10850863461454</span><br><span class="hljs-attribute">最, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 7.614709844115208</span><br><span class="hljs-attribute">、, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">117, TF-IDF值为: 335.25857156467714</span><br><span class="hljs-attribute">可靠性, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">5, TF-IDF值为: 30.27641217750595</span><br><span class="hljs-attribute">和, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">76, TF-IDF值为: 191.22539545388003</span><br><span class="hljs-attribute">安全性, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 14.940639869560068</span><br><span class="hljs-attribute">运载火箭, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">95, TF-IDF值为: 256.28439093389505</span><br><span class="hljs-attribute">火箭_NBA球队名</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">从, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">5, TF-IDF值为: 24.42678717029439</span><br><span class="hljs-attribute">1992, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 13.466708681227654</span><br><span class="hljs-attribute">年, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">52, TF-IDF值为: 130.0360663588247</span><br><span class="hljs-attribute">开始, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 12.11056487100238</span><br><span class="hljs-attribute">中国, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">4, TF-IDF值为: 13.64570498290586</span><br><span class="hljs-attribute">最, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">3, TF-IDF值为: 11.422064766172813</span><br><span class="hljs-attribute">、, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">16, TF-IDF值为: 45.847326025938756</span><br><span class="hljs-attribute">和, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">31, TF-IDF值为: 77.99983235618791</span><br><span class="hljs-attribute">最高, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">8, TF-IDF值为: 59.76255947824027</span><br><br>经过词义消岐，火箭在该句子中的意思为 燃气推进装置 .<br></code></pre></td></tr></table></figure><p>输入句子为：</p><blockquote><p>到目前为止火箭已经在休斯顿进行了电视宣传，并在大街小巷竖起广告栏。</p></blockquote><p>输出结果为：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs prolog">[<span class="hljs-string">&#x27;到&#x27;</span>, <span class="hljs-string">&#x27;目前为止&#x27;</span>, <span class="hljs-string">&#x27;已经&#x27;</span>, <span class="hljs-string">&#x27;休斯顿&#x27;</span>, <span class="hljs-string">&#x27;进行&#x27;</span>, <span class="hljs-string">&#x27;电视&#x27;</span>, <span class="hljs-string">&#x27;宣传&#x27;</span>, <span class="hljs-string">&#x27;并&#x27;</span>, <span class="hljs-string">&#x27;大街小巷&#x27;</span>, <span class="hljs-string">&#x27;竖起&#x27;</span>, <span class="hljs-string">&#x27;广告栏&#x27;</span>]<br>火箭<span class="hljs-symbol">_</span>燃气推进装置:<br>到, 频数为: <span class="hljs-number">11</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">39.19772273088667</span><br>已经, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">13.466708681227654</span><br>进行, 频数为: <span class="hljs-number">14</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">68.39500407682429</span><br>并, 频数为: <span class="hljs-number">11</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">49.17351928258037</span><br>火箭<span class="hljs-symbol">_NBA</span>球队名:<br>到, 频数为: <span class="hljs-number">6</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">21.38057603502909</span><br>已经, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">13.466708681227654</span><br>休斯顿, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">14.940639869560068</span><br>进行, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">9.770714868117755</span><br>并, 频数为: <span class="hljs-number">5</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">22.351599673900168</span><br><br>经过词义消岐，火箭在该句子中的意思为 燃气推进装置 .<br></code></pre></td></tr></table></figure><h3 id="总结">总结</h3><p>对于笔者的这个算法，虽然有一定的效果，但是也不总是识别正确。比如，对于最后一个测试的句子，识别的结果就是错误的，其实“休斯顿”才是识别该词语义项的关键词，但很遗憾，在笔者的算法中，“休斯顿”的权重并不高。</p><p>对于词义消岐算法，如果还是笔者的这个思路，那么有以下几方面需要改进：</p><ul><li><p>语料大小及丰富程度；</p></li><li><p>停用词的扩充；</p></li><li><p>更好的算法。</p><p>笔者的这篇文章仅作为词义消岐的简介以及简单实现，希望能对读者有所启发～</p></li></ul><p><strong>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</strong></p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>词义消岐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（八）使用CRF++实现命名实体识别(NER)</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AB%EF%BC%89%E4%BD%BF%E7%94%A8CRF-%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB-NER/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AB%EF%BC%89%E4%BD%BF%E7%94%A8CRF-%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB-NER/</url>
    
    <content type="html"><![CDATA[<h3 id="CRF与NER简介">CRF与NER简介</h3><p>CRF，英文全称为conditional random field, 中文名为条件随机场，是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型，其特点是假设输出随机变量构成马尔可夫（Markov）随机场。</p><p>较为简单的条件随机场是定义在线性链上的条件随机场，称为线性链条件随机场（linear chain conditional random field）. 线性链条件随机场可以用于序列标注等问题，而本文需要解决的命名实体识别(NER)任务正好可通过序列标注方法解决。这时，在条件概率模型P(Y|X)中，Y是输出变量，表示标记序列（或状态序列），X是输入变量，表示需要标注的观测序列。学习时，利用训练数据 集通过极大似然估计或正则化的极大似然估计得到条件概率模型p(Y|X)；预测时，对于给定的输入序列x，求出条件概率p(y|x)最大的输出序列y0.</p><p><img src="/img/nlp8_1.jpeg" alt=""></p><p>命名实体识别（Named Entity Recognition，简称NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。常见的实现NER的算法如下：</p><p><img src="/img/nlp8_2.jpeg" alt=""></p><p>本文不准备详细介绍条件随机场的原理与实现算法，关于具体的原理与实现算法，可以参考《统计学习算法》一书。我们将借助已实现条件随机场的工具——CRF++来实现命名实体识别。关于用深度学习算法来实现命名实体识别， 可以参考文章：<a href="https://www.jianshu.com/p/ee750877ab6f">NLP入门（五）用深度学习实现命名实体识别（NER）</a>。</p><h3 id="CRF">CRF++</h3><h4 id="简介">简介</h4><p>CRF++是著名的条件随机场的开源工具，也是目前综合性能最佳的CRF工具，采用C++语言编写而成。其最重要的功能我认为是采用了特征模板。这样就可以自动生成一系列的特征函数，而不用我们自己生成特征函数，我们要做的就是寻找特征，比如词性等。关于CRF++的特性，可以参考网址：<a href="http://taku910.github.io/crfpp/">http://taku910.github.io/crfpp/</a> 。</p><h4 id="安装">安装</h4><p>CRF++的安装可分为Windows环境和Linux环境下的安装。关于Linux环境下的安装，可以参考文章：<a href="https://blog.51cto.com/wutengfei/2095715">CRFPP/CRF++编译安装与部署</a> 。 在Windows中CRF++不需要安装，下载解压CRF++0.58文件即可以使用，下载网址为：<a href="https://blog.csdn.net/lilong117194/article/details/81160265">https://blog.csdn.net/lilong117194/article/details/81160265</a> 。</p><h4 id="使用">使用</h4><h5 id="1-语料">1. 语料</h5><p>以我们本次使用的命名实体识别的语料为例，作为CRF++训练的语料（前20行，每一句话以空格隔开。）如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">played VBD O<br>on IN O<br>Monday NNP O<br>( ( O<br>home NN O<br>team NN O<br><span class="hljs-keyword">in</span> IN O<br>CAPS NNP O<br>) ) O<br>: : O<br><br>American NNP B-MISC<br>League NNP I-MISC<br><br>Cleveland NNP B-ORG<br>2 CD O<br>DETROIT NNP B-ORG<br>1 CD O<br><br>BALTIMORE VB B-ORG<br></code></pre></td></tr></table></figure><p>需要注意字与标签之间的分隔符为制表符\t，否则会导致feature_index.cpp(86) [max_size == size] inconsistent column size错误。</p><h5 id="2-模板">2. 模板</h5><p>模板是使用CRF++的关键，它能帮助我们自动生成一系列的特征函数，而不用我们自己生成特征函数，而特征函数正是CRF算法的核心概念之一。一个简单的模板文件如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Unigram</span><br>U00:%x[-2,0]<br>U01:%x[0,1]<br>U02:%x[0,0]<br>U03:%x[1,0]<br>U04:%x[2,0]<br>U05:%x[-2,0]/%x[-1,0]/%x[0,0]<br>U06:%x[-1,0]/%x[0,0]/%x[1,0]<br>U07:%x[0,0]/%x[1,0]/%x[2,0]<br>U08:%x[-1,0]/%x[0,0]<br>U09:%x[0,0]/%x[1,0]<br> <br><span class="hljs-comment"># Bigram</span><br>B<br></code></pre></td></tr></table></figure><p>在这里，我们需要好好理解下模板文件的规则。T**:%x[#,#]中的T表示模板类型，两个&quot;#&quot;分别表示相对的行偏移与列偏移。一共有两种模板：</p><ul><li>第一种模板是Unigram template:第一个字符是U，用于描述unigram feature的模板。每一行%x[#,#]生成一个CRF中的点(state)函数: f(s, o), 其中s为t时刻的的标签(output)，o为t时刻的上下文。假设<code>home NN O</code>所在行为<code>CURRENT TOKEN</code>，</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">played VBD O<br>on IN O<br>Monday NNP O<br>( ( O<br>home NN O &lt;&lt; <span class="hljs-string">CURRENT TOKEN</span><br><span class="hljs-string">team NN O</span><br><span class="hljs-string">in IN O</span><br><span class="hljs-string">CAPS NNP O</span><br><span class="hljs-string">) ) O</span><br><span class="hljs-string">: : O</span><br></code></pre></td></tr></table></figure><p>那么%x[#,#]的对应规则如下：</p><table><thead><tr><th>template</th><th>expanded feature</th></tr></thead><tbody><tr><td>%x[0,0]</td><td>home</td></tr><tr><td>%x[0,1]</td><td>NN</td></tr><tr><td>%x[-1,0]</td><td>(</td></tr><tr><td>%x[-2,1]</td><td>NNP</td></tr><tr><td>%x[0,0]/%x[0,1]</td><td>home/NN</td></tr><tr><td>ABC%x[0,1]123</td><td>ABCNN123</td></tr></tbody></table><p>以“U01:%x[0,1]”为例，它在该语料中生成的示例函数如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">func1 = <span class="hljs-keyword">if</span> (output = O and feature=<span class="hljs-string">&quot;U01:NN&quot;</span>) <span class="hljs-built_in">return</span> 1 <span class="hljs-keyword">else</span> <span class="hljs-built_in">return</span> 0<br>func2 = <span class="hljs-keyword">if</span> (output = O and feature=<span class="hljs-string">&quot;U01:N&quot;</span>) <span class="hljs-built_in">return</span> 1 <span class="hljs-keyword">else</span> <span class="hljs-built_in">return</span> 0<br>func3 = <span class="hljs-keyword">if</span> (output = O and feature=<span class="hljs-string">&quot;U01:NNP&quot;</span>) <span class="hljs-built_in">return</span> 1  <span class="hljs-keyword">else</span> <span class="hljs-built_in">return</span> 0<br>....<br></code></pre></td></tr></table></figure><ul><li>第二种模板是Bigram template:第一个字符是B，每一行%x[#,#]生成一个CRFs中的边(Edge)函数:f(s’, s, o), 其中s’为t–1时刻的标签。也就是说,Bigram类型与Unigram大致相同,只是还要考虑到t–1时刻的标签。如果只写一个B的话,默认生成f(s’, s)，这意味着前一个output token和current token将组合成bigram features。</li></ul><h5 id="3-训练">3. 训练</h5><p>CRF++的训练命令一般格式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">crf_learn  -f 3 -c 4.0 template train.data model -t<br></code></pre></td></tr></table></figure><p>其中，template为模板文件，train.data为训练语料，-t表示可以得到一个model文件和一个model.txt文件，其他可选参数说明如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">-f, –freq=INT使用属性的出现次数不少于INT(默认为1)<br><br>-m, –maxiter=INT设置INT为LBFGS的最大迭代次数 (默认10k)<br><br>-c, –cost=FLOAT    设置FLOAT为代价参数，过大会过度拟合 (默认1.0)<br><br>-e, –eta=FLOAT设置终止标准FLOAT(默认0.0001)<br><br>-C, –convert将文本模式转为二进制模式<br><br>-t, –textmodel为调试建立文本模型文件<br><br>-a, –algorithm=(CRF|MIRA)    选择训练算法，默认为CRF-L2<br><br>-p, –thread=INT线程数(默认1)，利用多个CPU减少训练时间<br><br>-H, –shrinking-size=INT    设置INT为最适宜的跌代变量次数 (默认20)<br><br>-v, –version显示版本号并退出<br><br>-h, –<span class="hljs-built_in">help</span>显示帮助并退出<br></code></pre></td></tr></table></figure><p>在训练过程中，会输出一些信息，其意义如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">iter：迭代次数。当迭代次数达到maxiter时，迭代终止<br><br>terr：标记错误率<br><br>serr：句子错误率<br><br>obj：当前对象的值。当这个值收敛到一个确定值的时候，训练完成<br><br>diff：与上一个对象值之间的相对差。当此值低于eta时，训练完成<br></code></pre></td></tr></table></figure><h5 id="4-预测">4. 预测</h5><p>在训练完模型后，我们可以使用训练好的模型对新数据进行预测，预测命令格式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">crf_test -m model NER_predict.data &gt; predict.txt<br></code></pre></td></tr></table></figure><p><code>-m model</code>表示使用我们刚刚训练好的model模型，预测的数据文件为NER_predict.data,  <code>&gt; predict.txt</code>表示将预测后的数据写入到predict.txt中。</p><h3 id="NER实现实例">NER实现实例</h3><p>接下来，我们将利用CRF++来实现英文命名实体识别功能。</p><p>本项目实现NER的语料库如下(文件名为train.txt，一共42000行，这里只展示前15行，可以在文章最后的Github地址下载该语料库)：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">played on Monday ( home team <span class="hljs-keyword">in</span> CAPS ) :<br>VBD IN NNP ( NN NN IN NNP ) :<br>O O O O O O O O O O<br>American League<br>NNP NNP<br>B-MISC I-MISC<br>Cleveland 2 DETROIT 1<br>NNP CD NNP CD<br>B-ORG O B-ORG O<br>BALTIMORE 12 Oakland 11 ( 10 innings )<br>VB CD NNP CD ( CD NN )<br>B-ORG O B-ORG O O O O O<br>TORONTO 5 Minnesota 3<br>TO CD NNP CD<br>B-ORG O B-ORG O<br>......<br></code></pre></td></tr></table></figure><p>简单介绍下该语料库的结构：该语料库一共42000行，每三行为一组，其中，第一行为英语句子，第二行为句子中每个单词的词性，第三行为NER系统的标注，共分4个标注类别：PER（人名），LOC（位置），ORG（组织）以及MISC，其中B表示开始，I表示中间，O表示单字词，不计入NER，sO表示特殊单字词。</p><p>首先我们将该语料分为训练集和测试集，比例为9:1，实现的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-comment"># NER预料train.txt所在的路径</span><br><span class="hljs-built_in">dir</span> = <span class="hljs-string">&quot;/Users/Shared/CRF_4_NER/CRF_TEST&quot;</span><br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/train.txt&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    sents = [line.strip() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines()]<br><br><span class="hljs-comment"># 训练集与测试集的比例为9:1</span><br>RATIO = <span class="hljs-number">0.9</span><br>train_num = <span class="hljs-built_in">int</span>((<span class="hljs-built_in">len</span>(sents)//<span class="hljs-number">3</span>)*RATIO)<br><br><span class="hljs-comment"># 将文件分为训练集与测试集</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/NER_train.data&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&quot;w&quot;</span>) <span class="hljs-keyword">as</span> g:<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(train_num):<br>        words = sents[<span class="hljs-number">3</span>*i].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        postags = sents[<span class="hljs-number">3</span>*i+<span class="hljs-number">1</span>].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        tags = sents[<span class="hljs-number">3</span>*i+<span class="hljs-number">2</span>].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        <span class="hljs-keyword">for</span> word, postag, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(words, postags, tags):<br>            g.write(word+<span class="hljs-string">&#x27; &#x27;</span>+postag+<span class="hljs-string">&#x27; &#x27;</span>+tag+<span class="hljs-string">&#x27;\n&#x27;</span>)<br>        g.write(<span class="hljs-string">&#x27;\n&#x27;</span>)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/NER_test.data&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&quot;w&quot;</span>) <span class="hljs-keyword">as</span> h:<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(train_num+<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(sents)//<span class="hljs-number">3</span>):<br>        words = sents[<span class="hljs-number">3</span>*i].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        postags = sents[<span class="hljs-number">3</span>*i+<span class="hljs-number">1</span>].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        tags = sents[<span class="hljs-number">3</span>*i+<span class="hljs-number">2</span>].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        <span class="hljs-keyword">for</span> word, postag, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(words, postags, tags):<br>            h.write(word+<span class="hljs-string">&#x27; &#x27;</span>+postag+<span class="hljs-string">&#x27; &#x27;</span>+tag+<span class="hljs-string">&#x27;\n&#x27;</span>)<br>        h.write(<span class="hljs-string">&#x27;\n&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;OK!&#x27;</span>)<br></code></pre></td></tr></table></figure><p>运行此程序，得到NER_train.data, 此为训练集数据，NER_test.data，此为测试集数据。NER_train.data的前20行数据如下（以\t分隔开来）：</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs mathematica"><span class="hljs-variable">played</span> <span class="hljs-variable">VBD</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">on</span> <span class="hljs-variable">IN</span> <span class="hljs-built_in">O</span><br><span class="hljs-built_in">Monday</span> <span class="hljs-variable">NNP</span> <span class="hljs-built_in">O</span><br><span class="hljs-punctuation">(</span> <span class="hljs-punctuation">(</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">home</span> <span class="hljs-variable">NN</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">team</span> <span class="hljs-variable">NN</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">in</span> <span class="hljs-variable">IN</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">CAPS</span> <span class="hljs-variable">NNP</span> <span class="hljs-built_in">O</span><br><span class="hljs-punctuation">)</span> <span class="hljs-punctuation">)</span> <span class="hljs-built_in">O</span><br><span class="hljs-operator">:</span> <span class="hljs-operator">:</span> <span class="hljs-built_in">O</span><br><br><span class="hljs-variable">American</span> <span class="hljs-variable">NNP</span> <span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">MISC</span><br><span class="hljs-variable">League</span> <span class="hljs-variable">NNP</span> <span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">MISC</span><br><br><span class="hljs-variable">Cleveland</span> <span class="hljs-variable">NNP</span> <span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">ORG</span><br><span class="hljs-number">2</span> <span class="hljs-variable">CD</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">DETROIT</span> <span class="hljs-variable">NNP</span> <span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">ORG</span><br><span class="hljs-number">1</span> <span class="hljs-variable">CD</span> <span class="hljs-built_in">O</span><br><br><span class="hljs-variable">BALTIMORE</span> <span class="hljs-variable">VB</span> <span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">ORG</span><br></code></pre></td></tr></table></figure><p>我们使用的模板文件template内容如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># Unigram</span><br><span class="hljs-attribute">U00</span>:%x[-<span class="hljs-number">2</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U01</span>:%x[-<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U02</span>:%x[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U03</span>:%x[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U04</span>:%x[<span class="hljs-number">2</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U05</span>:%x[-<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]/%x[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U06</span>:%x[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]/%x[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]<br><br><span class="hljs-attribute">U10</span>:%x[-<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U11</span>:%x[-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U12</span>:%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U13</span>:%x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U14</span>:%x[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U15</span>:%x[-<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]/%x[-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U16</span>:%x[-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U17</span>:%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U18</span>:%x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]<br><br><span class="hljs-attribute">U20</span>:%x[-<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]/%x[-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U21</span>:%x[-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U22</span>:%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># Bigram</span><br><span class="hljs-attribute">B</span><br></code></pre></td></tr></table></figure><p>接着训练该数据，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">crf_learn -c 3.0 template NER_train.data model -t<br></code></pre></td></tr></table></figure><p>运行时的输出信息如下：</p><p><img src="/img/nlp8_3.jpeg" alt=""></p><p>在笔者的电脑上一共迭代了193次，运行时间为490.32秒，标记错误率为0.00004，句子错误率为0.00056。</p><p>接着，我们需要在测试集上对该模型的预测表现做评估。预测命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">crf_test -m model NER_test.data &gt; result.txt<br></code></pre></td></tr></table></figure><p>使用Python脚本统计预测的准确率，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-built_in">dir</span> = <span class="hljs-string">&quot;/Users/Shared/CRF_4_NER/CRF_TEST&quot;</span><br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/result.txt&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    sents = [line.strip() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines() <span class="hljs-keyword">if</span> line.strip()]<br><br>total = <span class="hljs-built_in">len</span>(sents)<br><span class="hljs-built_in">print</span>(total)<br><br>count = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>    words = sent.split()<br>    <span class="hljs-comment"># print(words)</span><br>    <span class="hljs-keyword">if</span> words[-<span class="hljs-number">1</span>] == words[-<span class="hljs-number">2</span>]:<br>        count += <span class="hljs-number">1</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy: %.4f&quot;</span> %(count/total))<br><span class="hljs-comment"># 0.9706</span><br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">21487</span><br><span class="hljs-attribute">Accuracy</span>: <span class="hljs-number">0</span>.<span class="hljs-number">9706</span><br></code></pre></td></tr></table></figure><p>由此可见，在测试集上的准确率高达0.9706，效果相当好。</p><p>最后，我们对新数据进行命名实体识别，看看模型在新数据上的识别效果。实现的Python代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> nltk<br><br><span class="hljs-built_in">dir</span> = <span class="hljs-string">&quot;/Users/Shared/CRF_4_NER/CRF_TEST&quot;</span><br><br>sentence = <span class="hljs-string">&quot;Venezuelan opposition leader and self-proclaimed interim president Juan Guaidó said Thursday he will return to his country by Monday, and that a dialogue with President Nicolas Maduro won&#x27;t be possible without discussing elections.&quot;</span><br><span class="hljs-comment">#sentence = &quot;Real Madrid&#x27;s season on the brink after 3-0 Barcelona defeat&quot;</span><br><span class="hljs-comment"># sentence = &quot;British artist David Hockney is known as a voracious smoker, but the habit got him into a scrape in Amsterdam on Wednesday.&quot;</span><br><span class="hljs-comment"># sentence = &quot;India is waiting for the release of an pilot who has been in Pakistani custody since he was shot down over Kashmir on Wednesday, a goodwill gesture which could defuse the gravest crisis in the disputed border region in years.&quot;</span><br><span class="hljs-comment"># sentence = &quot;Instead, President Donald Trump&#x27;s second meeting with North Korean despot Kim Jong Un ended in a most uncharacteristic fashion for a showman commander in chief: fizzle.&quot;</span><br><span class="hljs-comment"># sentence = &quot;And in a press conference at the Civic Leadership Academy in Queens, de Blasio said the program is already working.&quot;</span><br><span class="hljs-comment">#sentence = &quot;The United States is a founding member of the United Nations, World Bank, International Monetary Fund.&quot;</span><br><br>default_wt = nltk.word_tokenize <span class="hljs-comment"># 分词</span><br>words = default_wt(sentence)<br><span class="hljs-built_in">print</span>(words)<br>postags = nltk.pos_tag(words)<br><span class="hljs-built_in">print</span>(postags)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/NER_predict.data&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> postags:<br>        f.write(item[<span class="hljs-number">0</span>]+<span class="hljs-string">&#x27; &#x27;</span>+item[<span class="hljs-number">1</span>]+<span class="hljs-string">&#x27; O\n&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;write successfully!&quot;</span>)<br><br>os.chdir(<span class="hljs-built_in">dir</span>)<br>os.system(<span class="hljs-string">&quot;crf_test -m model NER_predict.data &gt; predict.txt&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;get predict file!&quot;</span>)<br><br><span class="hljs-comment"># 读取预测文件redict.txt</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/predict.txt&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    sents = [line.strip() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines() <span class="hljs-keyword">if</span> line.strip()]<br><br>word = []<br>predict = []<br><br><span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>    words = sent.split()<br>    word.append(words[<span class="hljs-number">0</span>])<br>    predict.append(words[-<span class="hljs-number">1</span>])<br><br><span class="hljs-comment"># print(word)</span><br><span class="hljs-comment"># print(predict)</span><br><br><span class="hljs-comment"># 去掉NER标注为O的元素</span><br>ner_reg_list = []<br><span class="hljs-keyword">for</span> word, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(word, predict):<br>    <span class="hljs-keyword">if</span> tag != <span class="hljs-string">&#x27;O&#x27;</span>:<br>        ner_reg_list.append((word, tag))<br><br><span class="hljs-comment"># 输出模型的NER识别结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;NER识别结果：&quot;</span>)<br><span class="hljs-keyword">if</span> ner_reg_list:<br>    <span class="hljs-keyword">for</span> i, item <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(ner_reg_list):<br>        <span class="hljs-keyword">if</span> item[<span class="hljs-number">1</span>].startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>            end = i+<span class="hljs-number">1</span><br>            <span class="hljs-keyword">while</span> end &lt;= <span class="hljs-built_in">len</span>(ner_reg_list)-<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> ner_reg_list[end][<span class="hljs-number">1</span>].startswith(<span class="hljs-string">&#x27;I&#x27;</span>):<br>                end += <span class="hljs-number">1</span><br><br>            ner_type = item[<span class="hljs-number">1</span>].split(<span class="hljs-string">&#x27;-&#x27;</span>)[<span class="hljs-number">1</span>]<br>            ner_type_dict = &#123;<span class="hljs-string">&#x27;PER&#x27;</span>: <span class="hljs-string">&#x27;PERSON: &#x27;</span>,<br>                             <span class="hljs-string">&#x27;LOC&#x27;</span>: <span class="hljs-string">&#x27;LOCATION: &#x27;</span>,<br>                             <span class="hljs-string">&#x27;ORG&#x27;</span>: <span class="hljs-string">&#x27;ORGANIZATION: &#x27;</span>,<br>                             <span class="hljs-string">&#x27;MISC&#x27;</span>: <span class="hljs-string">&#x27;MISC: &#x27;</span><br>                            &#125;<br>            <span class="hljs-built_in">print</span>(ner_type_dict[ner_type], <span class="hljs-string">&#x27; &#x27;</span>.join([item[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> ner_reg_list[i:end]]))<br></code></pre></td></tr></table></figure><p>识别的结果如下：</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">MISC:</span>  Venezuelan<br><span class="hljs-symbol">PERSON:</span>  Juan Guaidó<br><span class="hljs-symbol">PERSON:</span>  Nicolas Maduro<br></code></pre></td></tr></table></figure><p>识别有个地方不准确， Venezuelan应该是LOC，而不是MISC. 我们再接着测试其它的新数据：</p><p>输入语句1：</p><blockquote><p>Real Madrid’s season on the brink after 3-0 Barcelona defeat</p></blockquote><p>识别效果1：</p><blockquote><p>ORGANIZATION:  Real Madrid<br>LOCATION:  Barcelona</p></blockquote><p>输入语句2：</p><blockquote><p>British artist David Hockney is known as a voracious smoker, but the habit got him into a scrape in Amsterdam on Wednesday.</p></blockquote><p>识别效果2：</p><blockquote><p>MISC:  British<br>PERSON:  David Hockney<br>LOCATION:  Amsterdam</p></blockquote><p>输入语句3：</p><blockquote><p>India is waiting for the release of an pilot who has been in Pakistani custody since he was shot down over Kashmir on Wednesday, a goodwill gesture which could defuse the gravest crisis in the disputed border region in years.</p></blockquote><p>识别效果3：</p><blockquote><p>LOCATION:  India<br>LOCATION:  Pakistani<br>LOCATION:  Kashmir</p></blockquote><p>输入语句4：</p><blockquote><p>Instead, President Donald Trump’s second meeting with North Korean despot Kim Jong Un ended in a most uncharacteristic fashion for a showman commander in chief: fizzle.</p></blockquote><p>识别效果4：</p><blockquote><p>PERSON:  Donald Trump<br>PERSON:  Kim Jong Un</p></blockquote><p>输入语句5：</p><blockquote><p>And in a press conference at the Civic Leadership Academy in Queens, de Blasio said the program is already working.</p></blockquote><p>识别效果5：</p><blockquote><p>ORGANIZATION:  Civic Leadership Academy<br>LOCATION:  Queens<br>PERSON:  de Blasio</p></blockquote><p>输入语句6：</p><blockquote><p>The United States is a founding member of the United Nations, World Bank, International Monetary Fund.</p></blockquote><p>识别效果6：</p><blockquote><p>LOCATION:  United States<br>ORGANIZATION:  United Nations<br>PERSON:  World Bank<br>ORGANIZATION:  International Monetary Fund</p></blockquote><p>在这些例子中，有让我们惊喜之处：识别出了人物Donald Trump, Kim Jong Un. 但也有些不足指出，如将World Bank识别为人物，而不是组织机构。总的来说，识别效果还是让人满意的。</p><h3 id="总结">总结</h3><p>最近由于工作繁忙，无暇顾及博客。但转念一想，技术输出也是比较重要的，需要长期坚持下去～</p><p>本项目的Github地址为：<a href="https://github.com/percent4/CRF_4_NER">https://github.com/percent4/CRF_4_NER</a> 。</p><p>五一将至，祝大家假期愉快～</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>NER</tag>
      
      <tag>CRF++</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（七）中文预处理之繁简体转换及获取拼音</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%83%EF%BC%89%E4%B8%AD%E6%96%87%E9%A2%84%E5%A4%84%E7%90%86%E4%B9%8B%E7%B9%81%E7%AE%80%E4%BD%93%E8%BD%AC%E6%8D%A2%E5%8F%8A%E8%8E%B7%E5%8F%96%E6%8B%BC%E9%9F%B3/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%83%EF%BC%89%E4%B8%AD%E6%96%87%E9%A2%84%E5%A4%84%E7%90%86%E4%B9%8B%E7%B9%81%E7%AE%80%E4%BD%93%E8%BD%AC%E6%8D%A2%E5%8F%8A%E8%8E%B7%E5%8F%96%E6%8B%BC%E9%9F%B3/</url>
    
    <content type="html"><![CDATA[<p>在日常的中文NLP中，经常会涉及到中文的繁简体转换以及拼音的标注等问题，本文将介绍这两个方面的实现。</p><p>首先是中文的繁简体转换，不需要使用额外的Python模块，至需要以下两个Python代码文件即可：</p><ul><li><p><a href="http://langconv.py">langconv.py</a> 地址： <a href="https://raw.githubusercontent.com/skydark/nstools/master/zhtools/langconv.py">https://raw.githubusercontent.com/skydark/nstools/master/zhtools/langconv.py</a></p></li><li><p>zh_wiki.py 地址：<a href="https://raw.githubusercontent.com/skydark/nstools/master/zhtools/zh_wiki.py">https://raw.githubusercontent.com/skydark/nstools/master/zhtools/zh_wiki.py</a></p><p>示例代码如下（将代码文件与langconv.py与zh_wiki.py放在同一目录下）：</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langconv <span class="hljs-keyword">import</span> *<br><br><span class="hljs-comment"># 转换繁体到简体</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cht_2_chs</span>(<span class="hljs-params">line</span>):<br>    line = Converter(<span class="hljs-string">&#x27;zh-hans&#x27;</span>).convert(line)<br>    line.encode(<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>    <span class="hljs-keyword">return</span> line<br><br>line_cht= <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">台北市長柯文哲今在臉書開直播，先向網友報告自己3月16日至24日要出訪美國東部4城市，接著他無預警宣布，</span><br><span class="hljs-string">2月23日要先出訪以色列，預計停留4至5天。雖他強調台北市、以色列已在資安方面有所交流，也可到當地城市交流、</span><br><span class="hljs-string">參觀產業創新等內容，但柯也說「也是去看看一個小國在這麼惡劣環境，howtosurvive，他的祕訣是什麼？」這番話，</span><br><span class="hljs-string">也被解讀，頗有更上層樓、直指總統大位的思維。</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br>line_cht = line_cht.replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>ret_chs = cht_2_chs(line_cht)<br><span class="hljs-built_in">print</span>(ret_chs)<br><br><span class="hljs-comment"># 转换简体到繁体</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">chs_2_cht</span>(<span class="hljs-params">sentence</span>):<br>    sentence = Converter(<span class="hljs-string">&#x27;zh-hant&#x27;</span>).convert(sentence)<br>    <span class="hljs-keyword">return</span> sentence<br><br>line_chs = <span class="hljs-string">&#x27;忧郁的台湾乌龟&#x27;</span><br>line_cht = chs_2_cht(line_chs)<br><span class="hljs-built_in">print</span>(line_cht)<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><blockquote><p>台北市长柯文哲今在脸书开直播，先向网友报告自己3月16日至24日要出访美国东部4城市，接着他无预警宣布，2月23日要先出访以色列，预计停留4至5天。虽他强调台北市、以色列已在资安方面有所交流，也可到当地城市交流、参观产业创新等内容，但柯也说「也是去看看一个小国在这么恶劣环境，howtosurvive，他的祕诀是什么？」这番话，也被解读，颇有更上层楼、直指总统大位的思维。<br>憂郁的臺灣烏龜</p></blockquote><p>接着是获取中文汉字的拼音，这方面的Python模块有xpinyin, pypinyin等。本文以xpinyin为例，展示如何获取汉字的拼音。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> xpinyin <span class="hljs-keyword">import</span> Pinyin<br><br>p = Pinyin()<br><br><span class="hljs-comment"># 默认分隔符为-</span><br><span class="hljs-built_in">print</span>(p.get_pinyin(<span class="hljs-string">&quot;上海&quot;</span>))<br><br><span class="hljs-comment"># 显示声调</span><br><span class="hljs-built_in">print</span>(p.get_pinyin(<span class="hljs-string">&quot;上海&quot;</span>, tone_marks=<span class="hljs-string">&#x27;marks&#x27;</span>))<br><span class="hljs-built_in">print</span>(p.get_pinyin(<span class="hljs-string">&quot;上海&quot;</span>, tone_marks=<span class="hljs-string">&#x27;numbers&#x27;</span>))<br><br><span class="hljs-comment"># 去掉分隔符</span><br><span class="hljs-built_in">print</span>(p.get_pinyin(<span class="hljs-string">&quot;上海&quot;</span>, <span class="hljs-string">&#x27;&#x27;</span>))<br><span class="hljs-comment"># 设为分隔符为空格</span><br><span class="hljs-built_in">print</span>(p.get_pinyin(<span class="hljs-string">&quot;上海&quot;</span>, <span class="hljs-string">&#x27; &#x27;</span>))<br><br><span class="hljs-comment"># 获取拼音首字母</span><br><span class="hljs-built_in">print</span>(p.get_initial(<span class="hljs-string">&quot;上&quot;</span>))<br><span class="hljs-built_in">print</span>(p.get_initials(<span class="hljs-string">&quot;上海&quot;</span>))<br><span class="hljs-built_in">print</span>(p.get_initials(<span class="hljs-string">&quot;上海&quot;</span>, <span class="hljs-string">&#x27;&#x27;</span>))<br><span class="hljs-built_in">print</span>(p.get_initials(<span class="hljs-string">&quot;上海&quot;</span>, <span class="hljs-string">&#x27; &#x27;</span>))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">shang-hai</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">shàng-hǎi</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">shang4-hai3</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">shanghai</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">shang </span>hai<br>S<br>S-H<br><span class="hljs-keyword">SH</span><br><span class="hljs-keyword"></span>S H<br></code></pre></td></tr></table></figure><p>本次分享到此结束，感谢大家阅读~</p><p>注意：本人现已开通微信公众号： NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>繁简体转换</tag>
      
      <tag>拼音</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（六）pyltp的介绍与使用</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AD%EF%BC%89pyltp%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AD%EF%BC%89pyltp%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h3 id="pyltp的简介">pyltp的简介</h3><p>语言技术平台(LTP)经过哈工大社会计算与信息检索研究中心 11年的持续研发和推广，是国内外最具影响力的中文处理基础平台。它提供的功能包括中文分词、词性标注、命名实体识别、依存句法分析、语义角色标注等。</p><figure><img src="/img/nlp6_1.png" alt="语言技术平台架构" /><figcaption aria-hidden="true">语言技术平台架构</figcaption></figure><p>pyltp 是 LTP 的 Python封装，同时支持Python2和Python3版本。Python3的安装方法为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip3 install pyltp<br></code></pre></td></tr></table></figure><ul><li><p>官网下载网址：https://pypi.org/project/pyltp/0.1.7/</p></li><li><p>官方使用说明文档：https://pyltp.readthedocs.io/zh_CN/develop/api.html</p><p>在使用该模块前，需要下载完整的模型文件，文件下载地址为：<ahref="https://pan.baidu.com/share/link?shareid=1988562907&amp;uk=2738088569#list/path=%2F">https://pan.baidu.com/share/link?shareid=1988562907&amp;uk=2738088569#list/path=%2F</a>。pyltp 的所有输入的分析文本和输出的结果的编码均为UTF-8。模型的数据文件如下：</p></li></ul><figure><img src="/img/nlp6_2.png" alt="模型数据" /><figcaption aria-hidden="true">模型数据</figcaption></figure><p>其中，cws.model用于分词模型，lexicon.txt为分词时添加的用户字典，ner.model为命名实体识别模型，parser.model为依存句法分析模型，pisrl.model为语义角色标注模型，pos为词性标注模型。</p><h3 id="pyltp的使用">pyltp的使用</h3><p>pyltp的使用示例项目结构如下：</p><figure><img src="/img/nlp6_3.png" alt="示例项目" /><figcaption aria-hidden="true">示例项目</figcaption></figure><h5 id="分句">分句</h5><p>分句指的是将一段话或一片文章中的文字按句子分开，按句子形成独立的单元。示例的Python代码sentenct_split.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> SentenceSplitter<br><br><span class="hljs-comment"># 分句</span><br>doc = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span> \<br>      <span class="hljs-string">&#x27;盖茨原计划从明年1月9日至14日陆续访问中国和日本，目前，他决定在行程中增加对韩国的访问。莫莱尔表示，&#x27;</span> \<br>      <span class="hljs-string">&#x27;盖茨在访韩期间将会晤韩国国防部长官金宽镇，就朝鲜近日的行动交换意见，同时商讨加强韩美两军同盟关系等问题，&#x27;</span> \<br>      <span class="hljs-string">&#x27;拟定共同应对朝鲜挑衅和核计划的方案。&#x27;</span><br>sents = SentenceSplitter.split(doc)  <span class="hljs-comment"># 分句</span><br><br><br><span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>    <span class="hljs-built_in">print</span>(sent)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs dns">据韩联社<span class="hljs-number">12</span>月<span class="hljs-number">28</span>日反映，美国防部发言人杰夫·莫莱尔<span class="hljs-number">27</span>日表示，美国防部长盖茨将于<span class="hljs-number">2011年1月14</span>日访问韩国。<br>盖茨原计划从明年<span class="hljs-number">1</span>月<span class="hljs-number">9</span>日至<span class="hljs-number">14</span>日陆续访问中国和日本，目前，他决定在行程中增加对韩国的访问。<br>莫莱尔表示，盖茨在访韩期间将会晤韩国国防部长官金宽镇，就朝鲜近日的行动交换意见，同时商讨加强韩美两军同盟关系等问题，拟定共同应对朝鲜挑衅和核计划的方案。<br></code></pre></td></tr></table></figure><h5 id="分词">分词</h5><p>分词指的是将一句话按词语分开，按词语形成独立的单元。示例的Python代码words_split.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor<br><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;/&#x27;</span>.join(words))<br><br>segmentor.release()<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">据<span class="hljs-regexp">/韩联社/</span><span class="hljs-number">12</span>月<span class="hljs-regexp">/28日/</span>反映<span class="hljs-regexp">/，/</span>美<span class="hljs-regexp">/国防部/</span>发言人<span class="hljs-regexp">/杰夫·莫莱尔/</span><span class="hljs-number">27</span>日<span class="hljs-regexp">/表示/</span>，<span class="hljs-regexp">/美/</span>国防部长<span class="hljs-regexp">/盖茨/</span>将<span class="hljs-regexp">/于/</span><span class="hljs-number">2011</span>年<span class="hljs-regexp">/1月/</span><span class="hljs-number">14</span>日<span class="hljs-regexp">/访问/</span>韩国/。<br></code></pre></td></tr></table></figure><h5 id="词性标注">词性标注</h5><p>词性标注指的是一句话分完词后，制定每个词语的词性。示例的Python代码postagger.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor, Postagger<br><br><span class="hljs-comment"># 分词</span><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br><br>postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br>postags = postagger.postag(words)  <span class="hljs-comment"># 词性标注</span><br><br><span class="hljs-keyword">for</span> word, postag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(words, postags):<br>    <span class="hljs-built_in">print</span>(word, postag)<br><br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">词性标注结果说明</span><br><span class="hljs-string">https://ltp.readthedocs.io/zh_CN/latest/appendix.html#id3</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs excel">据 p<br>韩联社 ni<br><span class="hljs-number">12</span>月 nt<br><span class="hljs-number">28</span>日 nt<br>反映 v<br>， wp<br>美 j<br>国防部 <span class="hljs-built_in">n</span><br>发言人 <span class="hljs-built_in">n</span><br>杰夫·莫莱尔 nh<br><span class="hljs-number">27</span>日 nt<br>表示 v<br>， wp<br>美 j<br>国防部长 <span class="hljs-built_in">n</span><br>盖茨 nh<br>将 d<br>于 p<br><span class="hljs-number">2011</span>年 nt<br><span class="hljs-number">1</span>月 nt<br><span class="hljs-number">14</span>日 nt<br>访问 v<br>韩国 ns<br>。 wp<br></code></pre></td></tr></table></figure><p>词性标注结果可参考网址：<ahref="https://ltp.readthedocs.io/zh_CN/latest/appendix.html">https://ltp.readthedocs.io/zh_CN/latest/appendix.html</a>。</p><h5 id="命名实体识别">命名实体识别</h5><p>命名实体识别（NER）指的是识别出一句话或一段话或一片文章中的命名实体，比如人名，地名，组织机构名。示例的Python代码ner.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor, Postagger<br><br><span class="hljs-comment"># 分词</span><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br><br>postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br>postags = postagger.postag(words)  <span class="hljs-comment"># 词性标注</span><br><br><br>ner_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/ner.model&#x27;</span>)   <span class="hljs-comment"># 命名实体识别模型路径，模型名称为`pos.model`</span><br><br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> NamedEntityRecognizer<br>recognizer = NamedEntityRecognizer() <span class="hljs-comment"># 初始化实例</span><br>recognizer.load(ner_model_path)  <span class="hljs-comment"># 加载模型</span><br><span class="hljs-comment"># netags = recognizer.recognize(words, postags)  # 命名实体识别</span><br><br><br><span class="hljs-comment"># 提取识别结果中的人名，地名，组织机构名</span><br><br>persons, places, orgs = <span class="hljs-built_in">set</span>(), <span class="hljs-built_in">set</span>(), <span class="hljs-built_in">set</span>()<br><br><br>netags = <span class="hljs-built_in">list</span>(recognizer.recognize(words, postags))  <span class="hljs-comment"># 命名实体识别</span><br><span class="hljs-built_in">print</span>(netags)<br><span class="hljs-comment"># print(netags)</span><br>i = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> tag, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(netags, words):<br>    j = i<br>    <span class="hljs-comment"># 人名</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;Nh&#x27;</span> <span class="hljs-keyword">in</span> tag:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;S&#x27;</span>):<br>            persons.add(word)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>            union_person = word<br>            <span class="hljs-keyword">while</span> netags[j] != <span class="hljs-string">&#x27;E-Nh&#x27;</span>:<br>                j += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> j &lt; <span class="hljs-built_in">len</span>(words):<br>                    union_person += words[j]<br>            persons.add(union_person)<br>    <span class="hljs-comment"># 地名</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;Ns&#x27;</span> <span class="hljs-keyword">in</span> tag:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;S&#x27;</span>):<br>            places.add(word)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>            union_place = word<br>            <span class="hljs-keyword">while</span> netags[j] != <span class="hljs-string">&#x27;E-Ns&#x27;</span>:<br>                j += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> j &lt; <span class="hljs-built_in">len</span>(words):<br>                    union_place += words[j]<br>            places.add(union_place)<br>    <span class="hljs-comment"># 机构名</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;Ni&#x27;</span> <span class="hljs-keyword">in</span> tag:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;S&#x27;</span>):<br>            orgs.add(word)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>            union_org = word<br>            <span class="hljs-keyword">while</span> netags[j] != <span class="hljs-string">&#x27;E-Ni&#x27;</span>:<br>                j += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> j &lt; <span class="hljs-built_in">len</span>(words):<br>                    union_org += words[j]<br>            orgs.add(union_org)<br><br>    i += <span class="hljs-number">1</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;人名：&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>.join(persons))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;地名：&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>.join(places))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;组织机构：&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>.join(orgs))<br><br><br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br>recognizer.release()<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Ni&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-Ni&#x27;</span>, <span class="hljs-string">&#x27;E-Ni&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Nh&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Ns&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Nh&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Ns&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>人名： 杰夫·莫莱尔，盖茨<br>地名： 美，韩国<br>组织机构： 韩联社，美国防部<br></code></pre></td></tr></table></figure><p>命名实体识别结果可参考网址：<ahref="https://ltp.readthedocs.io/zh_CN/latest/appendix.html">https://ltp.readthedocs.io/zh_CN/latest/appendix.html</a>。</p><h4 id="依存句法分析">依存句法分析</h4><p>依存语法 (Dependency Parsing, DP)通过分析语言单位内成分之间的依存关系揭示其句法结构。直观来讲，依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关系。示例的Python代码parser.py代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor, Postagger, Parser<br><br><span class="hljs-comment"># 分词</span><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br><br>postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br>postags = postagger.postag(words)  <span class="hljs-comment"># 词性标注</span><br><br><br><span class="hljs-comment"># 依存句法分析</span><br>par_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/parser.model&#x27;</span>)  <span class="hljs-comment"># 模型路径，模型名称为`parser.model`</span><br><br>parser = Parser() <span class="hljs-comment"># 初始化实例</span><br>parser.load(par_model_path)  <span class="hljs-comment"># 加载模型</span><br>arcs = parser.parse(words, postags)  <span class="hljs-comment"># 句法分析</span><br><br>rely_id = [arc.head <span class="hljs-keyword">for</span> arc <span class="hljs-keyword">in</span> arcs]  <span class="hljs-comment"># 提取依存父节点id</span><br>relation = [arc.relation <span class="hljs-keyword">for</span> arc <span class="hljs-keyword">in</span> arcs]  <span class="hljs-comment"># 提取依存关系</span><br>heads = [<span class="hljs-string">&#x27;Root&#x27;</span> <span class="hljs-keyword">if</span> <span class="hljs-built_in">id</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> words[<span class="hljs-built_in">id</span>-<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> rely_id]  <span class="hljs-comment"># 匹配依存父节点词语</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(words)):<br>    <span class="hljs-built_in">print</span>(relation[i] + <span class="hljs-string">&#x27;(&#x27;</span> + words[i] + <span class="hljs-string">&#x27;, &#x27;</span> + heads[i] + <span class="hljs-string">&#x27;)&#x27;</span>)<br><br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br>parser.release()<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(据, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">SBV</span><span class="hljs-params">(韩联社, 反映)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">12</span>月, <span class="hljs-number">28</span>日)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(<span class="hljs-number">28</span>日, 反映)</span></span><br><span class="hljs-function"><span class="hljs-title">POB</span><span class="hljs-params">(反映, 据)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(，, 据)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(美, 国防部)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(国防部, 发言人)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(发言人, 杰夫·莫莱尔)</span></span><br><span class="hljs-function"><span class="hljs-title">SBV</span><span class="hljs-params">(杰夫·莫莱尔, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(<span class="hljs-number">27</span>日, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">HED</span><span class="hljs-params">(表示, Root)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(，, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(美, 国防部长)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(国防部长, 盖茨)</span></span><br><span class="hljs-function"><span class="hljs-title">SBV</span><span class="hljs-params">(盖茨, 访问)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(将, 访问)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(于, 访问)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">2011</span>年, <span class="hljs-number">14</span>日)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">1</span>月, <span class="hljs-number">14</span>日)</span></span><br><span class="hljs-function"><span class="hljs-title">POB</span><span class="hljs-params">(<span class="hljs-number">14</span>日, 于)</span></span><br><span class="hljs-function"><span class="hljs-title">VOB</span><span class="hljs-params">(访问, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">VOB</span><span class="hljs-params">(韩国, 访问)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(。, 表示)</span></span><br></code></pre></td></tr></table></figure><p>依存句法分析结果可参考网址：<ahref="https://ltp.readthedocs.io/zh_CN/latest/appendix.html">https://ltp.readthedocs.io/zh_CN/latest/appendix.html</a>。</p><h5 id="语义角色标注">语义角色标注</h5><p>语义角色标注是实现浅层语义分析的一种方式。在一个句子中，谓词是对主语的陈述或说明，指出“做什么”、“是什么”或“怎么样，代表了一个事件的核心，跟谓词搭配的名词称为论元。语义角色是指论元在动词所指事件中担任的角色。主要有：施事者（Agent）、受事者（Patient）、客体（Theme）、经验者（Experiencer）、受益者（Beneficiary）、工具（Instrument）、处所（Location）、目标（Goal）和来源（Source）等。示例的Python代码rolelabel.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor, Postagger, Parser, SementicRoleLabeller<br><br><span class="hljs-comment"># 分词</span><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br><br>postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br>postags = postagger.postag(words)  <span class="hljs-comment"># 词性标注</span><br><br><span class="hljs-comment"># 依存句法分析</span><br>par_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/parser.model&#x27;</span>)  <span class="hljs-comment"># 模型路径，模型名称为`parser.model`</span><br><br>parser = Parser() <span class="hljs-comment"># 初始化实例</span><br>parser.load(par_model_path)  <span class="hljs-comment"># 加载模型</span><br>arcs = parser.parse(words, postags)  <span class="hljs-comment"># 句法分析</span><br><br><span class="hljs-comment"># 语义角色标注</span><br>srl_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pisrl.model&#x27;</span>)  <span class="hljs-comment"># 语义角色标注模型目录路径</span><br>labeller = SementicRoleLabeller() <span class="hljs-comment"># 初始化实例</span><br>labeller.load(srl_model_path)  <span class="hljs-comment"># 加载模型</span><br>roles = labeller.label(words, postags, arcs)  <span class="hljs-comment"># 语义角色标注</span><br><br><span class="hljs-comment"># 打印结果</span><br><span class="hljs-keyword">for</span> role <span class="hljs-keyword">in</span> roles:<br>    <span class="hljs-built_in">print</span>(words[role.index], end=<span class="hljs-string">&#x27; &#x27;</span>)<br>    <span class="hljs-built_in">print</span>(role.index, <span class="hljs-string">&quot;&quot;</span>.join([<span class="hljs-string">&quot;%s:(%d,%d)&quot;</span> % (arg.name, arg.<span class="hljs-built_in">range</span>.start, arg.<span class="hljs-built_in">range</span>.end) <span class="hljs-keyword">for</span> arg <span class="hljs-keyword">in</span> role.arguments]))<br><br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br>parser.release()<br>labeller.release()<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">反映 <span class="hljs-number">4</span> <span class="hljs-built_in">A0</span>:(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<span class="hljs-built_in">A0</span>:(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<br>表示 <span class="hljs-number">11</span> MNR:(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>)<span class="hljs-built_in">A0</span>:(<span class="hljs-number">6</span>,<span class="hljs-number">9</span>)TMP:(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>)<span class="hljs-built_in">A1</span>:(<span class="hljs-number">13</span>,<span class="hljs-number">22</span>)<br>访问 <span class="hljs-number">21</span> <span class="hljs-built_in">A0</span>:(<span class="hljs-number">13</span>,<span class="hljs-number">15</span>)ADV:(<span class="hljs-number">16</span>,<span class="hljs-number">16</span>)TMP:(<span class="hljs-number">17</span>,<span class="hljs-number">20</span>)<span class="hljs-built_in">A1</span>:(<span class="hljs-number">22</span>,<span class="hljs-number">22</span>)<br></code></pre></td></tr></table></figure><h3 id="总结">总结</h3><p>本文介绍了中文NLP的一个杰出工具pyltp，并给出了该模块的各个功能的一个示例，希望能给读者一些思考与启示。本文到此结束，感谢大家阅读~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>pyltp</tag>
      
      <tag>NLP工具</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（五）用深度学习实现命名实体识别（NER）</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%94%EF%BC%89%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%94%EF%BC%89%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>在文章：<a href="https://percent4.github.io/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/">NLP入门（四）命名实体识别（NER）</a>中，笔者介绍了两个实现命名实体识别的工具——NLTK和Stanford NLP。在本文中，我们将会学习到如何使用深度学习工具来自己一步步地实现NER，只要你坚持看完，就一定会很有收获的。</p><p>OK，话不多说，让我们进入正题。</p><p>几乎所有的NLP都依赖一个强大的语料库，本项目实现NER的语料库如下(文件名为train.txt，一共42000行，这里只展示前15行，可以在文章最后的Github地址下载该语料库)：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">playedonMonday(hometeam<span class="hljs-keyword">in</span>CAPS):<br>VBDINNNP(NNNNINNNP):<br>OOOOOOOOOO<br>AmericanLeague<br>NNPNNP<br>B-MISCI-MISC<br>Cleveland2DETROIT1<br>NNPCDNNPCD<br>B-ORGOB-ORGO<br>BALTIMORE12Oakland11(10innings)<br>VBCDNNPCD(CDNN)<br>B-ORGOB-ORGOOOOO<br>TORONTO5Minnesota3<br>TOCDNNPCD<br>B-ORGOB-ORGO<br>......<br></code></pre></td></tr></table></figure><p>简单介绍下该语料库的结构：该语料库一共42000行，每三行为一组，其中，第一行为英语句子，第二行为每个句子的词性（关于英语单词的词性，可参考文章：<a href="https://percent4.github.io/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F%EF%BC%88Lemmatization%EF%BC%89/">NLP入门（三）词形还原（Lemmatization）</a>），第三行为NER系统的标注，具体的含义会在之后介绍。</p><p>我们的NER项目的名称为DL_4_NER，结构如下：</p><p><img src="/img/nlp5_1.png" alt="NER项目名称"></p><p>项目中每个文件的功能如下：</p><ul><li><p><a href="http://utils.py">utils.py</a>: 项目配置及数据导入</p></li><li><p>data_processing.py: 数据探索</p></li><li><p>Bi_LSTM_Model_training.py: 模型创建及训练</p></li><li><p>Bi_LSTM_Model_predict.py: 对新句子进行NER预测</p><p>接下来，笔者将结合代码文件，分部介绍该项目的步骤，当所有步骤介绍完毕后，我们的项目就结束了，而你，也就知道了如何用深度学习实现命名实体识别（NER）。</p><p>Let’s begin!</p></li></ul><h3 id="项目配置">项目配置</h3><p>第一步，是项目的配置及数据导入，在utils.py文件中实现，完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><span class="hljs-comment"># basic settings for DL_4_NER Project</span><br>BASE_DIR = <span class="hljs-string">&quot;F://NERSystem&quot;</span><br>CORPUS_PATH = <span class="hljs-string">&quot;%s/train.txt&quot;</span> % BASE_DIR<br><br>KERAS_MODEL_SAVE_PATH = <span class="hljs-string">&#x27;%s/Bi-LSTM-4-NER.h5&#x27;</span> % BASE_DIR<br>WORD_DICTIONARY_PATH = <span class="hljs-string">&#x27;%s/word_dictionary.pk&#x27;</span> % BASE_DIR<br>InVERSE_WORD_DICTIONARY_PATH = <span class="hljs-string">&#x27;%s/inverse_word_dictionary.pk&#x27;</span> % BASE_DIR<br>LABEL_DICTIONARY_PATH = <span class="hljs-string">&#x27;%s/label_dictionary.pk&#x27;</span> % BASE_DIR<br>OUTPUT_DICTIONARY_PATH = <span class="hljs-string">&#x27;%s/output_dictionary.pk&#x27;</span> % BASE_DIR<br><br>CONSTANTS = [<br>             KERAS_MODEL_SAVE_PATH,<br>             InVERSE_WORD_DICTIONARY_PATH,<br>             WORD_DICTIONARY_PATH,<br>             LABEL_DICTIONARY_PATH,<br>             OUTPUT_DICTIONARY_PATH<br>             ]<br><br><span class="hljs-comment"># load data from corpus to from pandas DataFrame</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>():<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CORPUS_PATH, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        text_data = [text.strip() <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> f.readlines()]<br>    text_data = [text_data[k].split(<span class="hljs-string">&#x27;\t&#x27;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(text_data))]<br>    index = <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(text_data), <span class="hljs-number">3</span>)<br><br>    <span class="hljs-comment"># Transforming data to matrix format for neural network</span><br>    input_data = <span class="hljs-built_in">list</span>()<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(index) - <span class="hljs-number">1</span>):<br>        rows = text_data[index[i-<span class="hljs-number">1</span>]:index[i]]<br>        sentence_no = np.array([i]*<span class="hljs-built_in">len</span>(rows[<span class="hljs-number">0</span>]), dtype=<span class="hljs-built_in">str</span>)<br>        rows.append(sentence_no)<br>        rows = np.array(rows).T<br>        input_data.append(rows)<br><br>    input_data = pd.DataFrame(np.concatenate([item <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> input_data]),\<br>                               columns=[<span class="hljs-string">&#x27;word&#x27;</span>, <span class="hljs-string">&#x27;pos&#x27;</span>, <span class="hljs-string">&#x27;tag&#x27;</span>, <span class="hljs-string">&#x27;sent_no&#x27;</span>])<br><br>    <span class="hljs-keyword">return</span> input_data<br></code></pre></td></tr></table></figure><p>在该代码中，先是设置了语料库文件的路径CORPUS_PATH，KERAS模型保存路径KERAS_MODEL_SAVE_PATH，以及在项目过程中会用到的三个字典的保存路径（以pickle文件形式保存）WORD_DICTIONARY_PATH，LABEL_DICTIONARY_PATH， OUTPUT_DICTIONARY_PATH。然后是load_data()函数，它将语料库中的文本以Pandas中的DataFrame结构展示出来，该数据框的前30行如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs apache">         <span class="hljs-attribute">word</span>  pos     tag sent_no<br><span class="hljs-attribute">0</span>      played  VBD       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">1</span>          <span class="hljs-literal">on</span>   IN       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">2</span>      Monday  NNP       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">3</span>           (    (       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">4</span>        home   NN       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">5</span>        team   NN       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">6</span>          in   IN       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">7</span>        CAPS  NNP       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">8</span>           )    )       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">9</span>           :    :       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">10</span>   American  NNP  B-MISC       <span class="hljs-number">2</span><br><span class="hljs-attribute">11</span>     League  NNP  I-MISC       <span class="hljs-number">2</span><br><span class="hljs-attribute">12</span>  Cleveland  NNP   B-ORG       <span class="hljs-number">3</span><br><span class="hljs-attribute">13</span>          <span class="hljs-number">2</span>   CD       O       <span class="hljs-number">3</span><br><span class="hljs-attribute">14</span>    DETROIT  NNP   B-ORG       <span class="hljs-number">3</span><br><span class="hljs-attribute">15</span>          <span class="hljs-number">1</span>   CD       O       <span class="hljs-number">3</span><br><span class="hljs-attribute">16</span>  BALTIMORE   VB   B-ORG       <span class="hljs-number">4</span><br><span class="hljs-attribute">17</span>         <span class="hljs-number">12</span>   CD       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">18</span>    Oakland  NNP   B-ORG       <span class="hljs-number">4</span><br><span class="hljs-attribute">19</span>         <span class="hljs-number">11</span>   CD       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">20</span>          (    (       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">21</span>         <span class="hljs-number">10</span>   CD       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">22</span>    innings   NN       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">23</span>          )    )       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">24</span>    TORONTO   TO   B-ORG       <span class="hljs-number">5</span><br><span class="hljs-attribute">25</span>          <span class="hljs-number">5</span>   CD       O       <span class="hljs-number">5</span><br><span class="hljs-attribute">26</span>  Minnesota  NNP   B-ORG       <span class="hljs-number">5</span><br><span class="hljs-attribute">27</span>          <span class="hljs-number">3</span>   CD       O       <span class="hljs-number">5</span><br><span class="hljs-attribute">28</span>  Milwaukee  NNP   B-ORG       <span class="hljs-number">6</span><br><span class="hljs-attribute">29</span>          <span class="hljs-number">3</span>   CD       O       <span class="hljs-number">6</span><br></code></pre></td></tr></table></figure><p>在该数据框中，word这一列表示文本语料库中的单词，pos这一列表示该单词的词性，tag这一列表示NER的标注，sent_no这一列表示该单词在第几个句子中。</p><h3 id="数据探索">数据探索</h3><p>接着，第二步是数据探索，即对输入的数据（input_data）进行一些数据review，完整的代码（data_processing.py）如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> accumulate<br><span class="hljs-keyword">from</span> operator <span class="hljs-keyword">import</span> itemgetter<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib <span class="hljs-keyword">as</span> mpl<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> BASE_DIR, CONSTANTS, load_data<br><br><span class="hljs-comment"># 设置matplotlib绘图时的字体</span><br>mpl.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>]=[<span class="hljs-string">&#x27;SimHei&#x27;</span>]<br><br><span class="hljs-comment"># 数据查看</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_review</span>():<br><br>    <span class="hljs-comment"># 数据导入</span><br>    input_data = load_data()<br><br>    <span class="hljs-comment"># 基本的数据review</span><br>    sent_num = input_data[<span class="hljs-string">&#x27;sent_no&#x27;</span>].astype(np.<span class="hljs-built_in">int</span>).<span class="hljs-built_in">max</span>()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;一共有%s个句子。\n&quot;</span>%sent_num)<br><br>    vocabulary = input_data[<span class="hljs-string">&#x27;word&#x27;</span>].unique()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;一共有%d个单词。&quot;</span>%<span class="hljs-built_in">len</span>(vocabulary))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;前10个单词为：%s.\n&quot;</span>%vocabulary[:<span class="hljs-number">11</span>])<br><br>    pos_arr = input_data[<span class="hljs-string">&#x27;pos&#x27;</span>].unique()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;单词的词性列表：%s.\n&quot;</span>%pos_arr)<br><br>    ner_tag_arr = input_data[<span class="hljs-string">&#x27;tag&#x27;</span>].unique()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;NER的标注列表：%s.\n&quot;</span> % ner_tag_arr)<br><br>    df = input_data[[<span class="hljs-string">&#x27;word&#x27;</span>, <span class="hljs-string">&#x27;sent_no&#x27;</span>]].groupby(<span class="hljs-string">&#x27;sent_no&#x27;</span>).count()<br>    sent_len_list = df[<span class="hljs-string">&#x27;word&#x27;</span>].tolist()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;句子长度及出现频数字典：\n%s.&quot;</span> % <span class="hljs-built_in">dict</span>(Counter(sent_len_list)))<br><br>    <span class="hljs-comment"># 绘制句子长度及出现频数统计图</span><br>    sort_sent_len_dist = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">dict</span>(Counter(sent_len_list)).items(), key=itemgetter(<span class="hljs-number">0</span>))<br>    sent_no_data = [item[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sort_sent_len_dist]<br>    sent_count_data = [item[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sort_sent_len_dist]<br>    plt.bar(sent_no_data, sent_count_data)<br>    plt.title(<span class="hljs-string">&quot;句子长度及出现频数统计图&quot;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;句子长度&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;句子长度出现的频数&quot;</span>)<br>    plt.savefig(<span class="hljs-string">&quot;%s/句子长度及出现频数统计图.png&quot;</span> % BASE_DIR)<br>    plt.close()<br><br>    <span class="hljs-comment"># 绘制句子长度累积分布函数(CDF)</span><br>    sent_pentage_list = [(count/sent_num) <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> accumulate(sent_count_data)]<br><br>    <span class="hljs-comment"># 寻找分位点为quantile的句子长度</span><br>    quantile = <span class="hljs-number">0.9992</span><br>    <span class="hljs-comment">#print(list(sent_pentage_list))</span><br>    <span class="hljs-keyword">for</span> length, per <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(sent_no_data, sent_pentage_list):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">round</span>(per, <span class="hljs-number">4</span>) == quantile:<br>            index = length<br>            <span class="hljs-keyword">break</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n分位点为%s的句子长度:%d.&quot;</span> % (quantile, index))<br><br>    <span class="hljs-comment"># 绘制CDF</span><br>    plt.plot(sent_no_data, sent_pentage_list)<br>    plt.hlines(quantile, <span class="hljs-number">0</span>, index, colors=<span class="hljs-string">&quot;c&quot;</span>, linestyles=<span class="hljs-string">&quot;dashed&quot;</span>)<br>    plt.vlines(index, <span class="hljs-number">0</span>, quantile, colors=<span class="hljs-string">&quot;c&quot;</span>, linestyles=<span class="hljs-string">&quot;dashed&quot;</span>)<br>    plt.text(<span class="hljs-number">0</span>, quantile, <span class="hljs-built_in">str</span>(quantile))<br>    plt.text(index, <span class="hljs-number">0</span>, <span class="hljs-built_in">str</span>(index))<br>    plt.title(<span class="hljs-string">&quot;句子长度累积分布函数图&quot;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;句子长度&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;句子长度累积频率&quot;</span>)<br>    plt.savefig(<span class="hljs-string">&quot;%s/句子长度累积分布函数图.png&quot;</span> % BASE_DIR)<br>    plt.close()<br><br><span class="hljs-comment"># 数据处理</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_processing</span>():<br>    <span class="hljs-comment"># 数据导入</span><br>    input_data = load_data()<br><br>    <span class="hljs-comment"># 标签及词汇表</span><br>    labels, vocabulary = <span class="hljs-built_in">list</span>(input_data[<span class="hljs-string">&#x27;tag&#x27;</span>].unique()), <span class="hljs-built_in">list</span>(input_data[<span class="hljs-string">&#x27;word&#x27;</span>].unique())<br><br>    <span class="hljs-comment"># 字典列表</span><br>    word_dictionary = &#123;word: i+<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocabulary)&#125;<br>    inverse_word_dictionary = &#123;i+<span class="hljs-number">1</span>: word <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocabulary)&#125;<br>    label_dictionary = &#123;label: i+<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels)&#125;<br>    output_dictionary = &#123;i+<span class="hljs-number">1</span>: labels <span class="hljs-keyword">for</span> i, labels <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels)&#125;<br><br>    dict_list = [word_dictionary, inverse_word_dictionary,label_dictionary, output_dictionary]<br><br>    <span class="hljs-comment"># 保存为pickle形式</span><br>    <span class="hljs-keyword">for</span> dict_item, path <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(dict_list, CONSTANTS[<span class="hljs-number">1</span>:]):<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(path, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            pickle.dump(dict_item, f)<br><br><span class="hljs-comment">#data_review()</span><br></code></pre></td></tr></table></figure><p>调用data_review()函数，输出的结果如下：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs prolog">一共有<span class="hljs-number">13998</span>个句子。<br><br>一共有<span class="hljs-number">24339</span>个单词。<br>前<span class="hljs-number">10</span>个单词为：[<span class="hljs-string">&#x27;played&#x27;</span> <span class="hljs-string">&#x27;on&#x27;</span> <span class="hljs-string">&#x27;Monday&#x27;</span> <span class="hljs-string">&#x27;(&#x27;</span> <span class="hljs-string">&#x27;home&#x27;</span> <span class="hljs-string">&#x27;team&#x27;</span> <span class="hljs-string">&#x27;in&#x27;</span> <span class="hljs-string">&#x27;CAPS&#x27;</span> <span class="hljs-string">&#x27;)&#x27;</span> <span class="hljs-string">&#x27;:&#x27;</span> <span class="hljs-string">&#x27;American&#x27;</span>].<br><br>单词的词性列表：[<span class="hljs-string">&#x27;VBD&#x27;</span> <span class="hljs-string">&#x27;IN&#x27;</span> <span class="hljs-string">&#x27;NNP&#x27;</span> <span class="hljs-string">&#x27;(&#x27;</span> <span class="hljs-string">&#x27;NN&#x27;</span> <span class="hljs-string">&#x27;)&#x27;</span> <span class="hljs-string">&#x27;:&#x27;</span> <span class="hljs-string">&#x27;CD&#x27;</span> <span class="hljs-string">&#x27;VB&#x27;</span> <span class="hljs-string">&#x27;TO&#x27;</span> <span class="hljs-string">&#x27;NNS&#x27;</span> <span class="hljs-string">&#x27;,&#x27;</span> <span class="hljs-string">&#x27;VBP&#x27;</span> <span class="hljs-string">&#x27;VBZ&#x27;</span><br> <span class="hljs-string">&#x27;.&#x27;</span> <span class="hljs-string">&#x27;VBG&#x27;</span> <span class="hljs-string">&#x27;PRP$&#x27;</span> <span class="hljs-string">&#x27;JJ&#x27;</span> <span class="hljs-string">&#x27;CC&#x27;</span> <span class="hljs-string">&#x27;JJS&#x27;</span> <span class="hljs-string">&#x27;RB&#x27;</span> <span class="hljs-string">&#x27;DT&#x27;</span> <span class="hljs-string">&#x27;VBN&#x27;</span> <span class="hljs-string">&#x27;&quot;&#x27;</span> <span class="hljs-string">&#x27;PRP&#x27;</span> <span class="hljs-string">&#x27;WDT&#x27;</span> <span class="hljs-string">&#x27;WRB&#x27;</span><br> <span class="hljs-string">&#x27;MD&#x27;</span> <span class="hljs-string">&#x27;WP&#x27;</span> <span class="hljs-string">&#x27;POS&#x27;</span> <span class="hljs-string">&#x27;JJR&#x27;</span> <span class="hljs-string">&#x27;WP$&#x27;</span> <span class="hljs-string">&#x27;RP&#x27;</span> <span class="hljs-string">&#x27;NNPS&#x27;</span> <span class="hljs-string">&#x27;RBS&#x27;</span> <span class="hljs-string">&#x27;FW&#x27;</span> <span class="hljs-string">&#x27;$&#x27;</span> <span class="hljs-string">&#x27;RBR&#x27;</span> <span class="hljs-string">&#x27;EX&#x27;</span> <span class="hljs-string">&quot;&#x27;&#x27;&quot;</span><br> <span class="hljs-string">&#x27;PDT&#x27;</span> <span class="hljs-string">&#x27;UH&#x27;</span> <span class="hljs-string">&#x27;SYM&#x27;</span> <span class="hljs-string">&#x27;LS&#x27;</span> <span class="hljs-string">&#x27;NN|SYM&#x27;</span>].<br><br><span class="hljs-symbol">NER</span>的标注列表：[<span class="hljs-string">&#x27;O&#x27;</span> <span class="hljs-string">&#x27;B-MISC&#x27;</span> <span class="hljs-string">&#x27;I-MISC&#x27;</span> <span class="hljs-string">&#x27;B-ORG&#x27;</span> <span class="hljs-string">&#x27;I-ORG&#x27;</span> <span class="hljs-string">&#x27;B-PER&#x27;</span> <span class="hljs-string">&#x27;B-LOC&#x27;</span> <span class="hljs-string">&#x27;I-PER&#x27;</span> <span class="hljs-string">&#x27;I-LOC&#x27;</span><br> <span class="hljs-string">&#x27;sO&#x27;</span>].<br><br>句子长度及出现频数字典：<br>&#123;<span class="hljs-number">1</span>: <span class="hljs-number">177</span>, <span class="hljs-number">2</span>: <span class="hljs-number">1141</span>, <span class="hljs-number">3</span>: <span class="hljs-number">620</span>, <span class="hljs-number">4</span>: <span class="hljs-number">794</span>, <span class="hljs-number">5</span>: <span class="hljs-number">769</span>, <span class="hljs-number">6</span>: <span class="hljs-number">639</span>, <span class="hljs-number">7</span>: <span class="hljs-number">999</span>, <span class="hljs-number">8</span>: <span class="hljs-number">977</span>, <span class="hljs-number">9</span>: <span class="hljs-number">841</span>, <span class="hljs-number">10</span>: <span class="hljs-number">501</span>, <span class="hljs-number">11</span>: <span class="hljs-number">395</span>, <span class="hljs-number">12</span>: <span class="hljs-number">316</span>, <span class="hljs-number">13</span>: <span class="hljs-number">339</span>, <span class="hljs-number">14</span>: <span class="hljs-number">291</span>, <span class="hljs-number">15</span>: <span class="hljs-number">275</span>, <span class="hljs-number">16</span>: <span class="hljs-number">225</span>, <span class="hljs-number">17</span>: <span class="hljs-number">229</span>, <span class="hljs-number">18</span>: <span class="hljs-number">212</span>, <span class="hljs-number">19</span>: <span class="hljs-number">197</span>, <span class="hljs-number">20</span>: <span class="hljs-number">221</span>, <span class="hljs-number">21</span>: <span class="hljs-number">228</span>, <span class="hljs-number">22</span>: <span class="hljs-number">221</span>, <span class="hljs-number">23</span>: <span class="hljs-number">230</span>, <span class="hljs-number">24</span>: <span class="hljs-number">210</span>, <span class="hljs-number">25</span>: <span class="hljs-number">207</span>, <span class="hljs-number">26</span>: <span class="hljs-number">224</span>, <span class="hljs-number">27</span>: <span class="hljs-number">188</span>, <span class="hljs-number">28</span>: <span class="hljs-number">199</span>, <span class="hljs-number">29</span>: <span class="hljs-number">214</span>, <span class="hljs-number">30</span>: <span class="hljs-number">183</span>, <span class="hljs-number">31</span>: <span class="hljs-number">202</span>, <span class="hljs-number">32</span>: <span class="hljs-number">167</span>, <span class="hljs-number">33</span>: <span class="hljs-number">167</span>, <span class="hljs-number">34</span>: <span class="hljs-number">141</span>, <span class="hljs-number">35</span>: <span class="hljs-number">130</span>, <span class="hljs-number">36</span>: <span class="hljs-number">119</span>, <span class="hljs-number">37</span>: <span class="hljs-number">105</span>, <span class="hljs-number">38</span>: <span class="hljs-number">112</span>, <span class="hljs-number">39</span>: <span class="hljs-number">98</span>, <span class="hljs-number">40</span>: <span class="hljs-number">78</span>, <span class="hljs-number">41</span>: <span class="hljs-number">74</span>, <span class="hljs-number">42</span>: <span class="hljs-number">63</span>, <span class="hljs-number">43</span>: <span class="hljs-number">51</span>, <span class="hljs-number">44</span>: <span class="hljs-number">42</span>, <span class="hljs-number">45</span>: <span class="hljs-number">39</span>, <span class="hljs-number">46</span>: <span class="hljs-number">19</span>, <span class="hljs-number">47</span>: <span class="hljs-number">22</span>, <span class="hljs-number">48</span>: <span class="hljs-number">19</span>, <span class="hljs-number">49</span>: <span class="hljs-number">15</span>, <span class="hljs-number">50</span>: <span class="hljs-number">16</span>, <span class="hljs-number">51</span>: <span class="hljs-number">8</span>, <span class="hljs-number">52</span>: <span class="hljs-number">9</span>, <span class="hljs-number">53</span>: <span class="hljs-number">5</span>, <span class="hljs-number">54</span>: <span class="hljs-number">4</span>, <span class="hljs-number">55</span>: <span class="hljs-number">9</span>, <span class="hljs-number">56</span>: <span class="hljs-number">2</span>, <span class="hljs-number">57</span>: <span class="hljs-number">2</span>, <span class="hljs-number">58</span>: <span class="hljs-number">2</span>, <span class="hljs-number">59</span>: <span class="hljs-number">2</span>, <span class="hljs-number">60</span>: <span class="hljs-number">3</span>, <span class="hljs-number">62</span>: <span class="hljs-number">2</span>, <span class="hljs-number">66</span>: <span class="hljs-number">1</span>, <span class="hljs-number">67</span>: <span class="hljs-number">1</span>, <span class="hljs-number">69</span>: <span class="hljs-number">1</span>, <span class="hljs-number">71</span>: <span class="hljs-number">1</span>, <span class="hljs-number">72</span>: <span class="hljs-number">1</span>, <span class="hljs-number">78</span>: <span class="hljs-number">1</span>, <span class="hljs-number">80</span>: <span class="hljs-number">1</span>, <span class="hljs-number">113</span>: <span class="hljs-number">1</span>, <span class="hljs-number">124</span>: <span class="hljs-number">1</span>&#125;.<br><br>分位点为<span class="hljs-number">0.9992</span>的句子长度:<span class="hljs-number">60.</span><br></code></pre></td></tr></table></figure><p>在该语料库中，一共有13998个句子，比预期的42000/3=14000个句子少两个。一个有24339个单词，单词量还是蛮大的，当然，这里对单词没有做任何处理，直接保留了语料库中的形式（后期可以继续优化）。单词的词性可以参考文章：<a href="https://www.jianshu.com/p/79255fe0c5b5">NLP入门（三）词形还原（Lemmatization）</a>。我们需要注意的是，NER的标注列表为[‘O’ ,‘B-MISC’, ‘I-MISC’, ‘B-ORG’ ,‘I-ORG’, ‘B-PER’ ,‘B-LOC’ ,‘I-PER’, ‘I-LOC’,‘sO’]，因此，本项目的NER一共分为四类：PER（人名），LOC（位置），ORG（组织）以及MISC，其中B表示开始，I表示中间，O表示单字词，不计入NER，sO表示特殊单字词。</p><p>接下来，让我们考虑下句子的长度，这对后面的建模时填充的句子长度有有参考作用。句子长度及出现频数的统计图如下：</p><p><img src="/img/nlp5_2.png" alt="句子长度及出现频数统计图"></p><p>可以看到，句子长度基本在60以下，当然，这也可以在输出的句子长度及出现频数字典中看到。那么，我们是否可以选在一个标准作为后面模型的句子填充的长度呢？答案是，利用出现频数的累计分布函数的分位点，在这里，我们选择分位点为0.9992,对应的句子长度为60，如下图：</p><p><img src="/img/nlp5_3.png" alt="句子长度累积分布函数图"></p><p>接着是数据处理函数data_processing()，它的功能主要是实现单词、标签字典，并保存为pickle文件形式，便于后续直接调用。</p><h3 id="建模">建模</h3><p>在第三步中，我们建立Bi-LSTM模型来训练训练，完整的Python代码（Bi_LSTM_Model_training.py）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> BASE_DIR, CONSTANTS, load_data<br><span class="hljs-keyword">from</span> data_processing <span class="hljs-keyword">import</span> data_processing<br><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> np_utils, plot_model<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Bidirectional, LSTM, Dense, Embedding, TimeDistributed<br><br><br><span class="hljs-comment"># 模型输入数据</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">input_data_for_model</span>(<span class="hljs-params">input_shape</span>):<br><br>    <span class="hljs-comment"># 数据导入</span><br>    input_data = load_data()<br>    <span class="hljs-comment"># 数据处理</span><br>    data_processing()<br>    <span class="hljs-comment"># 导入字典</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">1</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        word_dictionary = pickle.load(f)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">2</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        inverse_word_dictionary = pickle.load(f)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">3</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        label_dictionary = pickle.load(f)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">4</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        output_dictionary = pickle.load(f)<br>    vocab_size = <span class="hljs-built_in">len</span>(word_dictionary.keys())<br>    label_size = <span class="hljs-built_in">len</span>(label_dictionary.keys())<br><br>    <span class="hljs-comment"># 处理输入数据</span><br>    aggregate_function = <span class="hljs-keyword">lambda</span> <span class="hljs-built_in">input</span>: [(word, pos, label) <span class="hljs-keyword">for</span> word, pos, label <span class="hljs-keyword">in</span><br>                                            <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;word&#x27;</span>].values.tolist(),<br>                                                <span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;pos&#x27;</span>].values.tolist(),<br>                                                <span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;tag&#x27;</span>].values.tolist())]<br><br>    grouped_input_data = input_data.groupby(<span class="hljs-string">&#x27;sent_no&#x27;</span>).apply(aggregate_function)<br>    sentences = [sentence <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> grouped_input_data]<br><br>    x = [[word_dictionary[word[<span class="hljs-number">0</span>]] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sentences]<br>    x = pad_sequences(maxlen=input_shape, sequences=x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br>    y = [[label_dictionary[word[<span class="hljs-number">2</span>]] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sentences]<br>    y = pad_sequences(maxlen=input_shape, sequences=y, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br>    y = [np_utils.to_categorical(label, num_classes=label_size + <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> y]<br><br>    <span class="hljs-keyword">return</span> x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary<br><br><br><span class="hljs-comment"># 定义深度学习模型：Bi-LSTM</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_Bi_LSTM</span>(<span class="hljs-params">vocab_size, label_size, input_shape, output_dim, n_units, out_act, activation</span>):<br>    model = Sequential()<br>    model.add(Embedding(input_dim=vocab_size + <span class="hljs-number">1</span>, output_dim=output_dim,<br>                        input_length=input_shape, mask_zero=<span class="hljs-literal">True</span>))<br>    model.add(Bidirectional(LSTM(units=n_units, activation=activation,<br>                                 return_sequences=<span class="hljs-literal">True</span>)))<br>    model.add(TimeDistributed(Dense(label_size + <span class="hljs-number">1</span>, activation=out_act)))<br>    model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>, loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-comment"># 模型训练</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_train</span>():<br><br>    <span class="hljs-comment"># 将数据集分为训练集和测试集，占比为9:1</span><br>    input_shape = <span class="hljs-number">60</span><br>    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = input_data_for_model(input_shape)<br>    train_end = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(x)*<span class="hljs-number">0.9</span>)<br>    train_x, train_y = x[<span class="hljs-number">0</span>:train_end], np.array(y[<span class="hljs-number">0</span>:train_end])<br>    test_x, test_y = x[train_end:], np.array(y[train_end:])<br><br>    <span class="hljs-comment"># 模型输入参数</span><br>    activation = <span class="hljs-string">&#x27;selu&#x27;</span><br>    out_act = <span class="hljs-string">&#x27;softmax&#x27;</span><br>    n_units = <span class="hljs-number">100</span><br>    batch_size = <span class="hljs-number">32</span><br>    epochs = <span class="hljs-number">10</span><br>    output_dim = <span class="hljs-number">20</span><br><br>    <span class="hljs-comment"># 模型训练</span><br>    lstm_model = create_Bi_LSTM(vocab_size, label_size, input_shape, output_dim, n_units, out_act, activation)<br>    lstm_model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 模型保存</span><br>    model_save_path = CONSTANTS[<span class="hljs-number">0</span>]<br>    lstm_model.save(model_save_path)<br>    plot_model(lstm_model, to_file=<span class="hljs-string">&#x27;%s/LSTM_model.png&#x27;</span> % BASE_DIR)<br><br>    <span class="hljs-comment"># 在测试集上的效果</span><br>    N = test_x.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 测试的条数</span><br>    avg_accuracy = <span class="hljs-number">0</span>  <span class="hljs-comment"># 预测的平均准确率</span><br>    <span class="hljs-keyword">for</span> start, end <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, N, <span class="hljs-number">1</span>), <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, N+<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)):<br>        sentence = [inverse_word_dictionary[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> test_x[start] <span class="hljs-keyword">if</span> i != <span class="hljs-number">0</span>]<br>        y_predict = lstm_model.predict(test_x[start:end])<br>        input_sequences, output_sequences = [], []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(y_predict[<span class="hljs-number">0</span>])):<br>            output_sequences.append(np.argmax(y_predict[<span class="hljs-number">0</span>][i]))<br>            input_sequences.append(np.argmax(test_y[start][i]))<br><br>        <span class="hljs-built_in">eval</span> = lstm_model.evaluate(test_x[start:end], test_y[start:end])<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test Accuracy: loss = %0.6f accuracy = %0.2f%%&#x27;</span> % (<span class="hljs-built_in">eval</span>[<span class="hljs-number">0</span>], <span class="hljs-built_in">eval</span>[<span class="hljs-number">1</span>] * <span class="hljs-number">100</span>))<br>        avg_accuracy += <span class="hljs-built_in">eval</span>[<span class="hljs-number">1</span>]<br>        output_sequences = <span class="hljs-string">&#x27; &#x27;</span>.join([output_dictionary[key] <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> output_sequences <span class="hljs-keyword">if</span> key != <span class="hljs-number">0</span>]).split()<br>        input_sequences = <span class="hljs-string">&#x27; &#x27;</span>.join([output_dictionary[key] <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> input_sequences <span class="hljs-keyword">if</span> key != <span class="hljs-number">0</span>]).split()<br>        output_input_comparison = pd.DataFrame([sentence, output_sequences, input_sequences]).T<br>        <span class="hljs-built_in">print</span>(output_input_comparison.dropna())<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;#&#x27;</span> * <span class="hljs-number">80</span>)<br><br>    avg_accuracy /= N<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;测试样本的平均预测准确率：%.2f%%.&quot;</span> % (avg_accuracy * <span class="hljs-number">100</span>))<br><br>model_train()<br></code></pre></td></tr></table></figure><p>在上面的代码中，先是通过input_data_for_model()函数来处理好进入模型的数据，其参数为input_shape，即填充句子时的长度。然后是创建Bi-LSTM模型create_Bi_LSTM()，模型的示意图如下：</p><p><img src="/img/nlp5_4.png" alt="模型示意图"></p><p>最后，是在输入的数据上进行模型训练，将原始的数据分为训练集和测试集，占比为9:1，训练的周期为10次。</p><h3 id="模型训练">模型训练</h3><p>运行上述模型训练代码，一共训练10个周期，训练时间大概为500s，在训练集上的准确率达99%以上，在测试集上的平均准确率为95%以上。以下是最后几个测试集上的预测结果：</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs clean">......(前面的输出已忽略)<br>Test Accuracy: loss = <span class="hljs-number">0.000986</span> accuracy = <span class="hljs-number">100.00</span>%<br>          <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>   Cardiff  B-ORG  B-ORG<br><span class="hljs-number">1</span>         <span class="hljs-number">1</span>      O      O<br><span class="hljs-number">2</span>  Brighton  B-ORG  B-ORG<br><span class="hljs-number">3</span>         <span class="hljs-number">0</span>      O      O<br>################################################################################<br><br><span class="hljs-number">1</span>/<span class="hljs-number">1</span> [==============================] - <span class="hljs-number">0</span>s <span class="hljs-number">10</span>ms/step<br>Test Accuracy: loss = <span class="hljs-number">0.000274</span> accuracy = <span class="hljs-number">100.00</span>%<br>          <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>  Carlisle  B-ORG  B-ORG<br><span class="hljs-number">1</span>         <span class="hljs-number">0</span>      O      O<br><span class="hljs-number">2</span>      Hull  B-ORG  B-ORG<br><span class="hljs-number">3</span>         <span class="hljs-number">0</span>      O      O<br>################################################################################<br><br><span class="hljs-number">1</span>/<span class="hljs-number">1</span> [==============================] - <span class="hljs-number">0</span>s <span class="hljs-number">9</span>ms/step<br>Test Accuracy: loss = <span class="hljs-number">0.000479</span> accuracy = <span class="hljs-number">100.00</span>%<br>           <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>    Chester  B-ORG  B-ORG<br><span class="hljs-number">1</span>          <span class="hljs-number">1</span>      O      O<br><span class="hljs-number">2</span>  Cambridge  B-ORG  B-ORG<br><span class="hljs-number">3</span>          <span class="hljs-number">1</span>      O      O<br>################################################################################<br><br><span class="hljs-number">1</span>/<span class="hljs-number">1</span> [==============================] - <span class="hljs-number">0</span>s <span class="hljs-number">9</span>ms/step<br>Test Accuracy: loss = <span class="hljs-number">0.003092</span> accuracy = <span class="hljs-number">100.00</span>%<br>            <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>  Darlington  B-ORG  B-ORG<br><span class="hljs-number">1</span>           <span class="hljs-number">4</span>      O      O<br><span class="hljs-number">2</span>     Swansea  B-ORG  B-ORG<br><span class="hljs-number">3</span>           <span class="hljs-number">1</span>      O      O<br>################################################################################<br><br><span class="hljs-number">1</span>/<span class="hljs-number">1</span> [==============================] - <span class="hljs-number">0</span>s <span class="hljs-number">8</span>ms/step<br>Test Accuracy: loss = <span class="hljs-number">0.000705</span> accuracy = <span class="hljs-number">100.00</span>%<br>             <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>       Exeter  B-ORG  B-ORG<br><span class="hljs-number">1</span>            <span class="hljs-number">2</span>      O      O<br><span class="hljs-number">2</span>  Scarborough  B-ORG  B-ORG<br><span class="hljs-number">3</span>            <span class="hljs-number">2</span>      O      O<br>################################################################################<br>测试样本的平均预测准确率：<span class="hljs-number">95.55</span>%.<br></code></pre></td></tr></table></figure><p>该模型在原始数据上的识别效果还是可以的。</p><p>训练完模型后，BASE_DIR中的所有文件如下：</p><p><img src="/img/nlp5_5.png" alt="模型训练完后的所有文件截图"></p><h3 id="模型预测">模型预测</h3><p>最后，也许是整个项目最为激动人心的时刻，因为，我们要在新数据集上测试模型的识别效果。预测新数据的识别结果的完整Python代码（Bi_LSTM_Model_predict.py）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># Name entity recognition for new data</span><br><br><span class="hljs-comment"># Import the necessary modules</span><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> CONSTANTS<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<br><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize<br><br><span class="hljs-comment"># 导入字典</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">1</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    word_dictionary = pickle.load(f)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">4</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    output_dictionary = pickle.load(f)<br><br><span class="hljs-keyword">try</span>:<br>    <span class="hljs-comment"># 数据预处理</span><br>    input_shape = <span class="hljs-number">60</span><br>    sent = <span class="hljs-string">&#x27;New York is the biggest city in America.&#x27;</span><br>    new_sent = word_tokenize(sent)<br>    new_x = [[word_dictionary[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> new_sent]]<br>    x = pad_sequences(maxlen=input_shape, sequences=new_x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># 载入模型</span><br>    model_save_path = CONSTANTS[<span class="hljs-number">0</span>]<br>    lstm_model = load_model(model_save_path)<br><br>    <span class="hljs-comment"># 模型预测</span><br>    y_predict = lstm_model.predict(x)<br><br>    ner_tag = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(new_sent)):<br>        ner_tag.append(np.argmax(y_predict[<span class="hljs-number">0</span>][i]))<br><br>    ner = [output_dictionary[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ner_tag]<br>    <span class="hljs-built_in">print</span>(new_sent)<br>    <span class="hljs-built_in">print</span>(ner)<br><br>    <span class="hljs-comment"># 去掉NER标注为O的元素</span><br>    ner_reg_list = []<br>    <span class="hljs-keyword">for</span> word, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(new_sent, ner):<br>        <span class="hljs-keyword">if</span> tag != <span class="hljs-string">&#x27;O&#x27;</span>:<br>            ner_reg_list.append((word, tag))<br><br>    <span class="hljs-comment"># 输出模型的NER识别结果</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;NER识别结果：&quot;</span>)<br>    <span class="hljs-keyword">if</span> ner_reg_list:<br>        <span class="hljs-keyword">for</span> i, item <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(ner_reg_list):<br>            <span class="hljs-keyword">if</span> item[<span class="hljs-number">1</span>].startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>                end = i+<span class="hljs-number">1</span><br>                <span class="hljs-keyword">while</span> end &lt;= <span class="hljs-built_in">len</span>(ner_reg_list)-<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> ner_reg_list[end][<span class="hljs-number">1</span>].startswith(<span class="hljs-string">&#x27;I&#x27;</span>):<br>                    end += <span class="hljs-number">1</span><br><br>                ner_type = item[<span class="hljs-number">1</span>].split(<span class="hljs-string">&#x27;-&#x27;</span>)[<span class="hljs-number">1</span>]<br>                ner_type_dict = &#123;<span class="hljs-string">&#x27;PER&#x27;</span>: <span class="hljs-string">&#x27;PERSON: &#x27;</span>,<br>                                <span class="hljs-string">&#x27;LOC&#x27;</span>: <span class="hljs-string">&#x27;LOCATION: &#x27;</span>,<br>                                <span class="hljs-string">&#x27;ORG&#x27;</span>: <span class="hljs-string">&#x27;ORGANIZATION: &#x27;</span>,<br>                                <span class="hljs-string">&#x27;MISC&#x27;</span>: <span class="hljs-string">&#x27;MISC: &#x27;</span><br>                                &#125;<br>                <span class="hljs-built_in">print</span>(ner_type_dict[ner_type],\<br>                    <span class="hljs-string">&#x27; &#x27;</span>.join([item[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> ner_reg_list[i:end]]))<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型并未识别任何有效命名实体。&quot;</span>)<br><br><span class="hljs-keyword">except</span> KeyError <span class="hljs-keyword">as</span> err:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;您输入的句子有单词不在词汇表中，请重新输入！&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;不在词汇表中的单词为：%s.&quot;</span> % err)<br></code></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">[<span class="hljs-string">&#x27;New&#x27;</span>, <span class="hljs-string">&#x27;York&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;biggest&#x27;</span>, <span class="hljs-string">&#x27;city&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;America&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]<br>[<span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]<br>NER识别结果：<br><span class="hljs-keyword">LOCATION</span>:  <span class="hljs-built_in">New</span> York<br><span class="hljs-keyword">LOCATION</span>:  America<br></code></pre></td></tr></table></figure><p>接下来，再测试三个笔者自己想的句子：</p><p>输入为：</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">sent = <span class="hljs-symbol">&#x27;James</span> <span class="hljs-keyword">is</span> a world famous actor, whose home <span class="hljs-keyword">is</span> <span class="hljs-keyword">in</span> London.&#x27;<br></code></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;James&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;world&#x27;</span>, <span class="hljs-string">&#x27;famous&#x27;</span>, <span class="hljs-string">&#x27;actor&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;whose&#x27;</span>, <span class="hljs-string">&#x27;home&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;London&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>PERSON:  James<br>LOCATION:  London<br></code></pre></td></tr></table></figure><p>输入为：</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">sent = <span class="hljs-symbol">&#x27;Oxford</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">in</span> England, Jack <span class="hljs-keyword">is</span> from here.&#x27;<br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;Oxford&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;England&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;Jack&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;from&#x27;</span>, <span class="hljs-string">&#x27;here&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>PERSON:  Oxford<br>LOCATION:  England<br>PERSON:  Jack<br></code></pre></td></tr></table></figure><p>输入为：</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">sent</span> = <span class="hljs-string">&#x27;I love Shanghai.&#x27;</span><br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;Shanghai&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>LOCATION:  Shanghai<br></code></pre></td></tr></table></figure><p>在上面的例子中，只有Oxford的识别效果不理想，模型将它识别为PERSON，其实应该是ORGANIZATION。</p><p>接下来是三个来自CNN和wikipedia的句子：</p><p>输入为：</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">sent</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;the US runs the risk of a military defeat by China or Russia&quot;</span><br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">[<span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;US&#x27;</span>, <span class="hljs-string">&#x27;runs&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;risk&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;military&#x27;</span>, <span class="hljs-string">&#x27;defeat&#x27;</span>, <span class="hljs-string">&#x27;by&#x27;</span>, <span class="hljs-string">&#x27;China&#x27;</span>, <span class="hljs-string">&#x27;or&#x27;</span>, <span class="hljs-string">&#x27;Russia&#x27;</span>]<br>[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>]<br>NER识别结果：<br><span class="hljs-keyword">LOCATION</span>:  US<br><span class="hljs-keyword">LOCATION</span>:  China<br><span class="hljs-keyword">LOCATION</span>:  Russia<br></code></pre></td></tr></table></figure><p>输入为：</p><figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk">sent = <span class="hljs-comment">&quot;Home to the headquarters of the United Nations, New York is an important center for international diplomacy.&quot;</span><br></code></pre></td></tr></table></figure><p>输出为：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;Home&#x27;</span>, <span class="hljs-string">&#x27;to&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;headquarters&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;United&#x27;</span>, <span class="hljs-string">&#x27;Nations&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;New&#x27;</span>, <span class="hljs-string">&#x27;York&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;an&#x27;</span>, <span class="hljs-string">&#x27;important&#x27;</span>, <span class="hljs-string">&#x27;center&#x27;</span>, <span class="hljs-string">&#x27;for&#x27;</span>, <span class="hljs-string">&#x27;international&#x27;</span>, <span class="hljs-string">&#x27;diplomacy&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>ORGANIZATION:  United Nations<br>LOCATION:  New York<br></code></pre></td></tr></table></figure><p>输入为：</p><figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">sent</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;The United States is a founding member of the United Nations, World Bank, International Monetary Fund.&quot;</span><br></code></pre></td></tr></table></figure><p>输出为:</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;The&#x27;</span>, <span class="hljs-string">&#x27;United&#x27;</span>, <span class="hljs-string">&#x27;States&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;founding&#x27;</span>, <span class="hljs-string">&#x27;member&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;United&#x27;</span>, <span class="hljs-string">&#x27;Nations&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;World&#x27;</span>, <span class="hljs-string">&#x27;Bank&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;International&#x27;</span>, <span class="hljs-string">&#x27;Monetary&#x27;</span>, <span class="hljs-string">&#x27;Fund&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>LOCATION:  United States<br>ORGANIZATION:  United Nations<br>ORGANIZATION:  World Bank<br>ORGANIZATION:  International Monetary Fund<br></code></pre></td></tr></table></figure><p>这三个例子识别全部正确。</p><h3 id="总结">总结</h3><p>到这儿，笔者的这个项目就差不多了。我们有必要对这个项目做个总结。</p><p>首先是这个项目的优点。它的优点在于能够让你一步步地实现NER，而且除了语料库，你基本熟悉了如何创建一个识别NER系统的步骤，同时，对深度学习模型及其应用也有了深刻理解。因此，好处是显而易见的。当然，在实际工作中，语料库的整理才是最耗费时间的，能够占到90%或者更多的时间，因此，有一个好的语料库你才能展开工作。</p><p>接着讲讲这个项目的缺点。第一个，是语料库不够大，当然，约14000条句子也够了，但本项目没有对句子进行文本预处理，所以，有些单词的变形可能无法进入词汇表。第二个，缺少对新词的处理，一旦句子中出现一个新的单词，这个模型便无法处理，这是后期需要完善的地方。第三个，句子的填充长度为60，如果输入的句子长度大于60，则后面的部分将无法有效识别。</p><p>因此，后续还有更多的工作需要去做，当然，做一个中文NER也是可以考虑的。</p><p>本项目已上传Github,地址为 <a href="https://github.com/percent4/DL_4_NER">https://github.com/percent4/DL_4_NER</a> 。：欢迎大家参考~</p><p>注意：本人现已开通微信公众号： NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p><h3 id="参考文献">参考文献</h3><ol><li>BOOK： Applied Natural Language Processing with Python， Taweh Beysolow II</li><li>WEBSITE：<a href="https://github.com/Apress/applied-natural-language-processing-w-python">https://github.com/Apress/applied-natural-language-processing-w-python</a></li><li>WEBSITE: NLP入门（四）命名实体识别（NER）: <a href="https://www.jianshu.com/p/16e1f6a7aaef">https://www.jianshu.com/p/16e1f6a7aaef</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>NER</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（四）命名实体识别（NER）</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>本文将会简单介绍自然语言处理（NLP）中的命名实体识别（NER）。</p><p>命名实体识别（Named Entity Recognition，简称NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。</p><p>举个简单的例子，在句子“小明早上8点去学校上课。”中，对其进行命名实体识别，应该能提取信息</p><blockquote><p>人名：小明，时间：早上8点，地点：学校。</p></blockquote><p>本文将会介绍几个工具用来进行命名实体识别，后续有机会的话，我们将会尝试着用HMM、CRF或深度学习来实现命名实体识别。</p><p>首先我们来看一下NLTK和Stanford NLP中对命名实体识别的分类，如下图：</p><p><img src="/img/nlp4_1.png" alt="NLTK和Stanford NLP中对命名实体识别的分类"></p><p>在上图中，LOCATION和GPE有重合。GPE通常表示地理—政治条目，比如城市，州，国家，洲等。LOCATION除了上述内容外，还能表示名山大川等。FACILITY通常表示知名的纪念碑或人工制品等。</p><p>下面介绍两个工具来进行NER的任务：NLTK和Stanford NLP。</p><p>首先是NLTK，我们的示例文档（介绍FIFA，来源于维基百科）如下：</p><blockquote><p>FIFA was founded in 1904 to oversee international competition among the national associations of Belgium,<br>Denmark, France, Germany, the Netherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich, its<br>membership now comprises 211 national associations. Member countries must each also be members of one of<br>the six regional confederations into which the world is divided: Africa, Asia, Europe, North &amp; Central America<br>and the Caribbean, Oceania, and South America.</p></blockquote><p>实现NER的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> nltk<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_document</span>(<span class="hljs-params">document</span>):<br>   document = re.sub(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, document)<br>   <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(document, <span class="hljs-built_in">str</span>):<br>       document = document<br>   <span class="hljs-keyword">else</span>:<br>       <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Document is not string!&#x27;</span>)<br>   document = document.strip()<br>   sentences = nltk.sent_tokenize(document)<br>   sentences = [sentence.strip() <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences]<br>   <span class="hljs-keyword">return</span> sentences<br><br><span class="hljs-comment"># sample document</span><br>text = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">FIFA was founded in 1904 to oversee international competition among the national associations of Belgium, </span><br><span class="hljs-string">Denmark, France, Germany, the Netherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich, its </span><br><span class="hljs-string">membership now comprises 211 national associations. Member countries must each also be members of one of </span><br><span class="hljs-string">the six regional confederations into which the world is divided: Africa, Asia, Europe, North &amp; Central America </span><br><span class="hljs-string">and the Caribbean, Oceania, and South America.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># tokenize sentences</span><br>sentences = parse_document(text)<br>tokenized_sentences = [nltk.word_tokenize(sentence) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences]<br><span class="hljs-comment"># tag sentences and use nltk&#x27;s Named Entity Chunker</span><br>tagged_sentences = [nltk.pos_tag(sentence) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> tokenized_sentences]<br>ne_chunked_sents = [nltk.ne_chunk(tagged) <span class="hljs-keyword">for</span> tagged <span class="hljs-keyword">in</span> tagged_sentences]<br><span class="hljs-comment"># extract all named entities</span><br>named_entities = []<br><span class="hljs-keyword">for</span> ne_tagged_sentence <span class="hljs-keyword">in</span> ne_chunked_sents:<br>   <span class="hljs-keyword">for</span> tagged_tree <span class="hljs-keyword">in</span> ne_tagged_sentence:<br>       <span class="hljs-comment"># extract only chunks having NE labels</span><br>       <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(tagged_tree, <span class="hljs-string">&#x27;label&#x27;</span>):<br>           entity_name = <span class="hljs-string">&#x27; &#x27;</span>.join(c[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> tagged_tree.leaves()) <span class="hljs-comment">#get NE name</span><br>           entity_type = tagged_tree.label() <span class="hljs-comment"># get NE category</span><br>           named_entities.append((entity_name, entity_type))<br>           <span class="hljs-comment"># get unique named entities</span><br>           named_entities = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(named_entities))<br><br><span class="hljs-comment"># store named entities in a data frame</span><br>entity_frame = pd.DataFrame(named_entities, columns=[<span class="hljs-string">&#x27;Entity Name&#x27;</span>, <span class="hljs-string">&#x27;Entity Type&#x27;</span>])<br><span class="hljs-comment"># display results</span><br><span class="hljs-built_in">print</span>(entity_frame)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">        Entity Name   Entity Type<br><span class="hljs-number">0</span>              FIFA  <span class="hljs-keyword">ORGANIZATION</span><br><span class="hljs-keyword"></span><span class="hljs-number">1</span>   Central America  <span class="hljs-keyword">ORGANIZATION</span><br><span class="hljs-keyword"></span><span class="hljs-number">2</span>           <span class="hljs-keyword">Belgium </span>          GPE<br><span class="hljs-number">3</span>         Caribbean      LOCATION<br><span class="hljs-number">4</span>              Asia           GPE<br><span class="hljs-number">5</span>            France           GPE<br><span class="hljs-number">6</span>           Oceania           GPE<br><span class="hljs-number">7</span>           Germany           GPE<br><span class="hljs-number">8</span>     South America           GPE<br><span class="hljs-number">9</span>           Denmark           GPE<br><span class="hljs-number">10</span>           Zürich           GPE<br><span class="hljs-number">11</span>           Africa        PERSON<br><span class="hljs-number">12</span>           <span class="hljs-keyword">Sweden </span>          GPE<br><span class="hljs-number">13</span>      Netherlands           GPE<br><span class="hljs-number">14</span>            Spain           GPE<br><span class="hljs-number">15</span>      <span class="hljs-keyword">Switzerland </span>          GPE<br><span class="hljs-number">16</span>            <span class="hljs-keyword">North </span>          GPE<br><span class="hljs-number">17</span>           Europe           GPE<br></code></pre></td></tr></table></figure><p>可以看到，NLTK中的NER任务大体上完成得还是不错的，能够识别FIFA为组织（ORGANIZATION），Belgium,Asia为GPE, 但是也有一些不太如人意的地方，比如，它将Central America识别为ORGANIZATION，而实际上它应该为GPE；将Africa识别为PERSON，实际上应该为GPE。</p><p>接下来，我们尝试着用Stanford NLP工具。关于该工具，我们主要使用Stanford NER 标注工具。在使用这个工具之前，你需要在自己的电脑上安装Java（一般是JDK），并将Java添加到系统路径中，同时下载英语NER的文件包：stanford-ner-2018-10-16.zip（大小为172MB），下载地址为：<a href="https://nlp.stanford.edu/software/CRF-NER.shtml">https://nlp.stanford.edu/software/CRF-NER.shtml</a> 。以笔者的电脑为例，Java所在的路径为：C:\Program Files\Java\jdk1.8.0_161\bin\java.exe， 下载Stanford NER的zip文件解压后的文件夹的路径为：E://stanford-ner-2018-10-16，如下图所示：</p><p><img src="/img/nlp4_2.png" alt=""></p><p>在classifer文件夹中有如下文件：</p><p><img src="/img/nlp4_3.png" alt=""></p><p>它们代表的含义如下：</p><blockquote><p>3 class:Location, Person, Organization<br>4 class:Location, Person, Organization, Misc<br>7 class:Location, Person, Organization, Money, Percent, Date, Time</p></blockquote><p>可以使用Python实现Stanford NER，完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">from</span> nltk.tag <span class="hljs-keyword">import</span> StanfordNERTagger<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> nltk<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_document</span>(<span class="hljs-params">document</span>):<br>   document = re.sub(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, document)<br>   <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(document, <span class="hljs-built_in">str</span>):<br>       document = document<br>   <span class="hljs-keyword">else</span>:<br>       <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Document is not string!&#x27;</span>)<br>   document = document.strip()<br>   sentences = nltk.sent_tokenize(document)<br>   sentences = [sentence.strip() <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences]<br>   <span class="hljs-keyword">return</span> sentences<br><br><span class="hljs-comment"># sample document</span><br>text = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">FIFA was founded in 1904 to oversee international competition among the national associations of Belgium, </span><br><span class="hljs-string">Denmark, France, Germany, the Netherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich, its </span><br><span class="hljs-string">membership now comprises 211 national associations. Member countries must each also be members of one of </span><br><span class="hljs-string">the six regional confederations into which the world is divided: Africa, Asia, Europe, North &amp; Central America </span><br><span class="hljs-string">and the Caribbean, Oceania, and South America.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>sentences = parse_document(text)<br>tokenized_sentences = [nltk.word_tokenize(sentence) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences]<br><br><span class="hljs-comment"># set java path in environment variables</span><br>java_path = <span class="hljs-string">r&#x27;C:\Program Files\Java\jdk1.8.0_161\bin\java.exe&#x27;</span><br>os.environ[<span class="hljs-string">&#x27;JAVAHOME&#x27;</span>] = java_path<br><span class="hljs-comment"># load stanford NER</span><br>sn = StanfordNERTagger(<span class="hljs-string">&#x27;E://stanford-ner-2018-10-16/classifiers/english.muc.7class.distsim.crf.ser.gz&#x27;</span>,<br>                       path_to_jar=<span class="hljs-string">&#x27;E://stanford-ner-2018-10-16/stanford-ner.jar&#x27;</span>)<br><br><span class="hljs-comment"># tag sentences</span><br>ne_annotated_sentences = [sn.tag(sent) <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> tokenized_sentences]<br><span class="hljs-comment"># extract named entities</span><br>named_entities = []<br><span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> ne_annotated_sentences:<br>   temp_entity_name = <span class="hljs-string">&#x27;&#x27;</span><br>   temp_named_entity = <span class="hljs-literal">None</span><br>   <span class="hljs-keyword">for</span> term, tag <span class="hljs-keyword">in</span> sentence:<br>       <span class="hljs-comment"># get terms with NE tags</span><br>       <span class="hljs-keyword">if</span> tag != <span class="hljs-string">&#x27;O&#x27;</span>:<br>           temp_entity_name = <span class="hljs-string">&#x27; &#x27;</span>.join([temp_entity_name, term]).strip() <span class="hljs-comment">#get NE name</span><br>           temp_named_entity = (temp_entity_name, tag) <span class="hljs-comment"># get NE and its category</span><br>       <span class="hljs-keyword">else</span>:<br>           <span class="hljs-keyword">if</span> temp_named_entity:<br>               named_entities.append(temp_named_entity)<br>               temp_entity_name = <span class="hljs-string">&#x27;&#x27;</span><br>               temp_named_entity = <span class="hljs-literal">None</span><br><br><span class="hljs-comment"># get unique named entities</span><br>named_entities = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(named_entities))<br><span class="hljs-comment"># store named entities in a data frame</span><br>entity_frame = pd.DataFrame(named_entities, columns=[<span class="hljs-string">&#x27;Entity Name&#x27;</span>, <span class="hljs-string">&#x27;Entity Type&#x27;</span>])<br><span class="hljs-comment"># display results</span><br><span class="hljs-built_in">print</span>(entity_frame)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">                Entity Name   Entity <span class="hljs-keyword">Type</span><br><span class="hljs-number">0</span>                      <span class="hljs-number">1904</span>          <span class="hljs-keyword">DATE</span><br><span class="hljs-number">1</span>                   Denmark      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">2</span>                     Spain      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">3</span>   North &amp; Central America  ORGANIZATION<br><span class="hljs-number">4</span>             South America      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">5</span>                   Belgium      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">6</span>                    Zürich      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">7</span>           the Netherlands      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">8</span>                    France      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">9</span>                 Caribbean      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">10</span>                   Sweden      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">11</span>                  Oceania      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">12</span>                     Asia      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">13</span>                     FIFA  ORGANIZATION<br><span class="hljs-number">14</span>                   Europe      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">15</span>                   Africa      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">16</span>              Switzerland      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">17</span>                  Germany      LOCATION<br></code></pre></td></tr></table></figure><p>可以看到，在Stanford NER的帮助下，NER的实现效果较好，将Africa识别为LOCATION，将1904识别为时间（这在NLTK中没有识别出来），但还是对North &amp; Central America识别有误，将其识别为ORGANIZATION。</p><p>值得注意的是，并不是说Stanford NER一定会比NLTK NER的效果好，两者针对的对象，预料，算法可能有差异，因此，需要根据自己的需求决定使用什么工具。</p><p>本次分享到此结束，以后有机会的话，将会尝试着用HMM、CRF或深度学习来实现命名实体识别。</p><p>注意：本人现已开通微信公众号： NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>NER</tag>
      
      <tag>NLP工具</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（三）词形还原（Lemmatization）</title>
    <link href="/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F%EF%BC%88Lemmatization%EF%BC%89/"/>
    <url>/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F%EF%BC%88Lemmatization%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>词形还原（Lemmatization）是文本预处理中的重要部分，与词干提取（stemming）很相似。</p><p>简单说来，词形还原就是去掉单词的词缀，提取单词的主干部分，通常提取后的单词会是字典中的单词，不同于词干提取（stemming），提取后的单词不一定会出现在单词中。比如，单词“cars”词形还原后的单词为“car”，单词“ate”词形还原后的单词为“eat”。</p><p>在Python的nltk模块中，使用WordNet为我们提供了稳健的词形还原的函数。如以下示例Python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.stem <span class="hljs-keyword">import</span> WordNetLemmatizer<br><br>wnl = WordNetLemmatizer()<br><span class="hljs-comment"># lemmatize nouns</span><br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;cars&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>))<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;men&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>))<br><br><span class="hljs-comment"># lemmatize verbs</span><br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;running&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>))<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;ate&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>))<br><br><span class="hljs-comment"># lemmatize adjectives</span><br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;saddest&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>))<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;fancier&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>car<br>men<br>run<br>eat<br>sad<br>fancy</p></blockquote><p>在以上代码中，wnl.lemmatize()函数可以进行词形还原，第一个参数为单词，第二个参数为该单词的词性，如名词，动词，形容词等，返回的结果为输入单词的词形还原后的结果。</p><p>词形还原一般是简单的，但具体我们在使用时，指定单词的词性很重要，不然词形还原可能效果不好，如以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.stem <span class="hljs-keyword">import</span> WordNetLemmatizer<br><br>wnl = WordNetLemmatizer()<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;ate&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>))<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;fancier&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>ate<br>fancier</p></blockquote><p>那么，如何获取单词的词性呢？在NLP中，使用Parts of speech（POS）技术实现。在nltk中，可以使用nltk.pos_tag()获取单词在句子中的词性，如以下Python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">sentence = <span class="hljs-string">&#x27;The brown fox is quick and he is jumping over the lazy dog&#x27;</span><br><span class="hljs-keyword">import</span> nltk<br>tokens = nltk.word_tokenize(sentence)<br>tagged_sent = nltk.pos_tag(tokens)<br><span class="hljs-built_in">print</span>(tagged_sent)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>[(‘The’, ‘DT’), (‘brown’, ‘JJ’), (‘fox’, ‘NN’), (‘is’, ‘VBZ’), (‘quick’, ‘JJ’), (‘and’, ‘CC’), (‘he’, ‘PRP’), (‘is’, ‘VBZ’), (‘jumping’, ‘VBG’), (‘over’, ‘IN’), (‘the’, ‘DT’), (‘lazy’, ‘JJ’), (‘dog’, ‘NN’)]</p></blockquote><p>关于上述词性的说明，可以参考下表：</p><p><img src="/img/nlp3_1.webp" alt="词性说明表1"></p><p><img src="/img/nlp3_2.webp" alt="词性说明表2"></p><p>OK，知道了获取单词在句子中的词性，再结合词形还原，就能很好地完成词形还原功能。示例的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize, pos_tag<br><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> wordnet<br><span class="hljs-keyword">from</span> nltk.stem <span class="hljs-keyword">import</span> WordNetLemmatizer<br><br><span class="hljs-comment"># 获取单词的词性</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_wordnet_pos</span>(<span class="hljs-params">tag</span>):<br>    <span class="hljs-keyword">if</span> tag.startswith(<span class="hljs-string">&#x27;J&#x27;</span>):<br>        <span class="hljs-keyword">return</span> wordnet.ADJ<br>    <span class="hljs-keyword">elif</span> tag.startswith(<span class="hljs-string">&#x27;V&#x27;</span>):<br>        <span class="hljs-keyword">return</span> wordnet.VERB<br>    <span class="hljs-keyword">elif</span> tag.startswith(<span class="hljs-string">&#x27;N&#x27;</span>):<br>        <span class="hljs-keyword">return</span> wordnet.NOUN<br>    <span class="hljs-keyword">elif</span> tag.startswith(<span class="hljs-string">&#x27;R&#x27;</span>):<br>        <span class="hljs-keyword">return</span> wordnet.ADV<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>sentence = <span class="hljs-string">&#x27;football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal.&#x27;</span><br>tokens = word_tokenize(sentence)  <span class="hljs-comment"># 分词</span><br>tagged_sent = pos_tag(tokens)     <span class="hljs-comment"># 获取单词词性</span><br><br>wnl = WordNetLemmatizer()<br>lemmas_sent = []<br><span class="hljs-keyword">for</span> tag <span class="hljs-keyword">in</span> tagged_sent:<br>    wordnet_pos = get_wordnet_pos(tag[<span class="hljs-number">1</span>]) <span class="hljs-keyword">or</span> wordnet.NOUN<br>    lemmas_sent.append(wnl.lemmatize(tag[<span class="hljs-number">0</span>], pos=wordnet_pos)) <span class="hljs-comment"># 词形还原</span><br><br><span class="hljs-built_in">print</span>(lemmas_sent)<br><br></code></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>[‘football’, ‘be’, ‘a’, ‘family’, ‘of’, ‘team’, ‘sport’, ‘that’, ‘involve’, ‘,’, ‘to’, ‘vary’, ‘degree’, ‘,’, ‘kick’, ‘a’, ‘ball’, ‘to’, ‘score’, ‘a’, ‘goal’, ‘.’]</p></blockquote><p>输出的结果就是对句子中的单词进行词形还原后的结果。<br>本次分享到此结束，欢迎大家交流~</p><p>注意：本人现已开通微信公众号： NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>词形还原</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（二）探究TF-IDF的原理</title>
    <link href="/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%8C%EF%BC%89%E6%8E%A2%E7%A9%B6TF-IDF%E7%9A%84%E5%8E%9F%E7%90%86/"/>
    <url>/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%8C%EF%BC%89%E6%8E%A2%E7%A9%B6TF-IDF%E7%9A%84%E5%8E%9F%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h3 id="TF-IDF介绍">TF-IDF介绍</h3><p>TF-IDF是NLP中一种常用的统计方法，用以评估一个字词对于一个文件集或一个语料库中的其中一份文件的重要程度，通常用于提取文本的特征，即关键词。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。</p><p>在NLP中，TF-IDF的计算公式如下：</p><p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>t</mi><mi>f</mi><mi>i</mi><mi>d</mi><mi>f</mi><mo>=</mo><mi>t</mi><mi>f</mi><mo>∗</mo><mi>i</mi><mi>d</mi><mi>f</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">tfidf = tf*idf.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.10764em;">df</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.10764em;">df</span><span class="mord">.</span></span></span></span></span></p><p>其中，tf是词频(Term Frequency)，idf为逆向文件频率(Inverse Document Frequency)。</p><p>tf为词频，即一个词语在文档中的出现频率，假设一个词语在整个文档中出现了i次，而整个文档有N个词语，则tf的值为i/N.</p><p>idf为逆向文件频率，假设整个文档有n篇文章，而一个词语在k篇文章中出现，则idf值为</p><p class="katex-block "><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>i</mi><mi>d</mi><mi>f</mi><mo>=</mo><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mfrac><mi>n</mi><mi>k</mi></mfrac><mo stretchy="false">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">idf=\log_{2}(\frac{n}{k}).</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.10764em;">df</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.7936em;vertical-align:-0.686em;"></span><span class="mop"><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1076em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord">.</span></span></span></span></span></p><p>当然，不同地方的idf值计算公式会有稍微的不同。比如有些地方会在分母的k上加1，防止分母为0，还有些地方会让分子，分母都加上1，这是smoothing技巧。在本文中，还是采用最原始的idf值计算公式，因为这与gensim里面的计算公式一致。</p><p>假设整个文档有D篇文章，则单词i在第j篇文章中的tfidf值为</p><p><img src="/img/nlp2_1.webp" alt="gensim中tfidf的计算公式"></p><p>以上就是TF-IDF的计算方法。</p><h3 id="文本介绍及预处理">文本介绍及预处理</h3><p>我们将采用以下三个示例文本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">text1 =<span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal. </span><br><span class="hljs-string">Unqualified, the word football is understood to refer to whichever form of football is the most popular </span><br><span class="hljs-string">in the regional context in which the word appears. Sports commonly called football in certain places </span><br><span class="hljs-string">include association football (known as soccer in some countries); gridiron football (specifically American </span><br><span class="hljs-string">football or Canadian football); Australian rules football; rugby football (either rugby league or rugby union); </span><br><span class="hljs-string">and Gaelic football. These different variations of football are known as football codes.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>text2 = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Basketball is a team sport in which two teams of five players, opposing one another on a rectangular court, </span><br><span class="hljs-string">compete with the primary objective of shooting a basketball (approximately 9.4 inches (24 cm) in diameter) </span><br><span class="hljs-string">through the defender&#x27;s hoop (a basket 18 inches (46 cm) in diameter mounted 10 feet (3.048 m) high to a backboard </span><br><span class="hljs-string">at each end of the court) while preventing the opposing team from shooting through their own hoop. A field goal is </span><br><span class="hljs-string">worth two points, unless made from behind the three-point line, when it is worth three. After a foul, timed play stops </span><br><span class="hljs-string">and the player fouled or designated to shoot a technical foul is given one or more one-point free throws. The team with </span><br><span class="hljs-string">the most points at the end of the game wins, but if regulation play expires with the score tied, an additional period </span><br><span class="hljs-string">of play (overtime) is mandated.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>text3 = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Volleyball, game played by two teams, usually of six players on a side, in which the players use their hands to bat a </span><br><span class="hljs-string">ball back and forth over a high net, trying to make the ball touch the court within the opponents’ playing area before </span><br><span class="hljs-string">it can be returned. To prevent this a player on the opposing team bats the ball up and toward a teammate before it touches </span><br><span class="hljs-string">the court surface—that teammate may then volley it back across the net or bat it to a third teammate who volleys it across </span><br><span class="hljs-string">the net. A team is allowed only three touches of the ball before it must be returned over the net.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure><p>这三篇文章分别是关于足球，篮球，排球的介绍，它们组成一篇文档。</p><p>接下来是文本的预处理部分。</p><p>首先是对文本去掉换行符，然后是分句，分词，再去掉其中的标点，完整的Python代码如下，输入的参数为文章text:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> nltk<br><span class="hljs-keyword">import</span> string<br><br><span class="hljs-comment"># 文本预处理</span><br><span class="hljs-comment"># 函数：text文件分句，分词，并去掉标点</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_tokens</span>(<span class="hljs-params">text</span>):<br>    text = text.replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>    sents = nltk.sent_tokenize(text)  <span class="hljs-comment"># 分句</span><br>    tokens = []<br>    <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> nltk.word_tokenize(sent):  <span class="hljs-comment"># 分词</span><br>            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> string.punctuation: <span class="hljs-comment"># 去掉标点</span><br>                tokens.append(word)<br>    <span class="hljs-keyword">return</span> tokens<br></code></pre></td></tr></table></figure><p>接着，去掉文章中的通用词（stopwords），然后统计每个单词的出现次数，完整的Python代码如下，输入的参数为文章text:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords     <span class="hljs-comment">#停用词</span><br><br><span class="hljs-comment"># 对原始的text文件去掉停用词</span><br><span class="hljs-comment"># 生成count字典，即每个单词的出现次数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_count</span>(<span class="hljs-params">text</span>):<br>    tokens = get_tokens(text)<br>    filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> w <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)]    <span class="hljs-comment">#去掉停用词</span><br>    count = Counter(filtered)<br>    <span class="hljs-keyword">return</span> count<br></code></pre></td></tr></table></figure><p>以text3为例，生成的count字典如下：</p><blockquote><p>Counter({‘ball’: 4, ‘net’: 4, ‘teammate’: 3, ‘returned’: 2, ‘bat’: 2, ‘court’: 2, ‘team’: 2, ‘across’: 2, ‘touches’: 2, ‘back’: 2, ‘players’: 2, ‘touch’: 1, ‘must’: 1, ‘usually’: 1, ‘side’: 1, ‘player’: 1, ‘area’: 1, ‘Volleyball’: 1, ‘hands’: 1, ‘may’: 1, ‘toward’: 1, ‘A’: 1, ‘third’: 1, ‘two’: 1, ‘six’: 1, ‘opposing’: 1, ‘within’: 1, ‘prevent’: 1, ‘allowed’: 1, ‘’’: 1, ‘playing’: 1, ‘played’: 1, ‘volley’: 1, ‘surface—that’: 1, ‘volleys’: 1, ‘opponents’: 1, ‘use’: 1, ‘high’: 1, ‘teams’: 1, ‘bats’: 1, ‘To’: 1, ‘game’: 1, ‘make’: 1, ‘forth’: 1, ‘three’: 1, ‘trying’: 1})</p></blockquote><h3 id="Gensim中的TF-IDF">Gensim中的TF-IDF</h3><p>对文本进行预处理后，对于以上三个示例文本，我们都会得到一个count字典，里面是每个文本中单词的出现次数。下面，我们将用gensim中的已实现的TF-IDF模型，来输出每篇文章中TF-IDF排名前三的单词及它们的tfidf值，完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords     <span class="hljs-comment">#停用词</span><br><span class="hljs-keyword">from</span> gensim <span class="hljs-keyword">import</span> corpora, models, matutils<br><br><span class="hljs-comment">#training by gensim&#x27;s Ifidf Model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_words</span>(<span class="hljs-params">text</span>):<br>    tokens = get_tokens(text)<br>    filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> w <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)]<br>    <span class="hljs-keyword">return</span> filtered<br><br><span class="hljs-comment"># get text</span><br>count1, count2, count3 = get_words(text1), get_words(text2), get_words(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-comment"># training by TfidfModel in gensim</span><br>dictionary = corpora.Dictionary(countlist)<br>new_dict = &#123;v:k <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> dictionary.token2id.items()&#125;<br>corpus2 = [dictionary.doc2bow(count) <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> countlist]<br>tfidf2 = models.TfidfModel(corpus2)<br>corpus_tfidf = tfidf2[corpus2]<br><br><span class="hljs-comment"># output</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nTraining by gensim Tfidf Model.......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, doc <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(corpus_tfidf):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    sorted_words = <span class="hljs-built_in">sorted</span>(doc, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    <span class="hljs-keyword">for</span> num, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(new_dict[num], <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Training</span> by gensim Tfidf Model.......<br><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">1</span><br>    <span class="hljs-attribute">Word</span>: football, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">84766</span><br>    <span class="hljs-attribute">Word</span>: rugby, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">21192</span><br>    <span class="hljs-attribute">Word</span>: known, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">14128</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">2</span><br>    <span class="hljs-attribute">Word</span>: play, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">29872</span><br>    <span class="hljs-attribute">Word</span>: cm, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">19915</span><br>    <span class="hljs-attribute">Word</span>: diameter, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">19915</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">3</span><br>    <span class="hljs-attribute">Word</span>: net, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">45775</span><br>    <span class="hljs-attribute">Word</span>: teammate, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">34331</span><br>    <span class="hljs-attribute">Word</span>: across, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">22888</span><br></code></pre></td></tr></table></figure><p>输出的结果还是比较符合我们的预期的，比如关于足球的文章中提取了football, rugby关键词，关于篮球的文章中提取了plat, cm关键词，关于排球的文章中提取了net, teammate关键词。</p><h3 id="自己动手实践TF-IDF模型">自己动手实践TF-IDF模型</h3><p>有了以上我们对TF-IDF模型的理解，其实我们自己也可以动手实践一把，这是学习算法的最佳方式！</p><p>以下是笔者实践TF-IDF的代码（接文本预处理代码）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><br><span class="hljs-comment"># 计算tf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tf</span>(<span class="hljs-params">word, count</span>):<br>    <span class="hljs-keyword">return</span> count[word] / <span class="hljs-built_in">sum</span>(count.values())<br><span class="hljs-comment"># 计算count_list有多少个文件包含word</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">n_containing</span>(<span class="hljs-params">word, count_list</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> count_list <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> count)<br><br><span class="hljs-comment"># 计算idf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">idf</span>(<span class="hljs-params">word, count_list</span>):<br>    <span class="hljs-keyword">return</span> math.log2(<span class="hljs-built_in">len</span>(count_list) / (n_containing(word, count_list)))    <span class="hljs-comment">#对数以2为底</span><br><span class="hljs-comment"># 计算tf-idf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tfidf</span>(<span class="hljs-params">word, count, count_list</span>):<br>    <span class="hljs-keyword">return</span> tf(word, count) * idf(word, count_list)<br><br><span class="hljs-comment"># TF-IDF测试</span><br>count1, count2, count3 = make_count(text1), make_count(text2), make_count(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training by original algorithm......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, count <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(countlist):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    scores = &#123;word: tfidf(word, count, countlist) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> count&#125;<br>    sorted_words = <span class="hljs-built_in">sorted</span>(scores.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    <span class="hljs-comment"># sorted_words = matutils.unitvec(sorted_words)</span><br>    <span class="hljs-keyword">for</span> word, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(word, <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Training</span> by original algorithm......<br><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">1</span><br>    <span class="hljs-attribute">Word</span>: football, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">30677</span><br>    <span class="hljs-attribute">Word</span>: rugby, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">07669</span><br>    <span class="hljs-attribute">Word</span>: known, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">05113</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">2</span><br>    <span class="hljs-attribute">Word</span>: play, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">05283</span><br>    <span class="hljs-attribute">Word</span>: inches, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">03522</span><br>    <span class="hljs-attribute">Word</span>: worth, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">03522</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">3</span><br>    <span class="hljs-attribute">Word</span>: net, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">10226</span><br>    <span class="hljs-attribute">Word</span>: teammate, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">07669</span><br>    <span class="hljs-attribute">Word</span>: across, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">05113</span><br></code></pre></td></tr></table></figure><p>可以看到，笔者自己动手实践的TF-IDF模型提取的关键词与gensim一致，至于篮球中为什么后两个单词不一致，是因为这些单词的tfidf一样，随机选择的结果不同而已。但是有一个问题，那就是计算得到的tfidf值不一样，这是什么原因呢？</p><p>查阅gensim中计算tf-idf值的源代码（<a href="https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/tfidfmodel.py%EF%BC%89%EF%BC%9A">https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/tfidfmodel.py）：</a></p><p><img src="/img/nlp2_2.webp" alt="TfidfModel类的参数"></p><p><img src="/img/nlp2_3.webp" alt="normalize参数的说明"></p><p>也就是说，gensim对得到的tf-idf向量做了规范化（normalize），将其转化为单位向量。因此，我们需要在刚才的代码中加入规范化这一步，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 对向量做规范化, normalize</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">unitvec</span>(<span class="hljs-params">sorted_words</span>):<br>    lst = [item[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sorted_words]<br>    L2Norm = math.sqrt(<span class="hljs-built_in">sum</span>(np.array(lst)*np.array(lst)))<br>    unit_vector = [(item[<span class="hljs-number">0</span>], item[<span class="hljs-number">1</span>]/L2Norm) <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sorted_words]<br>    <span class="hljs-keyword">return</span> unit_vector<br><br><span class="hljs-comment"># TF-IDF测试</span><br>count1, count2, count3 = make_count(text1), make_count(text2), make_count(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training by original algorithm......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, count <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(countlist):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    scores = &#123;word: tfidf(word, count, countlist) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> count&#125;<br>    sorted_words = <span class="hljs-built_in">sorted</span>(scores.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    sorted_words = unitvec(sorted_words)   <span class="hljs-comment"># normalize</span><br>    <span class="hljs-keyword">for</span> word, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(word, <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Training</span> by original algorithm......<br><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">1</span><br>    <span class="hljs-attribute">Word</span>: football, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">84766</span><br>    <span class="hljs-attribute">Word</span>: rugby, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">21192</span><br>    <span class="hljs-attribute">Word</span>: known, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">14128</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">2</span><br>    <span class="hljs-attribute">Word</span>: play, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">29872</span><br>    <span class="hljs-attribute">Word</span>: shooting, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">19915</span><br>    <span class="hljs-attribute">Word</span>: diameter, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">19915</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">3</span><br>    <span class="hljs-attribute">Word</span>: net, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">45775</span><br>    <span class="hljs-attribute">Word</span>: teammate, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">34331</span><br>    <span class="hljs-attribute">Word</span>: back, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">22888</span><br></code></pre></td></tr></table></figure><p>现在的输出结果与gensim得到的结果一致！</p><h3 id="总结">总结</h3><p>Gensim是Python做NLP时鼎鼎大名的模块，有空还是多读读源码吧！以后，我们还会继续介绍TF-IDF在其它方面的应用，欢迎大家交流~</p><p>注意：本人现已开通微信公众号： NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p><p>本文的完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> nltk<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> string<br><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords     <span class="hljs-comment">#停用词</span><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter       <span class="hljs-comment">#计数</span><br><span class="hljs-keyword">from</span> gensim <span class="hljs-keyword">import</span> corpora, models, matutils<br><br>text1 =<span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal. </span><br><span class="hljs-string">Unqualified, the word football is understood to refer to whichever form of football is the most popular </span><br><span class="hljs-string">in the regional context in which the word appears. Sports commonly called football in certain places </span><br><span class="hljs-string">include association football (known as soccer in some countries); gridiron football (specifically American </span><br><span class="hljs-string">football or Canadian football); Australian rules football; rugby football (either rugby league or rugby union); </span><br><span class="hljs-string">and Gaelic football. These different variations of football are known as football codes.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>text2 = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Basketball is a team sport in which two teams of five players, opposing one another on a rectangular court, </span><br><span class="hljs-string">compete with the primary objective of shooting a basketball (approximately 9.4 inches (24 cm) in diameter) </span><br><span class="hljs-string">through the defender&#x27;s hoop (a basket 18 inches (46 cm) in diameter mounted 10 feet (3.048 m) high to a backboard </span><br><span class="hljs-string">at each end of the court) while preventing the opposing team from shooting through their own hoop. A field goal is </span><br><span class="hljs-string">worth two points, unless made from behind the three-point line, when it is worth three. After a foul, timed play stops </span><br><span class="hljs-string">and the player fouled or designated to shoot a technical foul is given one or more one-point free throws. The team with </span><br><span class="hljs-string">the most points at the end of the game wins, but if regulation play expires with the score tied, an additional period </span><br><span class="hljs-string">of play (overtime) is mandated.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>text3 = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Volleyball, game played by two teams, usually of six players on a side, in which the players use their hands to bat a </span><br><span class="hljs-string">ball back and forth over a high net, trying to make the ball touch the court within the opponents’ playing area before </span><br><span class="hljs-string">it can be returned. To prevent this a player on the opposing team bats the ball up and toward a teammate before it touches </span><br><span class="hljs-string">the court surface—that teammate may then volley it back across the net or bat it to a third teammate who volleys it across </span><br><span class="hljs-string">the net. A team is allowed only three touches of the ball before it must be returned over the net.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># 文本预处理</span><br><span class="hljs-comment"># 函数：text文件分句，分词，并去掉标点</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_tokens</span>(<span class="hljs-params">text</span>):<br>    text = text.replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>    sents = nltk.sent_tokenize(text)  <span class="hljs-comment"># 分句</span><br>    tokens = []<br>    <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> nltk.word_tokenize(sent):  <span class="hljs-comment"># 分词</span><br>            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> string.punctuation: <span class="hljs-comment"># 去掉标点</span><br>                tokens.append(word)<br>    <span class="hljs-keyword">return</span> tokens<br><br><span class="hljs-comment"># 对原始的text文件去掉停用词</span><br><span class="hljs-comment"># 生成count字典，即每个单词的出现次数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_count</span>(<span class="hljs-params">text</span>):<br>    tokens = get_tokens(text)<br>    filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> w <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)]    <span class="hljs-comment">#去掉停用词</span><br>    count = Counter(filtered)<br>    <span class="hljs-keyword">return</span> count<br><br><span class="hljs-comment"># 计算tf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tf</span>(<span class="hljs-params">word, count</span>):<br>    <span class="hljs-keyword">return</span> count[word] / <span class="hljs-built_in">sum</span>(count.values())<br><span class="hljs-comment"># 计算count_list有多少个文件包含word</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">n_containing</span>(<span class="hljs-params">word, count_list</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> count_list <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> count)<br><br><span class="hljs-comment"># 计算idf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">idf</span>(<span class="hljs-params">word, count_list</span>):<br>    <span class="hljs-keyword">return</span> math.log2(<span class="hljs-built_in">len</span>(count_list) / (n_containing(word, count_list)))    <span class="hljs-comment">#对数以2为底</span><br><span class="hljs-comment"># 计算tf-idf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tfidf</span>(<span class="hljs-params">word, count, count_list</span>):<br>    <span class="hljs-keyword">return</span> tf(word, count) * idf(word, count_list)<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 对向量做规范化, normalize</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">unitvec</span>(<span class="hljs-params">sorted_words</span>):<br>    lst = [item[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sorted_words]<br>    L2Norm = math.sqrt(<span class="hljs-built_in">sum</span>(np.array(lst)*np.array(lst)))<br>    unit_vector = [(item[<span class="hljs-number">0</span>], item[<span class="hljs-number">1</span>]/L2Norm) <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sorted_words]<br>    <span class="hljs-keyword">return</span> unit_vector<br><br><span class="hljs-comment"># TF-IDF测试</span><br>count1, count2, count3 = make_count(text1), make_count(text2), make_count(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training by original algorithm......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, count <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(countlist):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    scores = &#123;word: tfidf(word, count, countlist) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> count&#125;<br>    sorted_words = <span class="hljs-built_in">sorted</span>(scores.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    sorted_words = unitvec(sorted_words)   <span class="hljs-comment"># normalize</span><br>    <span class="hljs-keyword">for</span> word, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(word, <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br><br><span class="hljs-comment">#training by gensim&#x27;s Ifidf Model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_words</span>(<span class="hljs-params">text</span>):<br>    tokens = get_tokens(text)<br>    filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> w <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)]<br>    <span class="hljs-keyword">return</span> filtered<br><br><span class="hljs-comment"># get text</span><br>count1, count2, count3 = get_words(text1), get_words(text2), get_words(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-comment"># training by TfidfModel in gensim</span><br>dictionary = corpora.Dictionary(countlist)<br>new_dict = &#123;v:k <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> dictionary.token2id.items()&#125;<br>corpus2 = [dictionary.doc2bow(count) <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> countlist]<br>tfidf2 = models.TfidfModel(corpus2)<br>corpus_tfidf = tfidf2[corpus2]<br><br><span class="hljs-comment"># output</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nTraining by gensim Tfidf Model.......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, doc <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(corpus_tfidf):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    sorted_words = <span class="hljs-built_in">sorted</span>(doc, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    <span class="hljs-keyword">for</span> num, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(new_dict[num], <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br>        <br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">输出结果：</span><br><span class="hljs-string"></span><br><span class="hljs-string">Training by original algorithm......</span><br><span class="hljs-string"></span><br><span class="hljs-string">Top words in document 1</span><br><span class="hljs-string">    Word: football, TF-IDF: 0.84766</span><br><span class="hljs-string">    Word: rugby, TF-IDF: 0.21192</span><br><span class="hljs-string">    Word: word, TF-IDF: 0.14128</span><br><span class="hljs-string">Top words in document 2</span><br><span class="hljs-string">    Word: play, TF-IDF: 0.29872</span><br><span class="hljs-string">    Word: inches, TF-IDF: 0.19915</span><br><span class="hljs-string">    Word: points, TF-IDF: 0.19915</span><br><span class="hljs-string">Top words in document 3</span><br><span class="hljs-string">    Word: net, TF-IDF: 0.45775</span><br><span class="hljs-string">    Word: teammate, TF-IDF: 0.34331</span><br><span class="hljs-string">    Word: bat, TF-IDF: 0.22888</span><br><span class="hljs-string"></span><br><span class="hljs-string">Training by gensim Tfidf Model.......</span><br><span class="hljs-string"></span><br><span class="hljs-string">Top words in document 1</span><br><span class="hljs-string">    Word: football, TF-IDF: 0.84766</span><br><span class="hljs-string">    Word: rugby, TF-IDF: 0.21192</span><br><span class="hljs-string">    Word: known, TF-IDF: 0.14128</span><br><span class="hljs-string">Top words in document 2</span><br><span class="hljs-string">    Word: play, TF-IDF: 0.29872</span><br><span class="hljs-string">    Word: cm, TF-IDF: 0.19915</span><br><span class="hljs-string">    Word: diameter, TF-IDF: 0.19915</span><br><span class="hljs-string">Top words in document 3</span><br><span class="hljs-string">    Word: net, TF-IDF: 0.45775</span><br><span class="hljs-string">    Word: teammate, TF-IDF: 0.34331</span><br><span class="hljs-string">    Word: across, TF-IDF: 0.22888</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>TF-IDF</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（一）词袋模型及句子相似度</title>
    <link href="/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    <url>/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6/</url>
    
    <content type="html"><![CDATA[<p>本文作为笔者NLP入门系列文章第一篇，以后我们就要步入NLP时代。</p><p>本文将会介绍NLP中常见的词袋模型（Bag of Words）以及如何利用词袋模型来计算句子间的相似度（余弦相似度，cosine similarity）。</p><p>首先，让我们来看一下，什么是词袋模型。我们以下面两个简单句子为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">sent1 = <span class="hljs-string">&quot;I love sky, I love sea.&quot;</span><br>sent2 = <span class="hljs-string">&quot;I like running, I love reading.&quot;</span><br></code></pre></td></tr></table></figure><p>通常，NLP无法一下子处理完整的段落或句子，因此，第一步往往是分句和分词。这里只有句子，因此我们只需要分词即可。对于英语句子，可以使用NLTK中的word_tokenize函数，对于中文句子，则可使用jieba模块。故第一步为分词，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize<br>sents = [sent1, sent2]<br>texts = [[word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_tokenize(sent)] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents]<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scheme">[[<span class="hljs-symbol">&#x27;I</span>&#x27;, <span class="hljs-symbol">&#x27;love</span>&#x27;, <span class="hljs-symbol">&#x27;sky</span>&#x27;, &#x27;,&#x27;, <span class="hljs-symbol">&#x27;I</span>&#x27;, <span class="hljs-symbol">&#x27;love</span>&#x27;, <span class="hljs-symbol">&#x27;sea</span>&#x27;, <span class="hljs-symbol">&#x27;.</span>&#x27;], [<span class="hljs-symbol">&#x27;I</span>&#x27;, <span class="hljs-symbol">&#x27;like</span>&#x27;, <span class="hljs-symbol">&#x27;running</span>&#x27;, &#x27;,&#x27;, <span class="hljs-symbol">&#x27;I</span>&#x27;, <span class="hljs-symbol">&#x27;love</span>&#x27;, <span class="hljs-symbol">&#x27;reading</span>&#x27;, <span class="hljs-symbol">&#x27;.</span>&#x27;]]<br></code></pre></td></tr></table></figure><p>分词完毕。下一步是构建语料库，即所有句子中出现的单词及标点。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">all_list = []<br><span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>    all_list += text<br>corpus = <span class="hljs-built_in">set</span>(all_list)<br><span class="hljs-built_in">print</span>(corpus)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">&#123;&#x27;love&#x27;, &#x27;running&#x27;, &#x27;reading&#x27;, &#x27;sky&#x27;, &#x27;.&#x27;, &#x27;I&#x27;, &#x27;like&#x27;, &#x27;sea&#x27;, &#x27;,&#x27;&#125;<br></code></pre></td></tr></table></figure><p>可以看到，语料库中一共是8个单词及标点。接下来，对语料库中的单词及标点建立数字映射，便于后续的句子的向量表示。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">corpus_dict = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(corpus, <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(corpus))))<br><span class="hljs-built_in">print</span>(corpus_dict)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">&#123;&#x27;running&#x27;: <span class="hljs-number">1</span>, &#x27;reading&#x27;: <span class="hljs-number">2</span>, &#x27;love&#x27;: <span class="hljs-number">0</span>, &#x27;sky&#x27;: <span class="hljs-number">3</span>, &#x27;.&#x27;: <span class="hljs-number">4</span>, &#x27;I&#x27;: <span class="hljs-number">5</span>, &#x27;like&#x27;: <span class="hljs-number">6</span>, &#x27;sea&#x27;: <span class="hljs-number">7</span>, &#x27;,&#x27;: <span class="hljs-number">8</span>&#125;<br></code></pre></td></tr></table></figure><p>虽然单词及标点并没有按照它们出现的顺序来建立数字映射，不过这并不会影响句子的向量表示及后续的句子间的相似度。</p><p>下一步，也就是词袋模型的关键一步，就是建立句子的向量表示。这个表示向量并不是简单地以单词或标点出现与否来选择0，1数字，而是把单词或标点的出现频数作为其对应的数字表示，结合刚才的语料库字典，句子的向量表示的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 建立句子的向量表示</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">vector_rep</span>(<span class="hljs-params">text, corpus_dict</span>):<br>    vec = []<br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> corpus_dict.keys():<br>        <span class="hljs-keyword">if</span> key <span class="hljs-keyword">in</span> text:<br>            vec.append((corpus_dict[key], text.count(key)))<br>        <span class="hljs-keyword">else</span>:<br>            vec.append((corpus_dict[key], <span class="hljs-number">0</span>))<br><br>    vec = <span class="hljs-built_in">sorted</span>(vec, key= <span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">0</span>])<br><br>    <span class="hljs-keyword">return</span> vec<br><br>vec1 = vector_rep(texts[<span class="hljs-number">0</span>], corpus_dict)<br>vec2 = vector_rep(texts[<span class="hljs-number">1</span>], corpus_dict)<br><span class="hljs-built_in">print</span>(vec1)<br><span class="hljs-built_in">print</span>(vec2)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scheme">[(<span class="hljs-name">0</span>, <span class="hljs-number">2</span>), (<span class="hljs-name">1</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">2</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">3</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">4</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">5</span>, <span class="hljs-number">2</span>), (<span class="hljs-name">6</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">7</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">8</span>, <span class="hljs-number">1</span>)]<br>[(<span class="hljs-name">0</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">1</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">2</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">3</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">4</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">5</span>, <span class="hljs-number">2</span>), (<span class="hljs-name">6</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">7</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">8</span>, <span class="hljs-number">1</span>)]<br></code></pre></td></tr></table></figure><p>让我们稍微逗留一会儿，来看看这个向量。在第一句中I出现了两次，在预料库字典中，I对应的数字为5，因此在第一句中5出现2次，在列表中的元组即为(5,2)，代表单词I在第一句中出现了2次。以上的输出可能并不那么直观，真实的两个句子的代表向量应为：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-string">[2, 0, 0, 1, 1, 2, 0, 1, 1]</span><br><span class="hljs-string">[1, 1, 1, 0, 1, 2, 1, 0, 1]</span><br></code></pre></td></tr></table></figure><p>OK，词袋模型到此结束。接下来，我们会利用刚才得到的词袋模型，即两个句子的向量表示，来计算相似度。</p><p>在NLP中，如果得到了两个句子的向量表示，那么，一般会选择用余弦相似度作为它们的相似度，而向量的余弦相似度即为两个向量的夹角的余弦值。其计算的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> sqrt<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">similarity_with_2_sents</span>(<span class="hljs-params">vec1, vec2</span>):<br>    inner_product = <span class="hljs-number">0</span><br>    square_length_vec1 = <span class="hljs-number">0</span><br>    square_length_vec2 = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> tup1, tup2 <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(vec1, vec2):<br>        inner_product += tup1[<span class="hljs-number">1</span>]*tup2[<span class="hljs-number">1</span>]<br>        square_length_vec1 += tup1[<span class="hljs-number">1</span>]**<span class="hljs-number">2</span><br>        square_length_vec2 += tup2[<span class="hljs-number">1</span>]**<span class="hljs-number">2</span><br><br>    <span class="hljs-keyword">return</span> (inner_product/sqrt(square_length_vec1*square_length_vec2))<br><br><br>cosine_sim = similarity_with_2_sents(vec1, vec2)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;两个句子的余弦相似度为： %.4f。&#x27;</span>%cosine_sim)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">两个句子的余弦相似度为： 0.7303。<br></code></pre></td></tr></table></figure><p>这样，我们就通过句子的词袋模型，得到了它们间的句子相似度。</p><p>当然，在实际的NLP项目中，如果需要计算两个句子的相似度，我们只需调用gensim模块即可，它是NLP的利器，能够帮助我们处理很多NLP任务。下面为用gensim计算两个句子的相似度的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">sent1 = <span class="hljs-string">&quot;I love sky, I love sea.&quot;</span><br>sent2 = <span class="hljs-string">&quot;I like running, I love reading.&quot;</span><br><br><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize<br>sents = [sent1, sent2]<br>texts = [[word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_tokenize(sent)] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents]<br><span class="hljs-built_in">print</span>(texts)<br><br><span class="hljs-keyword">from</span> gensim <span class="hljs-keyword">import</span> corpora<br><span class="hljs-keyword">from</span> gensim.similarities <span class="hljs-keyword">import</span> Similarity<br><br><span class="hljs-comment">#  语料库</span><br>dictionary = corpora.Dictionary(texts)<br><br><span class="hljs-comment"># 利用doc2bow作为词袋模型</span><br>corpus = [dictionary.doc2bow(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts]<br>similarity = Similarity(<span class="hljs-string">&#x27;-Similarity-index&#x27;</span>, corpus, num_features=<span class="hljs-built_in">len</span>(dictionary))<br><span class="hljs-built_in">print</span>(similarity)<br><span class="hljs-comment"># 获取句子的相似度</span><br>new_sensence = sent1<br>test_corpus_1 = dictionary.doc2bow(word_tokenize(new_sensence))<br><br>cosine_sim = similarity[test_corpus_1][<span class="hljs-number">1</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;利用gensim计算得到两个句子的相似度： %.4f。&quot;</span>%cosine_sim)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs delphi">[[<span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;sky&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;sea&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>], [<span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;like&#x27;</span>, <span class="hljs-string">&#x27;running&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;reading&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]]<br>Similarity <span class="hljs-keyword">index</span> <span class="hljs-keyword">with</span> <span class="hljs-number">2</span> documents <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> shards (<span class="hljs-keyword">stored</span> under -Similarity-<span class="hljs-keyword">index</span>)<br>利用gensim计算得到两个句子的相似度： <span class="hljs-number">0.7303</span>。<br></code></pre></td></tr></table></figure><p>注意，如果在运行代码时出现以下warning:</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">gensim\utils.py:<span class="hljs-number">1209</span>: UserWarning: detected Windows; aliasing chunkize <span class="hljs-keyword">to</span> chunkize_serial<br>  warnings.warn(&quot;detected Windows; aliasing chunkize to chunkize_serial&quot;)<br><br>gensim\matutils.py:<span class="hljs-number">737</span>: FutureWarning: <span class="hljs-keyword">Conversion</span> <span class="hljs-keyword">of</span> the second argument <span class="hljs-keyword">of</span> issubdtype <span class="hljs-keyword">from</span> `<span class="hljs-type">int</span>` <span class="hljs-keyword">to</span> `np.signedinteger` <span class="hljs-keyword">is</span> deprecated. <span class="hljs-keyword">In</span> future, it will be treated <span class="hljs-keyword">as</span> `np.int32 == np.dtype(<span class="hljs-type">int</span>).<span class="hljs-keyword">type</span>`.<br>  <span class="hljs-keyword">if</span> np.issubdtype(vec.dtype, np.int):<br></code></pre></td></tr></table></figure><p>如果想要去掉这些warning，则在导入gensim模块的代码前添加以下代码即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> warnings<br>warnings.filterwarnings(action=<span class="hljs-string">&#x27;ignore&#x27;</span>,category=UserWarning,module=<span class="hljs-string">&#x27;gensim&#x27;</span>)<br>warnings.filterwarnings(action=<span class="hljs-string">&#x27;ignore&#x27;</span>,category=FutureWarning,module=<span class="hljs-string">&#x27;gensim&#x27;</span>)<br></code></pre></td></tr></table></figure><p>本文到此结束，感谢阅读！如果不当之处，请速联系笔者，欢迎大家交流！祝您好运~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>词袋模型</tag>
      
      <tag>句子相似度</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>技术文章写作计划</title>
    <link href="/2023/07/06/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0%E5%86%99%E4%BD%9C%E8%AE%A1%E5%88%92/"/>
    <url>/2023/07/06/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0%E5%86%99%E4%BD%9C%E8%AE%A1%E5%88%92/</url>
    
    <content type="html"><![CDATA[<ul class="task-list"><li><label><input type="checkbox"checked="" />滑动验证码的识别</label></li><li><label><input type="checkbox"checked="" />滑动验证码的获取</label></li><li><label><input type="checkbox" />点选验证码的识别</label></li><li><label><input type="checkbox" />ELK简单搭建的demo</label></li><li><label><input type="checkbox" />文本聚类</label></li><li><label><input type="checkbox" />智能问答</label></li><li><label><input type="checkbox" />车牌的识别</label></li><li><label><input type="checkbox"checked="" />个人足迹地图（WEB服务）</label></li><li><label><input type="checkbox" checked="" />别名发现系统</label></li><li><label><input type="checkbox" />读取doc和docx文档</label></li><li><label><input type="checkbox"checked="" />利用celery实现定时任务</label></li><li><label><input type="checkbox"checked="" />文本标注工具Doccano</label></li><li><label><input type="checkbox"checked="" />利用Conda创建Python虚拟环境</label></li><li><label><input type="checkbox"checked="" />利用SFTP连接Linux服务器并上传、下载文件</label></li><li><label><input type="checkbox" checked="" />Flask学习之RESTfulAPI</label></li><li><label><input type="checkbox" />Flask学习之JWT认证</label></li><li><label><input type="checkbox" checked="" />BSON文件读取</label></li><li><label><inputtype="checkbox" />Flask学习之Flask-SQLALCHEMY</label></li><li><label><input type="checkbox"checked="" />设计模式（完成三篇：单例模式、工厂模式、监听模式）</label></li><li><label><input type="checkbox" />Redis</label></li><li><label><input type="checkbox" />supervisor使用</label></li><li><label><input type="checkbox"checked="" />tornado之文件下载（包含中文文件下载）</label></li><li><label><input type="checkbox"checked="" />利用CRF实现中文分词</label></li><li><label><input type="checkbox"checked="" />利用CRF实现模型预测</label></li><li><label><input type="checkbox"checked="" />protobuf的初次使用</label></li><li><label><inputtype="checkbox" />更新tensorflow/serving中的models.config文件中的model_version_policy</label></li><li><label><input type="checkbox"checked="" />tensorflow同时使用多个session</label></li><li><label><input type="checkbox"checked="" />如何离线安装tensorflow模块</label></li><li><label><input type="checkbox"checked="" />tensorboard查看ckpt和pb文件模型</label></li><li><label><input type="checkbox"checked="" />将ckpt转化为pb文件</label></li><li><label><input type="checkbox"checked="" />tensorflow/serving之BERT模型部署和预测</label></li><li><label><input type="checkbox"checked="" />tensorflow/serving实现模型部署</label></li><li><label><input type="checkbox" /><span class="citation"data-cites="property">@property</span></label></li><li><label><input type="checkbox" />tf_record</label></li><li><label><input type="checkbox" />指代关系抽取</label></li><li><label><inputtype="checkbox" />实体链接（百度实体链接比赛、武器装备知识图谱）</label></li><li><label><input type="checkbox"checked="" />文本多分类BERT微调</label></li><li><label><input type="checkbox"checked="" />文本多标签分类BERT微调</label></li><li><label><input type="checkbox"checked="" />文本序列标注BERT微调</label></li><li><label><input type="checkbox" checked="" />keras-bertEnglish系列（3个模型稍微调整即可）</label></li><li><label><input type="checkbox"checked="" />keras-bert调用ALBERT</label></li><li><label><input type="checkbox"checked="" />keras-bert模型部署</label></li><li><label><input type="checkbox"checked="" />h5文件转化为pb文件进行部署</label></li><li><label><input type="checkbox"checked="" />tensorflow/serving高效调用</label></li><li><label><inputtype="checkbox" />tensorflow_hub实现英文文本二分类</label></li><li><label><inputtype="checkbox" />tensorflow2.0和transformers实现文本多分类</label></li><li><label><input type="checkbox" />抽取式问答</label></li><li><label><input type="checkbox"checked="" />完形填空与文本纠错</label></li><li><label><inputtype="checkbox" />transformers实现中文序列标注</label></li><li><label><inputtype="checkbox" />tokenizers中的token使用方法</label></li><li><label><input type="checkbox" />BPE token 算法</label></li><li><label><input type="checkbox" checked="" />Keras:K折交叉验证</label></li><li><label><input type="checkbox"checked="" />使用Prothemus对tensorflow/serving进行服务监控</label></li><li><label><input type="checkbox"checked="" />seqeval获取序列标注实体识别结果</label></li><li><label><input type="checkbox" />ES进阶</label></li><li><label><input type="checkbox"checked="" />从荷兰国旗问题到快速排序</label></li><li><label><input type="checkbox" />中英文大模型调研</label></li><li><label><input type="checkbox" />LLaMA模型的介绍及其使用</label></li><li><label><input type="checkbox" />Fine-tune LLaMA模型</label></li><li><label><input type="checkbox"checked="" />OpenAI的tokenizer调研</label></li><li><label><input type="checkbox" checked="" />Gitlab CI/CD入门</label></li><li><label><input type="checkbox"checked="" />LangChain使用</label></li><li><label><input type="checkbox" checked="" />Flask部署</label></li><li><label><input type="checkbox" />LangChain构建阅读助手</label></li><li><label><inputtype="checkbox" />使用LoRA训练Flan-T5-XXL模型</label></li><li><label><inputtype="checkbox" />VSCode连接远程服务器进行开发</label></li></ul>]]></content>
    
    
    <categories>
      
      <category>写作计划</category>
      
    </categories>
    
    
    <tags>
      
      <tag>写作计划</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用Hexo+Github搭建个人博客网站</title>
    <link href="/2023/07/06/%E4%BD%BF%E7%94%A8Hexo-Github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/"/>
    <url>/2023/07/06/%E4%BD%BF%E7%94%A8Hexo-Github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/</url>
    
    <content type="html"><![CDATA[<p>曾几何时，笔者也幻想过写个项目来搭建属于自己的个人博客。但是，写程序以及维护的成本，不禁让我犹豫再三，最后还是选择了CSDN等博客网站。将近六年的博客生涯，我尝试了不同的博客网站，各有各的利和弊，不变的是广告，这让人很不爽。直到今天，我看到了别人写的利用Hexo+Github来搭建个人博客网站，如获至宝。折腾了一阵以后，轻松完成了个人博客的搭建，这种清爽的界面风格，让人耳目一新，同时它又是免费的，功能繁多的，便于维护的。下面，我将会介绍如何来使用Hexo+Github搭建个人博客网站。</p><h3 id="准备工作">准备工作</h3><p>为了顺利地完成个人博客网站的搭建，需要做以下准备工作：</p><ul><li>安装Git和NodeJs（版本为18.16.1）；</li><li>安装Hexo（命令为<code>npm i -g hexo</code>）;</li><li>Github账号</li></ul><h3 id="搭建博客">搭建博客</h3><p>下面将分步来介绍如何使用Hexo和Github来搭建个人博客网站。</p><h4 id="创建github仓库">创建Github仓库</h4><p>在Github中新建一个名为username.github.io的空仓库，其中username是你在GitHub上的用户名，比如笔者的仓库名为percent.github.io。</p><h4 id="配置ssh">配置SSH</h4><p>如果想要使用远程从你的电脑上传文件至你的github仓库，那么，你就需要配置SSH。点击你个人Github上的Settings选项，在<code>SSH and GPG keys</code>中配置SSH的公钥，一般公钥位于<code>.ssh/id_rsa.pub</code>中，如下图：<img src="/img/hexo1.png" alt="配置SSH" /></p><h4 id="博客初始化">博客初始化</h4><p>新建一个空的文件夹，比如笔者新建了文件夹<code>github_blog</code>，使用<code>hexo init</code>命令初始化博客。初始化后的文件夹结构如下：<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs 1c">.<br>├── _config.yml<br>├── package.json<br>├── scaffolds<br>├── source<br><span class="hljs-string">|   ├── _drafts</span><br><span class="hljs-string">|   └── _posts</span><br>└── themes<br></code></pre></td></tr></table></figure> 上述文件说明如下：</p><ul><li>_config.yml 网站的 配置 信息，您可以在此配置大部分的参数。</li><li>package.json：应用程序的信息。EJS, Stylus 和 Markdown renderer已默认安装，您可以自由移除。</li><li>scaffolds：模版文件夹。当您新建文章时，Hexo会根据 scaffold来建立文件。</li><li>source：资源文件夹是存放用户资源的地方。</li><li>themes：主题文件夹。Hexo 会根据主题来生成静态页面。</li></ul><h4 id="生成个人博客网站">生成个人博客网站</h4><p>配置_config.yml文件，配置信息如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># Deployment</span><br><span class="hljs-comment">## Docs: https://hexo.io/docs/deployment.html</span><br><span class="hljs-attr">deploy:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">git</span><br>  <span class="hljs-attr">repo:</span> <span class="hljs-string">https://github.com/percent4/percent4.github.io.git(第一步创建的Github仓库)</span><br>  <span class="hljs-attr">branch:</span> <span class="hljs-string">master</span><br></code></pre></td></tr></table></figure><p>安装插件<code>npm install hexo-deployer-git --save</code>后，运行如下命令：<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">hexo</span> clean<span class="hljs-comment"># 清除数据</span><br>hexo d -g<span class="hljs-comment"># 生成博客</span><br></code></pre></td></tr></table></figure>这时候，你会看到博客数据会提交至Github的信息，而第一步创建的空仓库也有了提交内容，当然，你的个人博客也搭建搭建完毕，访问网址为：https://username.github.io/，其中username是你在GitHub上的用户名。界面如下： <imgsrc="/img/hexo2.png" alt="Hexo界面" /></p><h3 id="博客维护">博客维护</h3><p>Hexo提供了一套维护博客的优雅的办法。笔者在此仅介绍如何新建一篇博客。新建博客格式为markdown格式，比如我想创建一篇名为<code>利用Tornado搭建文档预览系统</code>的博客，可以使用以下命令：</p><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs haxe">hexo <span class="hljs-keyword">new</span> <span class="hljs-type"></span>利用Tornado搭建文档预览系统<br></code></pre></td></tr></table></figure><p>这时候会在你当前目录下的source/_posts文件夹下生成<code>利用Tornado搭建文档预览系统.md</code>,其中内容如下：<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">利用Tornado搭建文档预览系统</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2020-06-09 18:32:29</span><br><span class="hljs-attr">tags:</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure>其中title为博客标题，date为博客时间，tags为博客标签。在<code>---</code>后面可以写博客正文的内容。写完博客后，使用命令 <figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">hexo</span> clean<span class="hljs-comment"># 清除数据</span><br>hexo d -g<span class="hljs-comment"># 生成博客</span><br></code></pre></td></tr></table></figure> 就会更新个人博客。</p><h3 id="更换主题">更换主题</h3><p>Hexo提供的默认主题为landscape,我们想替换主题为Fluid.Hexo替换主题为Fluid的步骤如下：</p><ol type="1"><li>通过<code>npm</code>直接安装，进入博客目录执行命令：<code>npm install --save hexo-theme-fluid</code></li><li>将node_modules文件夹下的hexo-theme-fluid复制到themes文件夹，并重名为fluid</li><li>在博客目录下创建_config.fluid.yml，将主题的_config.yml内容复制进去，并将<code>theme:</code>后面的主题修改为fluid</li><li>使用<code>hexo s</code>进行本地部署，如无问题，则使用命令<code>hexo d -g</code>进行远程部署</li></ol><h3 id="总结">总结</h3><p>当然，Hexo还提供了许多丰富的功能，比如theme（主题）的个性化定制等，这会使得你的博客内容更加丰富，功能更加完善。</p><p>笔者大家的个人博客网站为：<ahref="https://percent4.github.io/">https://percent4.github.io/</a>，欢迎大家访问。以后，笔者将会逐渐往个人博客网站倾斜，而减少使用公开的博客社区。</p>]]></content>
    
    
    <categories>
      
      <category>Hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
      <tag>Github</tag>
      
      <tag>个人博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何使用Hexo？</title>
    <link href="/2023/07/06/hello-world/"/>
    <url>/2023/07/06/hello-world/</url>
    
    <content type="html"><![CDATA[<p>欢迎来到 <a href="https://hexo.io/">Hexo</a>!这是我的第一篇博客。查阅 <ahref="https://hexo.io/docs/">documentation</a> 获取更多信息。如果你在使用Hexo遇到问题，你可以在这里找到答案 <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a>，或者你可以在这上面提问：<ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="快速开始">快速开始</h2><h3 id="创建一篇新的博客">创建一篇新的博客</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>更新信息: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="运行服务">运行服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>更新信息: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="产生静态文件">产生静态文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>更新信息: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="远程部署">远程部署</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>更新信息: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
