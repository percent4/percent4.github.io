<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>NLP（二十六）限定领域的三元组抽取的一次尝试</title>
    <link href="/2023/07/09/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%E5%85%AD%EF%BC%89%E9%99%90%E5%AE%9A%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%89%E5%85%83%E7%BB%84%E6%8A%BD%E5%8F%96%E7%9A%84%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/"/>
    <url>/2023/07/09/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%E5%85%AD%EF%BC%89%E9%99%90%E5%AE%9A%E9%A2%86%E5%9F%9F%E7%9A%84%E4%B8%89%E5%85%83%E7%BB%84%E6%8A%BD%E5%8F%96%E7%9A%84%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文将会介绍笔者在2019语言与智能技术竞赛的三元组抽取比赛方面的一次尝试。由于该比赛早已结束，笔者当时也没有参加这个比赛，因此没有测评成绩，我们也只能拿到训练集和验证集。但是，这并不耽误我们在这方面做实验。</p><h3 id="比赛介绍">比赛介绍</h3><p>该比赛的网址为：<ahref="http://lic2019.ccf.org.cn/kg">http://lic2019.ccf.org.cn/kg</a>，该比赛主要是从给定的句子中提取三元组，给定schema约束集合及句子sent，其中schema定义了关系P以及其对应的主体S和客体O的类别，例如（S_TYPE:人物，P:妻子，O_TYPE:人物）、（S_TYPE:公司，P:创始人，O_TYPE:人物）等。比如下面的例子：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;text&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;九玄珠是在纵横中文网连载的一部小说，作者是龙马&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;spo_list&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>    <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;九玄珠&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;连载网站&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;纵横中文网&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;九玄珠&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;作者&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;龙马&quot;</span><span class="hljs-punctuation">]</span><br>  <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>该比赛一共提供了20多万标注质量很高的三元组，其中17万训练集，2万验证集和2万测试集，实体关系（schema）50个。</p><p>在具体介绍笔者的思路和实战前，先介绍下本次任务的处理思路：</p><figure><img src="/img/nlp26_1.png" alt="任务的处理思路" /><figcaption aria-hidden="true">任务的处理思路</figcaption></figure><p>首先是对拿到的数据进行数据分析，包括统计每个句子的长度及三元组数量，每种关系的数量分布情况。接着，对数据单独走序列标注模型和关系分析模型。最后在提取三元组的时候，用Pipeline模型，先用序列标注模型预测句子中的实体，再对实体（加上句子）走关系分类模型，预测实体的关系，最后形成有效的三元组。</p><p>接下来笔者将逐一介绍，项目结构图如下：</p><figure><img src="/img/nlp26_2.png" alt="项目结构图" /><figcaption aria-hidden="true">项目结构图</figcaption></figure><h3 id="数据分析">数据分析</h3><p>我们能拿到的只有训练集和验证集，没有测试集。我们对训练集做数据分析，训练集数据文件为train_data.json。</p><p>数据分析会统计训练集中每个句子的长度及三元组数量，还有关系的分布图，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># author: Jclian91</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><span class="hljs-comment"># time: 2020-03-12 21:52</span><br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">from</span> pprint <span class="hljs-keyword">import</span> pprint<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>plt.figure(figsize=(<span class="hljs-number">18</span>, <span class="hljs-number">8</span>), dpi=<span class="hljs-number">100</span>)   <span class="hljs-comment"># 输出图片大小为1800*800</span><br><span class="hljs-comment"># Mac系统设置中文字体支持</span><br>plt.rcParams[<span class="hljs-string">&quot;font.family&quot;</span>] = <span class="hljs-string">&#x27;Arial Unicode MS&#x27;</span><br><br><br><span class="hljs-comment"># 加载数据集</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>(<span class="hljs-params">filename</span>):<br>    D = []<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        content = f.readlines()<br><br>    content = [_.replace(<span class="hljs-string">&#x27; &#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>).replace(<span class="hljs-string">&#x27;\u3000&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>).replace(<span class="hljs-string">&#x27;\xa0&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>).replace(<span class="hljs-string">&#x27;\u2003&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> content]<br><br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> content:<br>        l = json.loads(l)<br>        D.append(&#123;<br>            <span class="hljs-string">&#x27;text&#x27;</span>: l[<span class="hljs-string">&#x27;text&#x27;</span>],<br>            <span class="hljs-string">&#x27;spo_list&#x27;</span>: [<br>                (spo[<span class="hljs-string">&#x27;subject&#x27;</span>], spo[<span class="hljs-string">&#x27;predicate&#x27;</span>], spo[<span class="hljs-string">&#x27;object&#x27;</span>])<br>                <span class="hljs-keyword">for</span> spo <span class="hljs-keyword">in</span> l[<span class="hljs-string">&#x27;spo_list&#x27;</span>]<br>            ]<br>        &#125;)<br>    <span class="hljs-keyword">return</span> D<br><br>filename = <span class="hljs-string">&#x27;../data/train_data.json&#x27;</span><br><br>D = load_data(filename=filename)<br>pprint(D)<br><br><span class="hljs-comment"># 创建text, text_length, spo_num的DataFrame</span><br>text_list = [_[<span class="hljs-string">&quot;text&quot;</span>] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> D]<br>spo_num = [<span class="hljs-built_in">len</span>(_[<span class="hljs-string">&quot;spo_list&quot;</span>])<span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> D]<br><br>df = pd.DataFrame(&#123;<span class="hljs-string">&quot;text&quot;</span>: text_list, <span class="hljs-string">&quot;spo_num&quot;</span>: spo_num&#125; )<br>df[<span class="hljs-string">&quot;text_length&quot;</span>] = df[<span class="hljs-string">&quot;text&quot;</span>].apply(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x))<br><span class="hljs-built_in">print</span>(df.head())<br><span class="hljs-built_in">print</span>(df.describe())<br><br><span class="hljs-comment"># 绘制spo_num的条形统计图</span><br>pprint(df[<span class="hljs-string">&#x27;spo_num&#x27;</span>].value_counts())<br>label_list = <span class="hljs-built_in">list</span>(df[<span class="hljs-string">&#x27;spo_num&#x27;</span>].value_counts().index)<br>num_list = df[<span class="hljs-string">&#x27;spo_num&#x27;</span>].value_counts().tolist()<br><br><span class="hljs-comment"># 利用Matplotlib模块绘制条形图</span><br>x = <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(num_list))<br>rects = plt.bar(x=x, height=num_list, width=<span class="hljs-number">0.6</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>, label=<span class="hljs-string">&quot;频数&quot;</span>)<br>plt.ylim(<span class="hljs-number">0</span>, <span class="hljs-number">80000</span>) <span class="hljs-comment"># y轴范围</span><br>plt.ylabel(<span class="hljs-string">&quot;数量&quot;</span>)<br>plt.xticks([index + <span class="hljs-number">0.1</span> <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> x], label_list)<br>plt.xlabel(<span class="hljs-string">&quot;三元组数量&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;三元组频数统计图&quot;</span>)<br><br><span class="hljs-comment"># 条形图的文字说明</span><br><span class="hljs-keyword">for</span> rect <span class="hljs-keyword">in</span> rects:<br>    height = rect.get_height()<br>    plt.text(rect.get_x() + rect.get_width() / <span class="hljs-number">2</span>, height+<span class="hljs-number">1</span>, <span class="hljs-built_in">str</span>(height), ha=<span class="hljs-string">&quot;center&quot;</span>, va=<span class="hljs-string">&quot;bottom&quot;</span>)<br><br><span class="hljs-comment"># plt.show()</span><br>plt.savefig(<span class="hljs-string">&#x27;./spo_num_bar_chart.png&#x27;</span>)<br><br>plt.close()<br><br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>plt.figure(figsize=(<span class="hljs-number">18</span>, <span class="hljs-number">8</span>), dpi=<span class="hljs-number">100</span>)   <span class="hljs-comment"># 输出图片大小为1800*800</span><br><span class="hljs-comment"># Mac系统设置中文字体支持</span><br>plt.rcParams[<span class="hljs-string">&quot;font.family&quot;</span>] = <span class="hljs-string">&#x27;Arial Unicode MS&#x27;</span><br><br><br><span class="hljs-comment"># 关系统计图</span><br>relation_dict = defaultdict(<span class="hljs-built_in">int</span>)<br><br><span class="hljs-keyword">for</span> spo_dict <span class="hljs-keyword">in</span> D:<br>    <span class="hljs-comment"># print(spo_dict[&quot;spo_list&quot;])</span><br>    <span class="hljs-keyword">for</span> spo <span class="hljs-keyword">in</span> spo_dict[<span class="hljs-string">&quot;spo_list&quot;</span>]:<br>        relation_dict[spo[<span class="hljs-number">1</span>]] += <span class="hljs-number">1</span><br><br>label_list = <span class="hljs-built_in">list</span>(relation_dict.keys())<br>num_list = <span class="hljs-built_in">list</span>(relation_dict.values())<br><br><span class="hljs-comment"># 利用Matplotlib模块绘制条形图</span><br>x = <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(num_list))<br>rects = plt.bar(x=x, height=num_list, width=<span class="hljs-number">0.6</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>, label=<span class="hljs-string">&quot;频数&quot;</span>)<br>plt.ylim(<span class="hljs-number">0</span>, <span class="hljs-number">80000</span>) <span class="hljs-comment"># y轴范围</span><br>plt.ylabel(<span class="hljs-string">&quot;数量&quot;</span>)<br>plt.xticks([index + <span class="hljs-number">0.1</span> <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> x], label_list)<br>plt.xticks(rotation=<span class="hljs-number">45</span>) <span class="hljs-comment"># x轴的标签旋转45度</span><br>plt.xlabel(<span class="hljs-string">&quot;三元组关系&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;三元组关系频数统计图&quot;</span>)<br><br><span class="hljs-comment"># 条形图的文字说明</span><br><span class="hljs-keyword">for</span> rect <span class="hljs-keyword">in</span> rects:<br>    height = rect.get_height()<br>    plt.text(rect.get_x() + rect.get_width() / <span class="hljs-number">2</span>, height+<span class="hljs-number">1</span>, <span class="hljs-built_in">str</span>(height), ha=<span class="hljs-string">&quot;center&quot;</span>, va=<span class="hljs-string">&quot;bottom&quot;</span>)<br><br><br>plt.savefig(<span class="hljs-string">&#x27;./relation_bar_chart.png&#x27;</span>)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs apache">             <span class="hljs-attribute">spo_num</span>    text_length<br><span class="hljs-attribute">count</span>  <span class="hljs-number">173108</span>.<span class="hljs-number">000000</span>  <span class="hljs-number">173108</span>.<span class="hljs-number">000000</span><br><span class="hljs-attribute">mean</span>        <span class="hljs-number">2</span>.<span class="hljs-number">103993</span>      <span class="hljs-number">54</span>.<span class="hljs-number">057190</span><br><span class="hljs-attribute">std</span>         <span class="hljs-number">1</span>.<span class="hljs-number">569331</span>      <span class="hljs-number">31</span>.<span class="hljs-number">498245</span><br><span class="hljs-attribute">min</span>         <span class="hljs-number">0</span>.<span class="hljs-number">000000</span>       <span class="hljs-number">5</span>.<span class="hljs-number">000000</span><br><span class="hljs-attribute">25</span>%         <span class="hljs-number">1</span>.<span class="hljs-number">000000</span>      <span class="hljs-number">32</span>.<span class="hljs-number">000000</span><br><span class="hljs-attribute">50</span>%         <span class="hljs-number">2</span>.<span class="hljs-number">000000</span>      <span class="hljs-number">45</span>.<span class="hljs-number">000000</span><br><span class="hljs-attribute">75</span>%         <span class="hljs-number">2</span>.<span class="hljs-number">000000</span>      <span class="hljs-number">68</span>.<span class="hljs-number">000000</span><br><span class="hljs-attribute">max</span>        <span class="hljs-number">25</span>.<span class="hljs-number">000000</span>     <span class="hljs-number">300</span>.<span class="hljs-number">000000</span><br></code></pre></td></tr></table></figure><p>句子的平均长度为54，最大长度为300；每句话中的三元组数量的平均值为2.1，最大值为25。</p><p>每句话中的三元组数量的分布图如下：</p><figure><img src="/img/nlp26_3.png" alt="每句话中的三元组数量分布图" /><figcaption aria-hidden="true">每句话中的三元组数量分布图</figcaption></figure><p>关系数量的分布图如下：</p><figure><img src="/img/nlp26_4.png" alt="关系数量分布图" /><figcaption aria-hidden="true">关系数量分布图</figcaption></figure><h3 id="序列标注模型">序列标注模型</h3><p>我们将句子中的主体和客体作为实体，分别标注为SUBJ和OBJ，标注体系采用BIO。一个简单的标注例子如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs bash">如 O<br>何 O<br>演 O<br>好 O<br>自 O<br>己 O<br>的 O<br>角 O<br>色 O<br>， O<br>请 O<br>读 O<br>《 O<br>演 O<br>员 O<br>自 O<br>我 O<br>修 O<br>养 O<br>》 O<br>《 O<br>喜 B-SUBJ<br>剧 I-SUBJ<br>之 I-SUBJ<br>王 I-SUBJ<br>》 O<br>周 B-OBJ<br>星 I-OBJ<br>驰 I-OBJ<br>崛 O<br>起 O<br>于 O<br>穷 O<br>困 O<br>潦 O<br>倒 O<br>之 O<br>中 O<br>的 O<br>独 O<br>门 O<br>秘 O<br>笈 O<br></code></pre></td></tr></table></figure><p>序列标注的模型采用ALBERT+Bi-LSTM+CRF，结构图如下：</p><figure><img src="/img/nlp26_5.png" alt="序列标注模型结构图" /><figcaption aria-hidden="true">序列标注模型结构图</figcaption></figure><p>模型方面的代码不再具体给出，有兴趣的同学可以参考文章<ahref="https://blog.csdn.net/jclian91/article/details/104826655">NLP（二十五）实现ALBERT+Bi-LSTM+CRF模型</a>，也可以参考文章最后给出的Github项目网址。</p><p>模型设置文本最大长度为128，利用ALBERT做特征提取，在自己的电脑上用CPU训练5个epoch，结果如下：</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs gradle">_________________________________________________________________<br>Train on <span class="hljs-number">173109</span> samples, validate on <span class="hljs-number">21639</span> samples<br>Epoch <span class="hljs-number">1</span>/<span class="hljs-number">10</span><br><span class="hljs-number">173109</span><span class="hljs-regexp">/173109 [==============================] - 422s 2ms/</span><span class="hljs-keyword">step</span> - loss: <span class="hljs-number">0.4460</span> - crf_viterbi_accuracy: <span class="hljs-number">0.8710</span> - val_loss: <span class="hljs-number">0.1613</span> - val_crf_viterbi_accuracy: <span class="hljs-number">0.9235</span><br>Epoch <span class="hljs-number">2</span>/<span class="hljs-number">10</span><br><span class="hljs-number">173109</span><span class="hljs-regexp">/173109 [==============================] - 417s 2ms/</span><span class="hljs-keyword">step</span> - loss: <span class="hljs-number">0.1170</span> - crf_viterbi_accuracy: <span class="hljs-number">0.9496</span> - val_loss: <span class="hljs-number">0.0885</span> - val_crf_viterbi_accuracy: <span class="hljs-number">0.9592</span><br>Epoch <span class="hljs-number">3</span>/<span class="hljs-number">10</span><br><span class="hljs-number">173109</span><span class="hljs-regexp">/173109 [==============================] - 417s 2ms/</span><span class="hljs-keyword">step</span> - loss: <span class="hljs-number">0.0758</span> - crf_viterbi_accuracy: <span class="hljs-number">0.9602</span> - val_loss: <span class="hljs-number">0.0653</span> - val_crf_viterbi_accuracy: <span class="hljs-number">0.9638</span><br>Epoch <span class="hljs-number">4</span>/<span class="hljs-number">10</span><br><span class="hljs-number">173109</span><span class="hljs-regexp">/173109 [==============================] - 415s 2ms/</span><span class="hljs-keyword">step</span> - loss: <span class="hljs-number">0.0586</span> - crf_viterbi_accuracy: <span class="hljs-number">0.9645</span> - val_loss: <span class="hljs-number">0.0544</span> - val_crf_viterbi_accuracy: <span class="hljs-number">0.9651</span><br>Epoch <span class="hljs-number">5</span>/<span class="hljs-number">10</span><br><span class="hljs-number">173109</span><span class="hljs-regexp">/173109 [==============================] - 422s 2ms/</span><span class="hljs-keyword">step</span> - loss: <span class="hljs-number">0.0488</span> - crf_viterbi_accuracy: <span class="hljs-number">0.9663</span> - val_loss: <span class="hljs-number">0.0464</span> - val_crf_viterbi_accuracy: <span class="hljs-number">0.9654</span><br>Epoch <span class="hljs-number">6</span>/<span class="hljs-number">10</span><br><span class="hljs-number">173109</span><span class="hljs-regexp">/173109 [==============================] - 423s 2ms/</span><span class="hljs-keyword">step</span> - loss: <span class="hljs-number">0.0399</span> - crf_viterbi_accuracy: <span class="hljs-number">0.9677</span> - val_loss: <span class="hljs-number">0.0375</span> - val_crf_viterbi_accuracy: <span class="hljs-number">0.9660</span><br>Epoch <span class="hljs-number">7</span>/<span class="hljs-number">10</span><br><span class="hljs-number">173109</span><span class="hljs-regexp">/173109 [==============================] - 415s 2ms/</span><span class="hljs-keyword">step</span> - loss: <span class="hljs-number">0.0293</span> - crf_viterbi_accuracy: <span class="hljs-number">0.9687</span> - val_loss: <span class="hljs-number">0.0265</span> - val_crf_viterbi_accuracy: <span class="hljs-number">0.9664</span><br>Epoch <span class="hljs-number">8</span>/<span class="hljs-number">10</span><br><span class="hljs-number">173109</span><span class="hljs-regexp">/173109 [==============================] - 414s 2ms/</span><span class="hljs-keyword">step</span> - loss: <span class="hljs-number">0.0174</span> - crf_viterbi_accuracy: <span class="hljs-number">0.9695</span> - val_loss: <span class="hljs-number">0.0149</span> - val_crf_viterbi_accuracy: <span class="hljs-number">0.9671</span><br>Epoch <span class="hljs-number">9</span>/<span class="hljs-number">10</span><br><span class="hljs-number">173109</span><span class="hljs-regexp">/173109 [==============================] - 422s 2ms/</span><span class="hljs-keyword">step</span> - loss: <span class="hljs-number">0.0049</span> - crf_viterbi_accuracy: <span class="hljs-number">0.9703</span> - val_loss: <span class="hljs-number">0.0036</span> - val_crf_viterbi_accuracy: <span class="hljs-number">0.9670</span><br>Epoch <span class="hljs-number">10</span>/<span class="hljs-number">10</span><br><span class="hljs-number">173109</span><span class="hljs-regexp">/173109 [==============================] - 429s 2ms/</span><span class="hljs-keyword">step</span> - loss: -<span class="hljs-number">0.0072</span> - crf_viterbi_accuracy: <span class="hljs-number">0.9709</span> - val_loss: -<span class="hljs-number">0.0078</span> - val_crf_viterbi_accuracy: <span class="hljs-number">0.9674</span><br>           precision    recall  f1-score   support<br><br>      OBJ     <span class="hljs-number">0.9593</span>    <span class="hljs-number">0.9026</span>    <span class="hljs-number">0.9301</span>     <span class="hljs-number">44598</span><br>     SUBJ     <span class="hljs-number">0.9670</span>    <span class="hljs-number">0.9238</span>    <span class="hljs-number">0.9449</span>     <span class="hljs-number">25521</span><br><br>micro avg     <span class="hljs-number">0.9621</span>    <span class="hljs-number">0.9104</span>    <span class="hljs-number">0.9355</span>     <span class="hljs-number">70119</span><br>macro avg     <span class="hljs-number">0.9621</span>    <span class="hljs-number">0.9104</span>    <span class="hljs-number">0.9355</span>     <span class="hljs-number">70119</span><br></code></pre></td></tr></table></figure><p>利用seqeval模块做评估，在验证集上的F1值约为93.55%。</p><h3 id="关系分类模型">关系分类模型</h3><p>需要对关系做一下说明，因为笔者会对句子（sent）中的主体（S）和客体（O）组合起来，加上句子，形成训练数据。举个例子，在句子<code>历史评价李氏朝鲜的创立并非太祖大王李成桂一人之功﹐其五子李芳远功不可没</code>，三元组为<code>[&#123;"predicate": "父亲", "object_type": "人物", "subject_type": "人物", "object": "李成桂", "subject": "李芳远"&#125;, &#123;"predicate": "国籍", "object_type": "国家", "subject_type": "人物", "object": "朝鲜", "subject": "李成桂"&#125;]&#125;</code>，在这句话中主体有李成桂，李芳远，客体有李成桂和朝鲜，关系有父亲（关系类型：2）和国籍（关系类型：22）。按照笔者的思路，这句话应组成4个关系分类样本，如下：</p><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs basic"><span class="hljs-symbol">2 </span>李芳远$李成桂$历史评价李氏朝鲜的创立并非太祖大王###一人之功﹐其五子###功不可没<br><span class="hljs-symbol">0 </span>李芳远$朝鲜$历史评价李氏##的创立并非太祖大王李成桂一人之功﹐其五子###功不可没<br><span class="hljs-symbol">0 </span>李成桂$李成桂$历史评价李氏朝鲜的创立并非太祖大王###一人之功﹐其五子李芳远功不可没<br><span class="hljs-symbol">22 </span>李成桂$朝鲜$历史评价李氏##的创立并非太祖大王###一人之功﹐其五子李芳远功不可没<br></code></pre></td></tr></table></figure><p>因此，就会出现关系0（表示“未知”），这样我们在提取三元组的时候就可以略过这条关系，形成真正有用的三元组。</p><p>因此，关系一共为51个（加上未知关系：0）。关系分类模型采用ALBERT+Bi-GRU+ATT，结构图如下：</p><figure><img src="/img/nlp26_6.png" alt="关系分类模型图" /><figcaption aria-hidden="true">关系分类模型图</figcaption></figure><p>模型方面的代码不再具体给出，有兴趣的同学可以参考文章<ahref="https://percent4.github.io/2023/07/08/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B8%80%EF%BC%89%E4%BA%BA%E7%89%A9%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E7%9A%84%E4%B8%80%E6%AC%A1%E5%AE%9E%E6%88%98/">NLP（二十一）人物关系抽取的一次实战</a>，也可以参考文章最后给出的Github项目网址。</p><p>模型设置文本最大长度为128，利用ALBERT做特征提取，在自己的电脑上用CPU训练30个epoch（实际上，由于有earlystopping机制，训练不到30个eopch），在验证集上的评估结果如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">Epoch</span> <span class="hljs-number">23</span><span class="hljs-string">/30</span><br><span class="hljs-number">396766</span><span class="hljs-string">/396766</span> [<span class="hljs-string">==============================</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">776s 2ms/step - loss: 0.1770 - accuracy: 0.9402 - val_loss: 0.2170 - val_accuracy:</span> <span class="hljs-number">0.9308</span><br><br><span class="hljs-attr">Epoch 00023:</span> <span class="hljs-string">val_accuracy</span> <span class="hljs-string">did</span> <span class="hljs-string">not</span> <span class="hljs-string">improve</span> <span class="hljs-string">from</span> <span class="hljs-number">0.93292</span><br><span class="hljs-number">49506</span><span class="hljs-string">/49506</span> [<span class="hljs-string">==============================</span>] <span class="hljs-bullet">-</span> <span class="hljs-string">151s</span> <span class="hljs-string">3ms/step</span><br><span class="hljs-string">在测试集上的效果：</span> [<span class="hljs-number">0.21701653493155634</span>, <span class="hljs-number">0.930776059627533</span>]<br>    <span class="hljs-string">precision</span>    <span class="hljs-string">recall</span>  <span class="hljs-string">f1-score</span>   <span class="hljs-string">support</span><br><br>          <span class="hljs-string">未知</span>       <span class="hljs-number">0.87</span>      <span class="hljs-number">0.76</span>      <span class="hljs-number">0.81</span>      <span class="hljs-number">5057</span><br>          <span class="hljs-string">祖籍</span>       <span class="hljs-number">0.92</span>      <span class="hljs-number">0.73</span>      <span class="hljs-number">0.82</span>       <span class="hljs-number">181</span><br>          <span class="hljs-string">父亲</span>       <span class="hljs-number">0.79</span>      <span class="hljs-number">0.88</span>      <span class="hljs-number">0.83</span>       <span class="hljs-number">609</span><br>        <span class="hljs-string">总部地点</span>       <span class="hljs-number">0.95</span>      <span class="hljs-number">0.95</span>      <span class="hljs-number">0.95</span>       <span class="hljs-number">310</span><br>         <span class="hljs-string">出生地</span>       <span class="hljs-number">0.94</span>      <span class="hljs-number">0.95</span>      <span class="hljs-number">0.94</span>      <span class="hljs-number">2330</span><br>           <span class="hljs-string">目</span>       <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>      <span class="hljs-number">1271</span><br>          <span class="hljs-string">面积</span>       <span class="hljs-number">0.90</span>      <span class="hljs-number">0.92</span>      <span class="hljs-number">0.91</span>        <span class="hljs-number">79</span><br>          <span class="hljs-string">简称</span>       <span class="hljs-number">0.97</span>      <span class="hljs-number">0.99</span>      <span class="hljs-number">0.98</span>       <span class="hljs-number">138</span><br>        <span class="hljs-string">上映时间</span>       <span class="hljs-number">0.94</span>      <span class="hljs-number">0.98</span>      <span class="hljs-number">0.96</span>       <span class="hljs-number">463</span><br>          <span class="hljs-string">妻子</span>       <span class="hljs-number">0.91</span>      <span class="hljs-number">0.83</span>      <span class="hljs-number">0.87</span>       <span class="hljs-number">680</span><br>        <span class="hljs-string">所属专辑</span>       <span class="hljs-number">0.97</span>      <span class="hljs-number">0.97</span>      <span class="hljs-number">0.97</span>      <span class="hljs-number">1282</span><br>        <span class="hljs-string">注册资本</span>       <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>        <span class="hljs-number">63</span><br>          <span class="hljs-string">首都</span>       <span class="hljs-number">0.92</span>      <span class="hljs-number">0.96</span>      <span class="hljs-number">0.94</span>        <span class="hljs-number">47</span><br>          <span class="hljs-string">导演</span>       <span class="hljs-number">0.92</span>      <span class="hljs-number">0.94</span>      <span class="hljs-number">0.93</span>      <span class="hljs-number">2603</span><br>           <span class="hljs-string">字</span>       <span class="hljs-number">0.96</span>      <span class="hljs-number">0.97</span>      <span class="hljs-number">0.97</span>       <span class="hljs-number">339</span><br>          <span class="hljs-string">身高</span>       <span class="hljs-number">0.98</span>      <span class="hljs-number">0.98</span>      <span class="hljs-number">0.98</span>       <span class="hljs-number">393</span><br>        <span class="hljs-string">出品公司</span>       <span class="hljs-number">0.96</span>      <span class="hljs-number">0.96</span>      <span class="hljs-number">0.96</span>       <span class="hljs-number">851</span><br>        <span class="hljs-string">修业年限</span>       <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>         <span class="hljs-number">2</span><br>        <span class="hljs-string">出生日期</span>       <span class="hljs-number">0.99</span>      <span class="hljs-number">0.99</span>      <span class="hljs-number">0.99</span>      <span class="hljs-number">2892</span><br>         <span class="hljs-string">制片人</span>       <span class="hljs-number">0.69</span>      <span class="hljs-number">0.88</span>      <span class="hljs-number">0.77</span>       <span class="hljs-number">127</span><br>          <span class="hljs-string">母亲</span>       <span class="hljs-number">0.75</span>      <span class="hljs-number">0.88</span>      <span class="hljs-number">0.81</span>       <span class="hljs-number">425</span><br>          <span class="hljs-string">编剧</span>       <span class="hljs-number">0.82</span>      <span class="hljs-number">0.80</span>      <span class="hljs-number">0.81</span>       <span class="hljs-number">771</span><br>          <span class="hljs-string">国籍</span>       <span class="hljs-number">0.92</span>      <span class="hljs-number">0.92</span>      <span class="hljs-number">0.92</span>      <span class="hljs-number">1621</span><br>          <span class="hljs-string">海拔</span>       <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>        <span class="hljs-number">43</span><br>        <span class="hljs-string">连载网站</span>       <span class="hljs-number">0.98</span>      <span class="hljs-number">1.00</span>      <span class="hljs-number">0.99</span>      <span class="hljs-number">1658</span><br>          <span class="hljs-string">丈夫</span>       <span class="hljs-number">0.84</span>      <span class="hljs-number">0.91</span>      <span class="hljs-number">0.87</span>       <span class="hljs-number">678</span><br>          <span class="hljs-string">朝代</span>       <span class="hljs-number">0.85</span>      <span class="hljs-number">0.92</span>      <span class="hljs-number">0.88</span>       <span class="hljs-number">419</span><br>          <span class="hljs-string">民族</span>       <span class="hljs-number">0.98</span>      <span class="hljs-number">0.99</span>      <span class="hljs-number">0.99</span>      <span class="hljs-number">1434</span><br>           <span class="hljs-string">号</span>       <span class="hljs-number">0.95</span>      <span class="hljs-number">0.99</span>      <span class="hljs-number">0.97</span>       <span class="hljs-number">197</span><br>         <span class="hljs-string">出版社</span>       <span class="hljs-number">0.98</span>      <span class="hljs-number">0.99</span>      <span class="hljs-number">0.99</span>      <span class="hljs-number">2272</span><br>         <span class="hljs-string">主持人</span>       <span class="hljs-number">0.82</span>      <span class="hljs-number">0.86</span>      <span class="hljs-number">0.84</span>       <span class="hljs-number">200</span><br>        <span class="hljs-string">专业代码</span>       <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>         <span class="hljs-number">3</span><br>          <span class="hljs-string">歌手</span>       <span class="hljs-number">0.89</span>      <span class="hljs-number">0.94</span>      <span class="hljs-number">0.91</span>      <span class="hljs-number">2857</span><br>          <span class="hljs-string">作词</span>       <span class="hljs-number">0.85</span>      <span class="hljs-number">0.81</span>      <span class="hljs-number">0.83</span>       <span class="hljs-number">884</span><br>          <span class="hljs-string">主角</span>       <span class="hljs-number">0.86</span>      <span class="hljs-number">0.77</span>      <span class="hljs-number">0.81</span>        <span class="hljs-number">39</span><br>         <span class="hljs-string">董事长</span>       <span class="hljs-number">0.81</span>      <span class="hljs-number">0.74</span>      <span class="hljs-number">0.78</span>        <span class="hljs-number">47</span><br>        <span class="hljs-string">毕业院校</span>       <span class="hljs-number">0.99</span>      <span class="hljs-number">0.99</span>      <span class="hljs-number">0.99</span>      <span class="hljs-number">1433</span><br>        <span class="hljs-string">占地面积</span>       <span class="hljs-number">0.89</span>      <span class="hljs-number">0.89</span>      <span class="hljs-number">0.89</span>        <span class="hljs-number">61</span><br>        <span class="hljs-string">官方语言</span>       <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>        <span class="hljs-number">15</span><br>        <span class="hljs-string">邮政编码</span>       <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>         <span class="hljs-number">4</span><br>        <span class="hljs-string">人口数量</span>       <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>        <span class="hljs-number">45</span><br>        <span class="hljs-string">所在城市</span>       <span class="hljs-number">0.90</span>      <span class="hljs-number">0.94</span>      <span class="hljs-number">0.92</span>        <span class="hljs-number">77</span><br>          <span class="hljs-string">作者</span>       <span class="hljs-number">0.97</span>      <span class="hljs-number">0.97</span>      <span class="hljs-number">0.97</span>      <span class="hljs-number">4359</span><br>        <span class="hljs-string">成立日期</span>       <span class="hljs-number">0.99</span>      <span class="hljs-number">0.99</span>      <span class="hljs-number">0.99</span>      <span class="hljs-number">1608</span><br>          <span class="hljs-string">作曲</span>       <span class="hljs-number">0.78</span>      <span class="hljs-number">0.77</span>      <span class="hljs-number">0.78</span>       <span class="hljs-number">849</span><br>          <span class="hljs-string">气候</span>       <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>      <span class="hljs-number">1.00</span>       <span class="hljs-number">103</span><br>          <span class="hljs-string">嘉宾</span>       <span class="hljs-number">0.76</span>      <span class="hljs-number">0.72</span>      <span class="hljs-number">0.74</span>       <span class="hljs-number">158</span><br>          <span class="hljs-string">主演</span>       <span class="hljs-number">0.94</span>      <span class="hljs-number">0.97</span>      <span class="hljs-number">0.95</span>      <span class="hljs-number">7383</span><br>         <span class="hljs-string">改编自</span>       <span class="hljs-number">0.95</span>      <span class="hljs-number">0.82</span>      <span class="hljs-number">0.88</span>        <span class="hljs-number">71</span><br>         <span class="hljs-string">创始人</span>       <span class="hljs-number">0.86</span>      <span class="hljs-number">0.87</span>      <span class="hljs-number">0.86</span>        <span class="hljs-number">75</span><br><br>    <span class="hljs-string">accuracy</span>                           <span class="hljs-number">0.93</span>     <span class="hljs-number">49506</span><br>   <span class="hljs-string">macro</span> <span class="hljs-string">avg</span>       <span class="hljs-number">0.92</span>      <span class="hljs-number">0.92</span>      <span class="hljs-number">0.92</span>     <span class="hljs-number">49506</span><br><span class="hljs-string">weighted</span> <span class="hljs-string">avg</span>       <span class="hljs-number">0.93</span>      <span class="hljs-number">0.93</span>      <span class="hljs-number">0.93</span>     <span class="hljs-number">49506</span><br></code></pre></td></tr></table></figure><h3 id="三元组提取">三元组提取</h3><p>最后一部分，也是本次比赛的最终目标，就是三元组提取。</p><p>三元组提取采用Pipeline模式，先用序列标注模型预测句子中的实体，然后再用关系分类模型判断实体关系的类别，过滤掉关系为未知的情形，就是我们想要提取的三元组了。</p><p>三元组提取的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># author: Jclian91</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><span class="hljs-comment"># time: 2020-03-14 20:41</span><br><span class="hljs-keyword">import</span> os, re, json, traceback<br><br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> keras_contrib.layers <span class="hljs-keyword">import</span> CRF<br><span class="hljs-keyword">from</span> keras_contrib.losses <span class="hljs-keyword">import</span> crf_loss<br><span class="hljs-keyword">from</span> keras_contrib.metrics <span class="hljs-keyword">import</span> crf_accuracy, crf_viterbi_accuracy<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br><span class="hljs-keyword">from</span> pprint <span class="hljs-keyword">import</span> pprint<br><span class="hljs-keyword">from</span> text_classification.att <span class="hljs-keyword">import</span> Attention<br><br><span class="hljs-keyword">from</span> albert_zh.extract_feature <span class="hljs-keyword">import</span> BertVector<br><br><span class="hljs-comment"># 读取label2id字典</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;../sequence_labeling/ccks2019_label2id.json&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> h:<br>    label_id_dict = json.loads(h.read())<br><br>id_label_dict = &#123;v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> label_id_dict.items()&#125;<br><span class="hljs-comment"># 利用ALBERT提取文本特征</span><br>bert_model = BertVector(pooling_strategy=<span class="hljs-string">&quot;NONE&quot;</span>, max_seq_len=<span class="hljs-number">128</span>)<br>f = <span class="hljs-keyword">lambda</span> text: bert_model.encode([text])[<span class="hljs-string">&quot;encodes&quot;</span>][<span class="hljs-number">0</span>]<br><br><span class="hljs-comment"># 载入NER模型</span><br>custom_objects = &#123;<span class="hljs-string">&#x27;CRF&#x27;</span>: CRF, <span class="hljs-string">&#x27;crf_loss&#x27;</span>: crf_loss, <span class="hljs-string">&#x27;crf_viterbi_accuracy&#x27;</span>: crf_viterbi_accuracy&#125;<br>ner_model = load_model(<span class="hljs-string">&quot;../sequence_labeling/ccks2019_ner.h5&quot;</span>, custom_objects=custom_objects)<br><br><span class="hljs-comment"># 载入分类模型</span><br>best_model_path = <span class="hljs-string">&#x27;../text_classification/models/per-rel-08-0.9234.h5&#x27;</span><br>classification_model = load_model(best_model_path, custom_objects=&#123;<span class="hljs-string">&quot;Attention&quot;</span>: Attention&#125;)<br><br><span class="hljs-comment"># 分类与id的对应关系</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;../data/relation2id.json&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> g:<br>    relation_id_dict = json.loads(g.read())<br><br>id_relation_dict = &#123;v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> relation_id_dict.items()&#125;<br><br><br><span class="hljs-comment"># 从预测的标签列表中获取实体</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_entity</span>(<span class="hljs-params">sent, tags_list</span>):<br><br>    entity_dict = defaultdict(<span class="hljs-built_in">list</span>)<br>    i = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> char, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(sent, tags_list):<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;B-&#x27;</span> <span class="hljs-keyword">in</span> tag:<br>            entity = char<br>            j = i+<span class="hljs-number">1</span><br>            entity_type = tag.split(<span class="hljs-string">&#x27;-&#x27;</span>)[-<span class="hljs-number">1</span>]<br>            <span class="hljs-keyword">while</span> j &lt; <span class="hljs-built_in">min</span>(<span class="hljs-built_in">len</span>(sent), <span class="hljs-built_in">len</span>(tags_list)) <span class="hljs-keyword">and</span> <span class="hljs-string">&#x27;I-%s&#x27;</span> % entity_type <span class="hljs-keyword">in</span> tags_list[j]:<br>                entity += sent[j]<br>                j += <span class="hljs-number">1</span><br><br>            entity_dict[entity_type].append(entity)<br><br>        i += <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">dict</span>(entity_dict)<br><br><span class="hljs-comment"># 三元组提取类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TripleExtract</span>(<span class="hljs-title class_ inherited__">object</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, text</span>):<br>        self.text = text.replace(<span class="hljs-string">&quot; &quot;</span>, <span class="hljs-string">&quot;&quot;</span>)    <span class="hljs-comment"># 输入句子</span><br><br>    <span class="hljs-comment"># 获取输入句子中的实体（即：主体和客体）</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_entity</span>(<span class="hljs-params">self</span>):<br>        train_x = np.array([f(self. text)])<br>        y = np.argmax(ner_model.predict(train_x), axis=<span class="hljs-number">2</span>)<br>        y = [id_label_dict[_] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> y[<span class="hljs-number">0</span>] <span class="hljs-keyword">if</span> _]<br><br>        <span class="hljs-comment"># 输出预测结果</span><br>        <span class="hljs-keyword">return</span> get_entity(self.text, y)<br><br>    <span class="hljs-comment"># 对实体做关系判定</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">relation_classify</span>(<span class="hljs-params">self</span>):<br>        entities = self.get_entity()<br>        subjects = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(entities.get(<span class="hljs-string">&quot;SUBJ&quot;</span>, [])))<br>        objs = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(entities.get(<span class="hljs-string">&quot;OBJ&quot;</span>, [])))<br><br>        spo_list = []<br><br>        <span class="hljs-keyword">for</span> subj <span class="hljs-keyword">in</span> subjects:<br>            <span class="hljs-keyword">for</span> obj <span class="hljs-keyword">in</span> objs:<br>                sample = <span class="hljs-string">&#x27;$&#x27;</span>.join([subj, obj, self.text.replace(subj, <span class="hljs-string">&#x27;#&#x27;</span>*<span class="hljs-built_in">len</span>(subj)).replace(obj, <span class="hljs-string">&quot;#&quot;</span>*<span class="hljs-built_in">len</span>(obj))])<br>                vec = bert_model.encode([sample])[<span class="hljs-string">&quot;encodes&quot;</span>][<span class="hljs-number">0</span>]<br>                x_train = np.array([vec])<br><br>                <span class="hljs-comment"># 模型预测并输出预测结果</span><br>                predicted = classification_model.predict(x_train)<br>                y = np.argmax(predicted[<span class="hljs-number">0</span>])<br><br>                relation = id_relation_dict[y]<br>                <span class="hljs-keyword">if</span> relation != <span class="hljs-string">&quot;未知&quot;</span>:<br>                    spo_list.append([subj, relation, obj])<br><br>        <span class="hljs-keyword">return</span> spo_list<br><br>    <span class="hljs-comment"># 提取三元组</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">extractor</span>(<span class="hljs-params">self</span>):<br><br>        <span class="hljs-keyword">return</span> self.relation_classify()<br></code></pre></td></tr></table></figure><p>运行三元组提取脚本，代码如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># author: Jclian91</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><span class="hljs-comment"># time: 2020-03-14 20:53</span><br>import os, re, json, traceback<br><span class="hljs-keyword">from</span> pprint import pprint<br><br><span class="hljs-keyword">from</span> triple_extract.triple_extractor import TripleExtract<br><br><br>text = <span class="hljs-string">&quot;真人版的《花木兰》由新西兰导演妮基·卡罗执导，由刘亦菲、甄子丹、郑佩佩、巩俐、李连杰等加盟，几乎是全亚洲阵容。&quot;</span><br><br>triple_extract = TripleExtract(text)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;原文： %s&quot;</span> % text)<br>entities = triple_extract.get_entity()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;实体： &quot;</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">&#x27;&#x27;</span>)<br>pprint(entities)<br><br>spo_list = triple_extract.extractor()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;三元组： &quot;</span>, <span class="hljs-attribute">end</span>=<span class="hljs-string">&#x27;&#x27;</span>)<br>pprint(spo_list)<br></code></pre></td></tr></table></figure><p>我们在网上找几条样本进行测试，测试的结果如下：</p><blockquote><p>原文：真人版的《花木兰》由新西兰导演妮基·卡罗执导，由刘亦菲、甄子丹、郑佩佩、巩俐、李连杰等加盟，几乎是全亚洲阵容。实体： {'OBJ': ['妮基·卡罗', '刘亦菲', '甄子丹', '郑佩佩', '巩俐','李连杰'], 'SUBJ': ['花木兰']} 三元组： [['花木兰', '主演', '刘亦菲'],['花木兰', '导演', '妮基·卡罗'], ['花木兰', '主演', '甄子丹'],['花木兰', '主演', '李连杰'], ['花木兰', '主演', '郑佩佩'], ['花木兰','主演', '巩俐']]</p></blockquote><blockquote><p>原文：《冒险小王子》作者周艺文先生，教育、文学领域的专家学者以及来自全国各地的出版业从业者参加了此次沙龙，并围绕儿童文学创作这一话题做了精彩的分享与交流。实体： {'OBJ': ['周艺文'], 'SUBJ': ['冒险小王子']} 三元组：[['冒险小王子', '作者', '周艺文']]</p></blockquote><blockquote><p>原文：宋应星是江西奉新人，公元1587年生，经历过明朝腐败至灭亡的最后时期。实体： {'OBJ': ['江西奉新', '1587年'], 'SUBJ': ['宋应星']} 三元组：[['宋应星', '出生地', '江西奉新'], ['宋应星', '出生日期', '1587年']]</p></blockquote><blockquote><p>原文： 韩愈，字退之，河阳（今河南孟县）人。 实体： {'OBJ': ['退之','河阳'], 'SUBJ': ['韩愈']} 三元组： [['韩愈', '出生地', '河阳'],['韩愈', '字', '退之']]</p></blockquote><blockquote><p>原文：公开资料显示，李强，男，汉族，出生于1971年12月，北京市人，北京市委党校在职研究生学历，教育学学士学位，1996年11月入党，1993年7月参加工作。实体： {'OBJ': ['汉族', '1971年12月', '北京市', '北京市委党校'], 'SUBJ':['李强']} 三元组： [['李强', '民族', '汉族'], ['李强', '出生地','北京市'], ['李强', '毕业院校', '北京市委党校'], ['李强', '出生日期','1971年12月']]</p></blockquote><blockquote><p>原文：杨牧，本名王靖献，早期笔名叶珊，1940年生于台湾花莲，著名诗人、作家。实体： {'OBJ': ['1940年', '台湾花莲'], 'SUBJ': ['杨牧']} 三元组：[['杨牧', '出生地', '台湾花莲'], ['杨牧', '出生日期', '1940年']]</p></blockquote><blockquote><p>原文： 杨广是隋文帝杨坚的第二个儿子。 实体： {'OBJ': ['杨坚'],'SUBJ': ['杨广']} 三元组： [['杨广', '父亲', '杨坚']]</p></blockquote><blockquote><p>原文：此次权益变动后，何金明与妻子宋琦、其子何浩不再拥有对上市公司的控制权。实体： {'OBJ': ['何金明'], 'SUBJ': ['宋琦', '何浩']} 三元组： [['何浩','父亲', '何金明'], ['宋琦', '丈夫', '何金明']]</p></blockquote><blockquote><p>原文：线上直播发布会中，谭维维首次演绎了新歌《章存仙》，这首歌由钱雷作曲、尹约作词，尹约也在直播现场透过手机镜头跟网友互动聊天。实体： {'OBJ': ['谭维维', '钱雷', '尹约', '尹约'], 'SUBJ': ['章存仙']}三元组： [['章存仙', '作曲', '钱雷'], ['章存仙', '作词', '尹约'],['章存仙', '歌手', '谭维维']]</p></blockquote><blockquote><p>原文： “土木之变”后，造就了明代杰出的民族英雄于谦。 实体： {'OBJ':['明代'], 'SUBJ': ['于谦']} 三元组： [['于谦', '朝代', '明代']]</p></blockquote><blockquote><p>原文：另外，哈尔滨历史博物馆也是全国面积最小的国有博物馆，该场馆面积只有50平方米，可称之“微缩博物馆”。实体： {'OBJ': ['50平方米'], 'SUBJ': ['哈尔滨历史博物馆']} 三元组：[['哈尔滨历史博物馆', '占地面积', '50平方米']]</p></blockquote><blockquote><p>原文： 孙杨的妈妈叫杨明，孙杨的名字后面一个字也是来源于她的名字。实体： {'OBJ': ['杨明', '孙杨'], 'SUBJ': ['孙杨']} 三元组： [['孙杨','母亲', '杨明']]</p></blockquote><blockquote><p>原文：企查查显示，达鑫电子成立于1998年6月，法定代表人张高圳，注册资本772.33万美元，股东仅新加坡达鑫控股有限公司一名。实体： {'OBJ': ['1998年6月'], 'SUBJ': ['达鑫电子']} 三元组：[['达鑫电子', '成立日期', '1998年6月']]</p></blockquote><h3 id="总结">总结</h3><p>本文标题为限定领域的三元组抽取的一次尝试，之所以取名为限定领域，是因为该任务的实体关系是确定，一共为50种关系。</p><p>当然，上述方法还存在着诸多不足，参考苏建林的文章<ahref="https://spaces.ac.cn/archives/6671">基于DGCNN和概率图的轻量级信息抽取模型</a>，我们发现不足之处如下：</p><ul><li><p>主体和客体的标注策略有问题，因为句子中有时候主体和客体会重叠在一起；</p></li><li><p>新引入了一类关系：未知，是否有办法避免引入；</p></li><li><p>其他（暂时未想到）</p><p>从比赛的角度将，本文的办法效果未知，应该会比联合模型的效果差一些。但是，这是作为笔者自己的模型，算法是一种尝试，之所以采用这种方法，是因为笔者一开始是从开放领域的三元组抽取入手的，而这种方法方便扩展至开放领域。关于开放领域的三元组抽取，笔者稍后就会写文章介绍，敬请期待。本文的源代码已经公开至Github，网址为： <ahref="https://github.com/percent4/ccks_triple_extract">https://github.com/percent4/ccks_triple_extract</a>。</p></li></ul><h3 id="参考网址">参考网址</h3><ol type="1"><li>NLP（二十五）实现ALBERT+Bi-LSTM+CRF模型：https://blog.csdn.net/jclian91/article/details/104826655</li><li>NLP（二十一）人物关系抽取的一次实战：https://blog.csdn.net/jclian91/article/details/104380371</li><li>基于DGCNN和概率图的轻量级信息抽取模型：https://spaces.ac.cn/archives/6671</li></ol>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>关系抽取</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（二十五）实现ALBERT+Bi-LSTM+CRF模型</title>
    <link href="/2023/07/09/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%E4%BA%94%EF%BC%89%E5%AE%9E%E7%8E%B0ALBERT-Bi-LSTM-CRF%E6%A8%A1%E5%9E%8B/"/>
    <url>/2023/07/09/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%E4%BA%94%EF%BC%89%E5%AE%9E%E7%8E%B0ALBERT-Bi-LSTM-CRF%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在文章<ahref="https://blog.csdn.net/jclian91/article/details/104806598">NLP（二十四）利用ALBERT实现命名实体识别</a>中，笔者介绍了ALBERT+Bi-LSTM模型在命名实体识别方面的应用。</p><p>在本文中，笔者将介绍如何实现ALBERT+Bi-LSTM+CRF模型，以及在人民日报NER数据集和CLUENER数据集上的表现。</p><p>功能项目方面的介绍里面不再多介绍，笔者只介绍模型训练和模型预测部分的代码。项目方面的代码可以参考文章<ahref="https://blog.csdn.net/jclian91/article/details/104806598">NLP（二十四）利用ALBERT实现命名实体识别</a>，模型为ALBERT+Bi-LSTM+CRF，结构图如下：</p><figure><img src="/img/nlp25_1.png" alt="ALBERT+Bi-LSTM+CRF模型结构图" /><figcaption aria-hidden="true">ALBERT+Bi-LSTM+CRF模型结构图</figcaption></figure><p>模型训练的代码（albert_model_train.py）中新增导入keras-contrib模块中的CRF层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> keras_contrib.layers <span class="hljs-keyword">import</span> CRF<br><span class="hljs-keyword">from</span> keras_contrib.losses <span class="hljs-keyword">import</span> crf_loss<br><span class="hljs-keyword">from</span> keras_contrib.metrics <span class="hljs-keyword">import</span> crf_accuracy, crf_viterbi_accuracy<br></code></pre></td></tr></table></figure><p>模型方面的代码如下： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Build model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_model</span>(<span class="hljs-params">max_para_length, n_tags</span>):<br>    <span class="hljs-comment"># Bert Embeddings</span><br>    bert_output = Input(shape=(max_para_length, <span class="hljs-number">312</span>, ), name=<span class="hljs-string">&quot;bert_output&quot;</span>)<br>    <span class="hljs-comment"># LSTM model</span><br>    lstm = Bidirectional(LSTM(units=<span class="hljs-number">128</span>, return_sequences=<span class="hljs-literal">True</span>), name=<span class="hljs-string">&quot;bi_lstm&quot;</span>)(bert_output)<br>    drop = Dropout(<span class="hljs-number">0.1</span>, name=<span class="hljs-string">&quot;dropout&quot;</span>)(lstm)<br>    dense = TimeDistributed(Dense(n_tags, activation=<span class="hljs-string">&quot;softmax&quot;</span>), name=<span class="hljs-string">&quot;time_distributed&quot;</span>)(drop)<br>    crf = CRF(n_tags)<br>    out = crf(dense)<br>    model = Model(inputs=bert_output, outputs=out)<br>    <span class="hljs-comment"># model.compile(optimizer=&#x27;adam&#x27;, loss=&#x27;categorical_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;])</span><br>    model.<span class="hljs-built_in">compile</span>(loss=crf.loss_function, optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>, metrics=[crf.accuracy])<br><br>    <span class="hljs-comment"># 模型结构总结</span><br>    model.summary()<br>    plot_model(model, to_file=<span class="hljs-string">&quot;albert_bi_lstm.png&quot;</span>, show_shapes=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure></p><p>设置文本的最大长度MAX_SEQ_LEN =128，训练10个epoch，在测试集上的F1值（利用seqeval模块评估）输出如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs apache">           <span class="hljs-attribute">precision</span>    recall  f1-score   support<br><br>      <span class="hljs-attribute">LOC</span>     <span class="hljs-number">0</span>.<span class="hljs-number">9766</span>    <span class="hljs-number">0</span>.<span class="hljs-number">9032</span>    <span class="hljs-number">0</span>.<span class="hljs-number">9385</span>      <span class="hljs-number">3658</span><br>      <span class="hljs-attribute">ORG</span>     <span class="hljs-number">0</span>.<span class="hljs-number">9700</span>    <span class="hljs-number">0</span>.<span class="hljs-number">9465</span>    <span class="hljs-number">0</span>.<span class="hljs-number">9581</span>      <span class="hljs-number">2185</span><br>      <span class="hljs-attribute">PER</span>     <span class="hljs-number">0</span>.<span class="hljs-number">9880</span>    <span class="hljs-number">0</span>.<span class="hljs-number">9721</span>    <span class="hljs-number">0</span>.<span class="hljs-number">9800</span>      <span class="hljs-number">1864</span><br><br><span class="hljs-attribute">micro</span> avg     <span class="hljs-number">0</span>.<span class="hljs-number">9775</span>    <span class="hljs-number">0</span>.<span class="hljs-number">9321</span>    <span class="hljs-number">0</span>.<span class="hljs-number">9543</span>      <span class="hljs-number">7707</span><br><span class="hljs-attribute">macro</span> avg     <span class="hljs-number">0</span>.<span class="hljs-number">9775</span>    <span class="hljs-number">0</span>.<span class="hljs-number">9321</span>    <span class="hljs-number">0</span>.<span class="hljs-number">9541</span>      <span class="hljs-number">7707</span><br></code></pre></td></tr></table></figure><p>之前用ALBERT+Bi-LSTM模型得到的F1值为91.96%，而ALBERT+Bi-LSTM+CRF模型能达到95.43%，提升效果不错。</p><p>模型预测代码（model_predict.py）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># author: Jclian91</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><span class="hljs-comment"># time: 2020-03-11 13:16</span><br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> keras_contrib.layers <span class="hljs-keyword">import</span> CRF<br><span class="hljs-keyword">from</span> keras_contrib.losses <span class="hljs-keyword">import</span> crf_loss<br><span class="hljs-keyword">from</span> keras_contrib.metrics <span class="hljs-keyword">import</span> crf_accuracy, crf_viterbi_accuracy<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br><span class="hljs-keyword">from</span> pprint <span class="hljs-keyword">import</span> pprint<br><br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> MAX_SEQ_LEN, event_type<br><span class="hljs-keyword">from</span> albert_zh.extract_feature <span class="hljs-keyword">import</span> BertVector<br><br><span class="hljs-comment"># 读取label2id字典</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s_label2id.json&quot;</span> % event_type, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> h:<br>    label_id_dict = json.loads(h.read())<br><br>id_label_dict = &#123;v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> label_id_dict.items()&#125;<br><br><span class="hljs-comment"># 利用ALBERT提取文本特征</span><br>bert_model = BertVector(pooling_strategy=<span class="hljs-string">&quot;NONE&quot;</span>, max_seq_len=MAX_SEQ_LEN)<br>f = <span class="hljs-keyword">lambda</span> text: bert_model.encode([text])[<span class="hljs-string">&quot;encodes&quot;</span>][<span class="hljs-number">0</span>]<br><br><span class="hljs-comment"># 载入模型</span><br>custom_objects = &#123;<span class="hljs-string">&#x27;CRF&#x27;</span>: CRF, <span class="hljs-string">&#x27;crf_loss&#x27;</span>: crf_loss, <span class="hljs-string">&#x27;crf_viterbi_accuracy&#x27;</span>: crf_viterbi_accuracy&#125;<br>ner_model = load_model(<span class="hljs-string">&quot;%s_ner.h5&quot;</span> % event_type, custom_objects=custom_objects)<br><br><br><span class="hljs-comment"># 从预测的标签列表中获取实体</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_entity</span>(<span class="hljs-params">sent, tags_list</span>):<br><br>    entity_dict = defaultdict(<span class="hljs-built_in">list</span>)<br>    i = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> char, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(sent, tags_list):<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;B-&#x27;</span> <span class="hljs-keyword">in</span> tag:<br>            entity = char<br>            j = i+<span class="hljs-number">1</span><br>            entity_type = tag.split(<span class="hljs-string">&#x27;-&#x27;</span>)[-<span class="hljs-number">1</span>]<br>            <span class="hljs-keyword">while</span> j &lt; <span class="hljs-built_in">min</span>(<span class="hljs-built_in">len</span>(sent), <span class="hljs-built_in">len</span>(tags_list)) <span class="hljs-keyword">and</span> <span class="hljs-string">&#x27;I-%s&#x27;</span> % entity_type <span class="hljs-keyword">in</span> tags_list[j]:<br>                entity += sent[j]<br>                j += <span class="hljs-number">1</span><br><br>            entity_dict[entity_type].append(entity)<br><br>        i += <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">dict</span>(entity_dict)<br><br><br><span class="hljs-comment"># 输入句子，进行预测</span><br><span class="hljs-keyword">while</span> <span class="hljs-number">1</span>:<br>    <span class="hljs-comment"># 输入句子</span><br>    text = <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;Please enter an sentence: &quot;</span>).replace(<span class="hljs-string">&#x27; &#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>    <span class="hljs-comment"># 利用训练好的模型进行预测</span><br>    train_x = np.array([f(text)])<br>    y = np.argmax(ner_model.predict(train_x), axis=<span class="hljs-number">2</span>)<br>    y = [id_label_dict[_] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> y[<span class="hljs-number">0</span>] <span class="hljs-keyword">if</span> _]<br><br>    <span class="hljs-comment"># 输出预测结果</span><br>    pprint(get_entity(text, y))<br></code></pre></td></tr></table></figure><p>在网上找几条新闻，预测结果如下：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs prolog"><span class="hljs-symbol">Please</span> enter an sentence: 驴妈妈旅游网创始人洪清华近日接受媒体采访谈及驴妈妈的发展模式时表示：现在，电商有两种做法——小而美的电商追求盈利，大而全的电商钟情规模。<br>&#123;<span class="hljs-string">&#x27;PER&#x27;</span>: [<span class="hljs-string">&#x27;洪清华&#x27;</span>]&#125;<br><span class="hljs-symbol">Please</span> enter an sentence: <span class="hljs-symbol">EF</span>英孚教育集团是全球最大的私人英语教育机构，主要致力于英语培训、留学旅游以及英语文化交流等方面。<br>&#123;<span class="hljs-string">&#x27;ORG&#x27;</span>: [<span class="hljs-string">&#x27;EF英孚教育集团&#x27;</span>]&#125;<br><span class="hljs-symbol">Please</span> enter an sentence: 宋元时期起，在台湾早期开发的过程中，中华文化传统已随着大陆垦民传入台湾。<br>&#123;<span class="hljs-string">&#x27;LOC&#x27;</span>: [<span class="hljs-string">&#x27;台湾&#x27;</span>, <span class="hljs-string">&#x27;中华&#x27;</span>, <span class="hljs-string">&#x27;台湾&#x27;</span>]&#125;<br><span class="hljs-symbol">Please</span> enter an sentence: 吸引了众多投资者来津发展，康师傅红烧牛肉面就是于<span class="hljs-number">1992</span>年在天津诞生。<br>&#123;<span class="hljs-string">&#x27;LOC&#x27;</span>: [<span class="hljs-string">&#x27;天津&#x27;</span>]&#125;<br><span class="hljs-symbol">Please</span> enter an sentence: 经过激烈角逐，那英战队成功晋级<span class="hljs-number">16</span>强的学员有实力非凡的姚贝娜、挚情感打动观众的朱克、音乐创作能力十分突出的侯磊。<br>&#123;<span class="hljs-string">&#x27;PER&#x27;</span>: [<span class="hljs-string">&#x27;姚贝娜&#x27;</span>, <span class="hljs-string">&#x27;朱克&#x27;</span>, <span class="hljs-string">&#x27;侯磊&#x27;</span>]&#125;<br></code></pre></td></tr></table></figure><p>接下来我们看看该模型在CLUENER数据集上的表现。CLUENER数据集是在清华大学开源的文本分类数据集THUCTC基础上，选出部分数据进行细粒度命名实体标注，原数据来源于SinaNewsRSS，实体有：地址（address），书名（book），公司（company），游戏（game），政府（goverment），电影（movie），姓名（name），组织机构（organization），职位（position），景点（scene），该数据集的介绍网站为：<ahref="https://www.cluebenchmarks.com/introduce.html">https://www.cluebenchmarks.com/introduce.html</a>。</p><p>下载数据集，用脚本将其处理成模型支持的数据格式，因为缺少test数据集，故模型评测的时候用dev数据集代替。设置模型的文本最大长度MAX_SEQ_LEN=128，训练10个epoch，在测试集上的F1值（利用seqeval模块评估）输出如下：</p><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs livecodeserver"><span class="hljs-keyword">sentences</span> <span class="hljs-built_in">length</span>: <span class="hljs-number">10748</span> <br><span class="hljs-keyword">last</span> <span class="hljs-keyword">sentence</span>:  艺术家也讨厌画廊的老板，内心恨他们，这样的话，你是在这样的状态下，两年都是一次性合作，甚至两年、<br><span class="hljs-built_in">start</span> ALBERT encding<br><span class="hljs-function"><span class="hljs-keyword">end</span> <span class="hljs-title">ALBERT</span> <span class="hljs-title">encoding</span></span><br><span class="hljs-keyword">sentences</span> <span class="hljs-built_in">length</span>: <span class="hljs-number">1343</span> <br><span class="hljs-keyword">last</span> <span class="hljs-keyword">sentence</span>:  另外意大利的PlayGeneration杂志也刚刚给出了<span class="hljs-number">92</span>%的高分。<br><span class="hljs-built_in">start</span> ALBERT encding<br><span class="hljs-function"><span class="hljs-keyword">end</span> <span class="hljs-title">ALBERT</span> <span class="hljs-title">encoding</span></span><br><span class="hljs-keyword">sentences</span> <span class="hljs-built_in">length</span>: <span class="hljs-number">1343</span> <br><span class="hljs-keyword">last</span> <span class="hljs-keyword">sentence</span>:  另外意大利的PlayGeneration杂志也刚刚给出了<span class="hljs-number">92</span>%的高分。<br><span class="hljs-built_in">start</span> ALBERT encding<br><span class="hljs-function"><span class="hljs-keyword">end</span> <span class="hljs-title">ALBERT</span> <span class="hljs-title">encoding</span></span><br>......<br>.......<br>              precision    recall  f1-score   support<br><br>        book     <span class="hljs-number">0.9343</span>    <span class="hljs-number">0.8421</span>    <span class="hljs-number">0.8858</span>       <span class="hljs-number">152</span><br>    position     <span class="hljs-number">0.9549</span>    <span class="hljs-number">0.8965</span>    <span class="hljs-number">0.9248</span>       <span class="hljs-number">425</span><br>  government     <span class="hljs-number">0.9372</span>    <span class="hljs-number">0.9180</span>    <span class="hljs-number">0.9275</span>       <span class="hljs-number">244</span><br>        game     <span class="hljs-number">0.6968</span>    <span class="hljs-number">0.6725</span>    <span class="hljs-number">0.6844</span>       <span class="hljs-number">287</span><br>organization     <span class="hljs-number">0.8836</span>    <span class="hljs-number">0.8605</span>    <span class="hljs-number">0.8719</span>       <span class="hljs-number">344</span><br>     company     <span class="hljs-number">0.8659</span>    <span class="hljs-number">0.7760</span>    <span class="hljs-number">0.8184</span>       <span class="hljs-number">366</span><br>     address     <span class="hljs-number">0.8394</span>    <span class="hljs-number">0.8187</span>    <span class="hljs-number">0.8289</span>       <span class="hljs-number">364</span><br>       movie     <span class="hljs-number">0.9217</span>    <span class="hljs-number">0.7067</span>    <span class="hljs-number">0.8000</span>       <span class="hljs-number">150</span><br>        name     <span class="hljs-number">0.8771</span>    <span class="hljs-number">0.8071</span>    <span class="hljs-number">0.8406</span>       <span class="hljs-number">451</span><br>       scene     <span class="hljs-number">0.9939</span>    <span class="hljs-number">0.8191</span>    <span class="hljs-number">0.8981</span>       <span class="hljs-number">199</span><br><br>   micro <span class="hljs-built_in">avg</span>     <span class="hljs-number">0.8817</span>    <span class="hljs-number">0.8172</span>    <span class="hljs-number">0.8482</span>      <span class="hljs-number">2982</span><br>   macro <span class="hljs-built_in">avg</span>     <span class="hljs-number">0.8835</span>    <span class="hljs-number">0.8172</span>    <span class="hljs-number">0.8482</span>      <span class="hljs-number">2982</span><br></code></pre></td></tr></table></figure><p>在网上找几条新闻，预测结果如下：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs prolog"><span class="hljs-symbol">Please</span> enter an sentence: 据中山外侨局消息，近日，秘鲁国会议员、祖籍中山市开发区的玛利亚·洪大女士在秘鲁国会大厦亲切会见了中山市人民政府副市长冯煜荣一行，对中山市友好代表团的来访表示热烈的欢迎。<br>&#123;<span class="hljs-string">&#x27;address&#x27;</span>: [<span class="hljs-string">&#x27;中山市开发区&#x27;</span>, <span class="hljs-string">&#x27;秘鲁国会大厦&#x27;</span>],<br> <span class="hljs-string">&#x27;government&#x27;</span>: [<span class="hljs-string">&#x27;中山外侨局&#x27;</span>, <span class="hljs-string">&#x27;秘鲁国会&#x27;</span>, <span class="hljs-string">&#x27;中山市人民政府&#x27;</span>],<br> <span class="hljs-string">&#x27;name&#x27;</span>: [<span class="hljs-string">&#x27;玛利亚·洪大&#x27;</span>, <span class="hljs-string">&#x27;冯煜荣&#x27;</span>],<br> <span class="hljs-string">&#x27;position&#x27;</span>: [<span class="hljs-string">&#x27;议员&#x27;</span>, <span class="hljs-string">&#x27;副市长&#x27;</span>]&#125;<br> <span class="hljs-symbol">Please</span> enter an sentence: “隔离结束回来，发现公司不见了”，网上的段子，真发生在了昆山达鑫电子有限公司员工身上。<br>&#123;<span class="hljs-string">&#x27;company&#x27;</span>: [<span class="hljs-string">&#x27;昆山达鑫电子有限公司&#x27;</span>]&#125;<br><span class="hljs-symbol">Please</span> enter an sentence: 由黄子韬、易烊千玺、胡冰卿、王子腾等一众青年演员主演的热血励志剧《热血同行》正在热播中。<br>&#123;<span class="hljs-string">&#x27;game&#x27;</span>: [<span class="hljs-string">&#x27;《热血同行》&#x27;</span>], <span class="hljs-string">&#x27;name&#x27;</span>: [<span class="hljs-string">&#x27;黄子韬&#x27;</span>, <span class="hljs-string">&#x27;易烊千玺&#x27;</span>, <span class="hljs-string">&#x27;胡冰卿&#x27;</span>, <span class="hljs-string">&#x27;王子腾&#x27;</span>], <span class="hljs-string">&#x27;position&#x27;</span>: [<span class="hljs-string">&#x27;演员&#x27;</span>]&#125;<br><span class="hljs-symbol">Please</span> enter an sentence: 近日，由作家出版社主办的韩作荣《天生我才——李白传》新书发布会在京举行<br>&#123;<span class="hljs-string">&#x27;book&#x27;</span>: [<span class="hljs-string">&#x27;《天生我才——李白传》&#x27;</span>], <span class="hljs-string">&#x27;name&#x27;</span>: [<span class="hljs-string">&#x27;韩作荣&#x27;</span>], <span class="hljs-string">&#x27;organization&#x27;</span>: [<span class="hljs-string">&#x27;作家出版社&#x27;</span>]&#125;<br></code></pre></td></tr></table></figure><p>本项目已经开源，Github网址为：<ahref="https://github.com/percent4/ALBERT_NER_KERAS">https://github.com/percent4/ALBERT_NER_KERAS</a>。</p><p>本文到此结束，感谢大家阅读，欢迎关注笔者的微信公众号：<code>NLP奇幻之旅</code>。</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>ALBERT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（二十四）利用ALBERT实现命名实体识别</title>
    <link href="/2023/07/09/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%E5%9B%9B%EF%BC%89%E5%88%A9%E7%94%A8ALBERT%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/"/>
    <url>/2023/07/09/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%E5%9B%9B%EF%BC%89%E5%88%A9%E7%94%A8ALBERT%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文将会介绍如何利用ALBERT来实现<code>命名实体识别</code>。如果有对<code>命名实体识别</code>不清楚的读者，请参考笔者的文章<ahref="https://percent4.github.io/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/">NLP入门（四）命名实体识别（NER）</a>。</p><p>本文的项目结构如下：</p><figure><img src="/img/nlp24_1.png" alt="项目结构" /><figcaption aria-hidden="true">项目结构</figcaption></figure><p>其中，<code>albert_zh</code>为ALBERT提取文本特征模块，这方面的代码已经由别人开源，我们只需要拿来使用即可。data目录下为我们本次讲解所需要的数据，图中只有example开头的数据集，这是人民日报的标注语料，实体为人名（PER）、地名（LOC）和组织机构名（ORG）。数据集一行一个字符以及标注符号，标注系统采用<code>BIO</code>系统，我们以example.train的第一句为例，标注信息如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs bash">海 O<br>钓 O<br>比 O<br>赛 O<br>地 O<br>点 O<br>在 O<br>厦 B-LOC<br>门 I-LOC<br>与 O<br>金 B-LOC<br>门 I-LOC<br>之 O<br>间 O<br>的 O<br>海 O<br>域 O<br>。 O<br></code></pre></td></tr></table></figure><p>在<code>utils.py</code>文件中，配置了一些关于文件路径和模型参数方面的信息，其中规定了输入的文本长度最大为128，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># author: Jclian91</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><span class="hljs-comment"># time: 2020-03-11 21:12</span><br><br><span class="hljs-comment"># 数据相关的配置</span><br>event_type = <span class="hljs-string">&quot;example&quot;</span><br><br>train_file_path = <span class="hljs-string">&quot;./data/%s.train&quot;</span> % event_type<br>dev_file_path = <span class="hljs-string">&quot;./data/%s.dev&quot;</span> % event_type<br>test_file_path = <span class="hljs-string">&quot;./data/%s.test&quot;</span> % event_type<br><br><span class="hljs-comment"># 模型相关的配置</span><br>MAX_SEQ_LEN = <span class="hljs-number">128</span>   <span class="hljs-comment"># 输入的文本最大长度</span><br></code></pre></td></tr></table></figure><p>在<code>load_data.py</code>文件中，我们将处理训练集、验证集和测试集数据，并将标签转换为id，形成label2id.json文件，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># author: Jclian91</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><span class="hljs-comment"># time: 2020-03-11 10:04</span><br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> train_file_path, event_type<br><br><br><span class="hljs-comment"># 读取数据集</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_data</span>(<span class="hljs-params">file_path</span>):<br>    <span class="hljs-comment"># 读取数据集</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        content = [_.strip() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> f.readlines()]<br><br>    <span class="hljs-comment"># 添加原文句子以及该句子的标签</span><br><br>    <span class="hljs-comment"># 读取空行所在的行号</span><br>    index = [-<span class="hljs-number">1</span>]<br>    index.extend([i <span class="hljs-keyword">for</span> i, _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(content) <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27; &#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> _])<br>    index.append(<span class="hljs-built_in">len</span>(content))<br><br>    <span class="hljs-comment"># 按空行分割，读取原文句子及标注序列</span><br>    sentences, tags = [], []<br>    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(index)-<span class="hljs-number">1</span>):<br>        sent, tag = [], []<br>        segment = content[index[j]+<span class="hljs-number">1</span>: index[j+<span class="hljs-number">1</span>]]<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> segment:<br>            sent.append(line.split()[<span class="hljs-number">0</span>])<br>            tag.append(line.split()[-<span class="hljs-number">1</span>])<br><br>        sentences.append(<span class="hljs-string">&#x27;&#x27;</span>.join(sent))<br>        tags.append(tag)<br><br>    <span class="hljs-comment"># 去除空的句子及标注序列，一般放在末尾</span><br>    sentences = [_ <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> sentences <span class="hljs-keyword">if</span> _]<br>    tags = [_ <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> tags <span class="hljs-keyword">if</span> _]<br><br>    <span class="hljs-keyword">return</span> sentences, tags<br><br><br><span class="hljs-comment"># 读取训练集数据</span><br><span class="hljs-comment"># 将标签转换成id</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">label2id</span>():<br><br>    train_sents, train_tags = read_data(train_file_path)<br><br>    <span class="hljs-comment"># 标签转换成id，并保存成文件</span><br>    unique_tags = []<br>    <span class="hljs-keyword">for</span> seq <span class="hljs-keyword">in</span> train_tags:<br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> seq:<br>            <span class="hljs-keyword">if</span> _ <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> unique_tags:<br>                unique_tags.append(_)<br><br>    label_id_dict = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(unique_tags, <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(unique_tags) + <span class="hljs-number">1</span>)))<br><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s_label2id.json&quot;</span> % event_type, <span class="hljs-string">&quot;w&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> g:<br>        g.write(json.dumps(label_id_dict, ensure_ascii=<span class="hljs-literal">False</span>, indent=<span class="hljs-number">2</span>))<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    label2id()<br></code></pre></td></tr></table></figure><p>运行代码，生成的example_label2id.json文件如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;O&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;B-LOC&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;I-LOC&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">3</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;B-PER&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">4</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;I-PER&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">5</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;B-ORG&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">6</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;I-ORG&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">7</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>生成该文件是为了方便我们后边的模型训练和预测的时候调用。</p><p>接着就是最重要的模型训练部分了，模型的结构图如下：</p><figure><img src="/img/nlp24_2.png" alt="模型结构图" /><figcaption aria-hidden="true">模型结构图</figcaption></figure><p>我们采用ALBERT作为文本特征提取，后接经典的序列标注算法——Bi-LSTM算法。<code>albert_model_train.py</code>的完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Model, Input<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Dense, Bidirectional, Dropout, LSTM, TimeDistributed, Masking<br><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> to_categorical, plot_model<br><span class="hljs-keyword">from</span> seqeval.metrics <span class="hljs-keyword">import</span> classification_report<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> event_type<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> MAX_SEQ_LEN, train_file_path, test_file_path, dev_file_path<br><span class="hljs-keyword">from</span> load_data <span class="hljs-keyword">import</span> read_data<br><span class="hljs-keyword">from</span> albert_zh.extract_feature <span class="hljs-keyword">import</span> BertVector<br><br><span class="hljs-comment"># 利用ALBERT提取文本特征</span><br>bert_model = BertVector(pooling_strategy=<span class="hljs-string">&quot;NONE&quot;</span>, max_seq_len=MAX_SEQ_LEN)<br>f = <span class="hljs-keyword">lambda</span> text: bert_model.encode([text])[<span class="hljs-string">&quot;encodes&quot;</span>][<span class="hljs-number">0</span>]<br><br><span class="hljs-comment"># 读取label2id字典</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s_label2id.json&quot;</span> % event_type, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> h:<br>    label_id_dict = json.loads(h.read())<br><br>id_label_dict = &#123;v:k <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> label_id_dict.items()&#125;<br><br><br><span class="hljs-comment"># 载入数据</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">input_data</span>(<span class="hljs-params">file_path</span>):<br><br>    sentences, tags = read_data(file_path)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;sentences length: %s &quot;</span> % <span class="hljs-built_in">len</span>(sentences))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;last sentence: &quot;</span>, sentences[-<span class="hljs-number">1</span>])<br><br>    <span class="hljs-comment"># ALBERT ERCODING</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;start ALBERT encding&quot;</span>)<br>    x = np.array([f(sent) <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sentences])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;end ALBERT encoding&quot;</span>)<br><br>    <span class="hljs-comment"># 对y值统一长度为MAX_SEQ_LEN</span><br>    new_y = []<br>    <span class="hljs-keyword">for</span> seq <span class="hljs-keyword">in</span> tags:<br>        num_tag = [label_id_dict[_] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> seq]<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(seq) &lt; MAX_SEQ_LEN:<br>            num_tag = num_tag + [<span class="hljs-number">0</span>] * (MAX_SEQ_LEN-<span class="hljs-built_in">len</span>(seq))<br>        <span class="hljs-keyword">else</span>:<br>            num_tag = num_tag[: MAX_SEQ_LEN]<br><br>        new_y.append(num_tag)<br><br>    <span class="hljs-comment"># 将y中的元素编码成ont-hot encoding</span><br>    y = np.empty(shape=(<span class="hljs-built_in">len</span>(tags), MAX_SEQ_LEN, <span class="hljs-built_in">len</span>(label_id_dict.keys())+<span class="hljs-number">1</span>))<br><br>    <span class="hljs-keyword">for</span> i, seq <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(new_y):<br>        y[i, :, :] = to_categorical(seq, num_classes=<span class="hljs-built_in">len</span>(label_id_dict.keys())+<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">return</span> x, y<br><br><br><span class="hljs-comment"># Build model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_model</span>(<span class="hljs-params">max_para_length, n_tags</span>):<br>    <span class="hljs-comment"># Bert Embeddings</span><br>    bert_output = Input(shape=(max_para_length, <span class="hljs-number">312</span>, ), name=<span class="hljs-string">&quot;bert_output&quot;</span>)<br>    <span class="hljs-comment"># LSTM model</span><br>    lstm = Bidirectional(LSTM(units=<span class="hljs-number">128</span>, return_sequences=<span class="hljs-literal">True</span>), name=<span class="hljs-string">&quot;bi_lstm&quot;</span>)(bert_output)<br>    drop = Dropout(<span class="hljs-number">0.1</span>, name=<span class="hljs-string">&quot;dropout&quot;</span>)(lstm)<br>    out = TimeDistributed(Dense(n_tags, activation=<span class="hljs-string">&quot;softmax&quot;</span>), name=<span class="hljs-string">&quot;time_distributed&quot;</span>)(drop)<br>    model = Model(inputs=bert_output, outputs=out)<br>    model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>, loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br>    <span class="hljs-comment"># 模型结构总结</span><br>    model.summary()<br>    plot_model(model, to_file=<span class="hljs-string">&quot;albert_bi_lstm.png&quot;</span>, show_shapes=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-comment"># 模型训练</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_model</span>():<br><br>    <span class="hljs-comment"># 读取训练集，验证集和测试集数据</span><br>    train_x, train_y = input_data(train_file_path)<br>    dev_x, dev_y = input_data(dev_file_path)<br>    test_x, test_y = input_data(test_file_path)<br><br>    <span class="hljs-comment"># 模型训练</span><br>    model = build_model(MAX_SEQ_LEN, <span class="hljs-built_in">len</span>(label_id_dict.keys())+<span class="hljs-number">1</span>)<br><br>    history = model.fit(train_x, train_y, validation_data=(dev_x, dev_y), batch_size=<span class="hljs-number">32</span>, epochs=<span class="hljs-number">10</span>)<br><br>    model.save(<span class="hljs-string">&quot;%s_ner.h5&quot;</span> % event_type)<br><br>    <span class="hljs-comment"># 绘制loss和acc图像</span><br>    plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>    epochs = <span class="hljs-built_in">len</span>(history.history[<span class="hljs-string">&#x27;loss&#x27;</span>])<br>    plt.plot(<span class="hljs-built_in">range</span>(epochs), history.history[<span class="hljs-string">&#x27;loss&#x27;</span>], label=<span class="hljs-string">&#x27;loss&#x27;</span>)<br>    plt.plot(<span class="hljs-built_in">range</span>(epochs), history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>], label=<span class="hljs-string">&#x27;val_loss&#x27;</span>)<br>    plt.legend()<br><br>    plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>    epochs = <span class="hljs-built_in">len</span>(history.history[<span class="hljs-string">&#x27;acc&#x27;</span>])<br>    plt.plot(<span class="hljs-built_in">range</span>(epochs), history.history[<span class="hljs-string">&#x27;acc&#x27;</span>], label=<span class="hljs-string">&#x27;acc&#x27;</span>)<br>    plt.plot(<span class="hljs-built_in">range</span>(epochs), history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>], label=<span class="hljs-string">&#x27;val_acc&#x27;</span>)<br>    plt.legend()<br>    plt.savefig(<span class="hljs-string">&quot;%s_loss_acc.png&quot;</span> % event_type)<br><br>    <span class="hljs-comment"># 模型在测试集上的表现</span><br>    <span class="hljs-comment"># 预测标签</span><br>    y = np.argmax(model.predict(test_x), axis=<span class="hljs-number">2</span>)<br>    pred_tags = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(y.shape[<span class="hljs-number">0</span>]):<br>        pred_tags.append([id_label_dict[_] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> y[i] <span class="hljs-keyword">if</span> _])<br><br>    <span class="hljs-comment"># 因为存在预测的标签长度与原来的标注长度不一致的情况，因此需要调整预测的标签</span><br>    test_sents, test_tags = read_data(test_file_path)<br>    final_tags = []<br>    <span class="hljs-keyword">for</span> test_tag, pred_tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(test_tags, pred_tags):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(test_tag) == <span class="hljs-built_in">len</span>(pred_tag):<br>            final_tags.append(pred_tag)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">len</span>(test_tag) &lt; <span class="hljs-built_in">len</span>(pred_tag):<br>            final_tags.append(pred_tag[:<span class="hljs-built_in">len</span>(test_tag)])<br>        <span class="hljs-keyword">else</span>:<br>            final_tags.append(pred_tag + [<span class="hljs-string">&#x27;O&#x27;</span>] * (<span class="hljs-built_in">len</span>(test_tag) - <span class="hljs-built_in">len</span>(pred_tag)))<br><br>    <span class="hljs-comment"># 利用seqeval对测试集进行验证</span><br>    <span class="hljs-built_in">print</span>(classification_report(test_tags, final_tags, digits=<span class="hljs-number">4</span>))<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    train_model()<br></code></pre></td></tr></table></figure><p>模型训练过程中的输出结果如下（部分输出省略）：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs markdown">sentences length: 20864 <br>last sentence:  思想自由是对自我而言，用中国传统的说法是有所为；兼容并包是指对待他人，要有所不为。<br>start ALBERT encding<br>end ALBERT encoding<br>sentences length: 2318 <br>last sentence:  良性肿瘤、恶性肿瘤虽然只是一字之差，但两者有根本性的差别。<br>start ALBERT encding<br>end ALBERT encoding<br>sentences length: 4636 <br>last sentence:  因此，村民进行民主选举的心态是在这样一种背景映衬下加以表现的，这无疑给该片增添了几分厚重的历史文化氛围。<br>start ALBERT encding<br>end ALBERT encoding<br><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_<br><span class="hljs-section">Layer (type)                 Output Shape              Param #   </span><br><span class="hljs-section">=================================================================</span><br>bert<span class="hljs-emphasis">_output (InputLayer)     (None, 128, 312)          0         </span><br><span class="hljs-emphasis"><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_</span><br>bi<span class="hljs-emphasis">_lstm (Bidirectional)      (None, 128, 256)          451584    </span><br><span class="hljs-emphasis"><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_</span><br>dropout (Dropout)            (None, 128, 256)          0         <br><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_<br><span class="hljs-section">time<span class="hljs-emphasis">_distributed (TimeDistri (None, 128, 8)            2056      </span></span><br><span class="hljs-emphasis"><span class="hljs-section">=================================================================</span></span><br><span class="hljs-emphasis"><span class="hljs-section">Total params: 453,640</span></span><br><span class="hljs-emphasis"><span class="hljs-section">Trainable params: 453,640</span></span><br><span class="hljs-emphasis"><span class="hljs-section">Non-trainable params: 0</span></span><br><span class="hljs-emphasis"><span class="hljs-section"><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_</span></span><br><span class="hljs-section">Train on 20864 samples, validate on 2318 samples</span><br><span class="hljs-section">......</span><br><span class="hljs-section">......</span><br><span class="hljs-section">......</span><br><span class="hljs-section">20864/20864 [==============================] - 97s 5ms/step - loss: 0.0091 - acc: 0.9969 - val<span class="hljs-emphasis">_loss: 0.0397 - val_</span>acc: 0.9900</span><br><span class="hljs-section">           precision    recall  f1-score   support</span><br><span class="hljs-section"></span><br><span class="hljs-section">      ORG     0.9001    0.9112    0.9056      2185</span><br><span class="hljs-section">      LOC     0.9383    0.8898    0.9134      3658</span><br><span class="hljs-section">      PER     0.9543    0.9415    0.9479      1864</span><br><span class="hljs-section"></span><br><span class="hljs-section">micro avg     0.9310    0.9084    0.9196      7707</span><br><span class="hljs-section">macro avg     0.9313    0.9084    0.9195      7707</span><br></code></pre></td></tr></table></figure><p>在测试集上的F1值为91.96%。同时，训练过程中的loss和acc曲线如下图：</p><figure><img src="/img/nlp24_3.png" alt="训练过程中的loss和acc曲线图" /><figcaption aria-hidden="true">训练过程中的loss和acc曲线图</figcaption></figure><p>模型预测部分的代码（脚本为model_predict.py）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># author: Jclian91</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><span class="hljs-comment"># time: 2020-03-11 13:16</span><br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> albert_zh.extract_feature <span class="hljs-keyword">import</span> BertVector<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict<br><span class="hljs-keyword">from</span> pprint <span class="hljs-keyword">import</span> pprint<br><br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> MAX_SEQ_LEN, event_type<br><br><span class="hljs-comment"># 读取label2id字典</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s_label2id.json&quot;</span> % event_type, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> h:<br>    label_id_dict = json.loads(h.read())<br><br>id_label_dict = &#123;v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> label_id_dict.items()&#125;<br><br><span class="hljs-comment"># 利用ALBERT提取文本特征</span><br>bert_model = BertVector(pooling_strategy=<span class="hljs-string">&quot;NONE&quot;</span>, max_seq_len=MAX_SEQ_LEN)<br>f = <span class="hljs-keyword">lambda</span> text: bert_model.encode([text])[<span class="hljs-string">&quot;encodes&quot;</span>][<span class="hljs-number">0</span>]<br><br><span class="hljs-comment"># 载入模型</span><br>ner_model = load_model(<span class="hljs-string">&quot;%s_ner.h5&quot;</span> % event_type)<br><br><br><span class="hljs-comment"># 从预测的标签列表中获取实体</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_entity</span>(<span class="hljs-params">sent, tags_list</span>):<br><br>    entity_dict = defaultdict(<span class="hljs-built_in">list</span>)<br>    i = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> char, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(sent, tags_list):<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;B-&#x27;</span> <span class="hljs-keyword">in</span> tag:<br>            entity = char<br>            j = i+<span class="hljs-number">1</span><br>            entity_type = tag.split(<span class="hljs-string">&#x27;-&#x27;</span>)[-<span class="hljs-number">1</span>]<br>            <span class="hljs-keyword">while</span> j &lt; <span class="hljs-built_in">min</span>(<span class="hljs-built_in">len</span>(sent), <span class="hljs-built_in">len</span>(tags_list)) <span class="hljs-keyword">and</span> <span class="hljs-string">&#x27;I-%s&#x27;</span> % entity_type <span class="hljs-keyword">in</span> tags_list[j]:<br>                entity += sent[j]<br>                j += <span class="hljs-number">1</span><br><br>            entity_dict[entity_type].append(entity)<br><br>        i += <span class="hljs-number">1</span><br><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">dict</span>(entity_dict)<br><br><br><span class="hljs-comment"># 输入句子，进行预测</span><br><span class="hljs-keyword">while</span> <span class="hljs-number">1</span>:<br>    <span class="hljs-comment"># 输入句子</span><br>    text = <span class="hljs-built_in">input</span>(<span class="hljs-string">&quot;Please enter an sentence: &quot;</span>).replace(<span class="hljs-string">&#x27; &#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>    <span class="hljs-comment"># 利用训练好的模型进行预测</span><br>    train_x = np.array([f(text)])<br>    y = np.argmax(ner_model.predict(train_x), axis=<span class="hljs-number">2</span>)<br>    y = [id_label_dict[_] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> y[<span class="hljs-number">0</span>] <span class="hljs-keyword">if</span> _]<br><br>    <span class="hljs-comment"># 输出预测结果</span><br>    pprint(get_entity(text, y)<br></code></pre></td></tr></table></figure><p>随机在网上找几条新闻测试，结果如下：</p><blockquote><p>Please enter an sentence:昨天进行的女单半决赛中，陈梦4-2击败了队友王曼昱，伊藤美诚则以4-0横扫了中国选手丁宁。{'LOC': ['中国'], 'PER': ['陈梦', '王曼昱', '伊藤美诚', '丁宁']} Pleaseenter an sentence:报道还提到，德国卫生部长延斯·施潘在会上也表示，如果不能率先开发出且使用疫苗，那么60%至70%的人可能会被感染新冠病毒。{'ORG': ['德国卫生部'], 'PER': ['延斯·施潘']} Please enter an sentence:“隔离结束回来，发现公司不见了”，网上的段子，真发生在了昆山达鑫电子有限公司员工身上。{'ORG': ['昆山达鑫电子有限公司']} Please enter an sentence:真人版的《花木兰》由新西兰导演妮基·卡罗执导，由刘亦菲、甄子丹、郑佩佩、巩俐、李连杰等加盟，几乎是全亚洲整容。{'LOC': ['新西兰', '亚洲'], 'PER': ['妮基·卡罗', '刘亦菲', '甄子丹','郑佩佩', '巩俐', '李连杰']}</p></blockquote><p>本项目已经开源，Github网址为：<ahref="https://github.com/percent4/ALBERT_NER_KERAS">https://github.com/percent4/ALBERT_NER_KERAS</a>。</p><p>本文到此结束，感谢大家阅读，欢迎关注笔者的微信公众号：<code>NLP奇幻之旅</code>。</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>NER</tag>
      
      <tag>ALBERT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（二十三）序列标注算法评估模块seqeval的使用</title>
    <link href="/2023/07/09/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B8%89%EF%BC%89%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E7%AE%97%E6%B3%95%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9D%97seqeval%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <url>/2023/07/09/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B8%89%EF%BC%89%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E7%AE%97%E6%B3%95%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9D%97seqeval%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在NLP中，序列标注算法是常见的深度学习模型，但是，对于序列标注算法的评估，我们真的熟悉吗？</p><p>在本文中，笔者将会序列标注算法的模型效果评估方法和<code>seqeval</code>的使用。</p><h3 id="序列标注算法的模型效果评估">序列标注算法的模型效果评估</h3><p>在序列标注算法中，一般我们会形成如下的序列列表，如下：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scheme">[<span class="hljs-symbol">&#x27;O</span>&#x27;, <span class="hljs-symbol">&#x27;O</span>&#x27;, <span class="hljs-symbol">&#x27;B-MISC</span>&#x27;, <span class="hljs-symbol">&#x27;I-MISC</span>&#x27;, <span class="hljs-symbol">&#x27;B-MISC</span>&#x27;, <span class="hljs-symbol">&#x27;I-MISC</span>&#x27;, <span class="hljs-symbol">&#x27;O</span>&#x27;, <span class="hljs-symbol">&#x27;B-PER</span>&#x27;, <span class="hljs-symbol">&#x27;I-PER</span>&#x27;]<br></code></pre></td></tr></table></figure><p>一般序列标注算法的格式有<code>BIO</code>，<code>IOBES</code>，<code>BMES</code>等。其中，<code>实体</code>指的是从B开头标签开始的，同一类型（比如：PER/LOC/ORG）的，非O的连续标签序列。</p><p>常见的序列标注算法的模型效果评估指标有准确率（accuracy）、查准率(percision)、召回率(recall)、F1值等，计算的公式如下：</p><ul><li><p>准确率: accuracy = 预测对的元素个数/总的元素个数</p></li><li><p>查准率：precision = 预测正确的实体个数 /预测的实体总个数</p></li><li><p>召回率：recall = 预测正确的实体个数 / 标注的实体总个数</p></li><li><p>F1值：F1 = 2 <em>准确率 </em> 召回率 / (准确率 + 召回率)</p><p>举个例子，我们有如下的真实序列<code>y_true</code>和预测序列<code>y_pred</code>，如下：</p></li></ul><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">y_true</span> = [<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-MISC&#x27;</span>, <span class="hljs-string">&#x27;I-MISC&#x27;</span>, <span class="hljs-string">&#x27;I-MISC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;I-PER&#x27;</span>]<br><span class="hljs-attr">y_pred</span> = [<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-MISC&#x27;</span>, <span class="hljs-string">&#x27;I-MISC&#x27;</span>, <span class="hljs-string">&#x27;B-MISC&#x27;</span>, <span class="hljs-string">&#x27;I-MISC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;I-PER&#x27;</span>]<br></code></pre></td></tr></table></figure><p>列表中一个有9个元素，其中预测对的元素个数为6个，那么准确率为2/3。标注的实体总个数为2个，预测的实体总个数为3个，预测正确的实体个数为1个，那么precision=1/3,recall=1/2, F1=0.4。</p><h3 id="seqeval的使用">seqeval的使用</h3><p>一般我们的序列标注算法，是用<code>conlleval.pl</code>脚本实现，但这是用perl语言实现的。在Python中，也有相应的序列标注算法的模型效果评估的第三方模块，那就是<code>seqeval</code>，其官网网址为：<ahref="https://pypi.org/project/seqeval/0.0.3/">https://pypi.org/project/seqeval/0.0.3/</a>。</p><p><code>seqeval</code>支持<code>BIO</code>，<code>IOBES</code>标注模式，可用于命名实体识别，词性标注，语义角色标注等任务的评估。</p><p>官网文档中给出了两个例子，笔者修改如下：</p><p>例子1：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">from</span> seqeval.metrics <span class="hljs-keyword">import</span> f1_score<br><span class="hljs-keyword">from</span> seqeval.metrics <span class="hljs-keyword">import</span> precision_score<br><span class="hljs-keyword">from</span> seqeval.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-keyword">from</span> seqeval.metrics <span class="hljs-keyword">import</span> recall_score<br><span class="hljs-keyword">from</span> seqeval.metrics <span class="hljs-keyword">import</span> classification_report<br><br>y_true = [<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-MISC&#x27;</span>, <span class="hljs-string">&#x27;I-MISC&#x27;</span>, <span class="hljs-string">&#x27;I-MISC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;I-PER&#x27;</span>]<br>y_pred = [<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-MISC&#x27;</span>, <span class="hljs-string">&#x27;I-MISC&#x27;</span>, <span class="hljs-string">&#x27;B-MISC&#x27;</span>, <span class="hljs-string">&#x27;I-MISC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;I-PER&#x27;</span>]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;accuary: &quot;</span>, accuracy_score(y_true, y_pred))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;p: &quot;</span>, precision_score(y_true, y_pred))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;r: &quot;</span>, recall_score(y_true, y_pred))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;f1: &quot;</span>, f1_score(y_true, y_pred))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;classification report: &quot;</span>)<br><span class="hljs-built_in">print</span>(classification_report(y_true, y_pred))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">accuary</span>:  <span class="hljs-number">0</span>.<span class="hljs-number">6666666666666666</span><br><span class="hljs-attribute">p</span>:  <span class="hljs-number">0</span>.<span class="hljs-number">3333333333333333</span><br><span class="hljs-attribute">r</span>:  <span class="hljs-number">0</span>.<span class="hljs-number">5</span><br><span class="hljs-attribute">f1</span>:  <span class="hljs-number">0</span>.<span class="hljs-number">4</span><br><span class="hljs-attribute">classification</span> report: <br>           <span class="hljs-attribute">precision</span>    recall  f1-score   support<br><br>     <span class="hljs-attribute">MISC</span>       <span class="hljs-number">0</span>.<span class="hljs-number">00</span>      <span class="hljs-number">0</span>.<span class="hljs-number">00</span>      <span class="hljs-number">0</span>.<span class="hljs-number">00</span>         <span class="hljs-number">1</span><br>      <span class="hljs-attribute">PER</span>       <span class="hljs-number">1</span>.<span class="hljs-number">00</span>      <span class="hljs-number">1</span>.<span class="hljs-number">00</span>      <span class="hljs-number">1</span>.<span class="hljs-number">00</span>         <span class="hljs-number">1</span><br><br><span class="hljs-attribute">micro</span> avg       <span class="hljs-number">0</span>.<span class="hljs-number">33</span>      <span class="hljs-number">0</span>.<span class="hljs-number">50</span>      <span class="hljs-number">0</span>.<span class="hljs-number">40</span>         <span class="hljs-number">2</span><br><span class="hljs-attribute">macro</span> avg       <span class="hljs-number">0</span>.<span class="hljs-number">50</span>      <span class="hljs-number">0</span>.<span class="hljs-number">50</span>      <span class="hljs-number">0</span>.<span class="hljs-number">50</span>         <span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><p>例子2：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">from</span> seqeval.metrics <span class="hljs-keyword">import</span> f1_score<br><span class="hljs-keyword">from</span> seqeval.metrics <span class="hljs-keyword">import</span> precision_score<br><span class="hljs-keyword">from</span> seqeval.metrics <span class="hljs-keyword">import</span> accuracy_score<br><span class="hljs-keyword">from</span> seqeval.metrics <span class="hljs-keyword">import</span> recall_score<br><span class="hljs-keyword">from</span> seqeval.metrics <span class="hljs-keyword">import</span> classification_report<br><br>y_true = [[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-MISC&#x27;</span>, <span class="hljs-string">&#x27;I-MISC&#x27;</span>, <span class="hljs-string">&#x27;I-MISC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>], [<span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;I-PER&#x27;</span>]]<br>y_pred =  [[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-MISC&#x27;</span>, <span class="hljs-string">&#x27;I-MISC&#x27;</span>, <span class="hljs-string">&#x27;B-MISC&#x27;</span>, <span class="hljs-string">&#x27;I-MISC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>], [<span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;I-PER&#x27;</span>]]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;accuary: &quot;</span>, accuracy_score(y_true, y_pred))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;p: &quot;</span>, precision_score(y_true, y_pred))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;r: &quot;</span>, recall_score(y_true, y_pred))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;f1: &quot;</span>, f1_score(y_true, y_pred))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;classification report: &quot;</span>)<br><span class="hljs-built_in">print</span>(classification_report(y_true, y_pred))<br></code></pre></td></tr></table></figure><p>输出结果同上。</p><h3 id="在keras中使用seqeval">在Keras中使用seqeval</h3><p>笔者一年多年写过文章：<ahref="https://percent4.github.io/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%94%EF%BC%89%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/">用深度学习实现命名实体识别（NER）</a>，我们对模型训练部分的代码加以改造，使之在训练过程中能输出F1值。</p><p>在Github上下载项目<code>DL_4_NER</code>，网址为：<ahref="https://github.com/percent4/DL_4_NER">https://github.com/percent4/DL_4_NER</a>。修改utils.py中的文件夹路径，以及模型训练部分的代码（DL_4_NER/Bi_LSTM_Model_training.py）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> BASE_DIR, CONSTANTS, load_data<br><span class="hljs-keyword">from</span> data_processing <span class="hljs-keyword">import</span> data_processing<br><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> np_utils, plot_model<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Bidirectional, LSTM, Dense, Embedding, TimeDistributed<br><br><br><span class="hljs-comment"># 模型输入数据</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">input_data_for_model</span>(<span class="hljs-params">input_shape</span>):<br><br>    <span class="hljs-comment"># 数据导入</span><br>    input_data = load_data()<br>    <span class="hljs-comment"># 数据处理</span><br>    data_processing()<br>    <span class="hljs-comment"># 导入字典</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">1</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        word_dictionary = pickle.load(f)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">2</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        inverse_word_dictionary = pickle.load(f)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">3</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        label_dictionary = pickle.load(f)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">4</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        output_dictionary = pickle.load(f)<br>    vocab_size = <span class="hljs-built_in">len</span>(word_dictionary.keys())<br>    label_size = <span class="hljs-built_in">len</span>(label_dictionary.keys())<br><br>    <span class="hljs-comment"># 处理输入数据</span><br>    aggregate_function = <span class="hljs-keyword">lambda</span> <span class="hljs-built_in">input</span>: [(word, pos, label) <span class="hljs-keyword">for</span> word, pos, label <span class="hljs-keyword">in</span><br>                                            <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;word&#x27;</span>].values.tolist(),<br>                                                <span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;pos&#x27;</span>].values.tolist(),<br>                                                <span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;tag&#x27;</span>].values.tolist())]<br><br>    grouped_input_data = input_data.groupby(<span class="hljs-string">&#x27;sent_no&#x27;</span>).apply(aggregate_function)<br>    sentences = [sentence <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> grouped_input_data]<br><br>    x = [[word_dictionary[word[<span class="hljs-number">0</span>]] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sentences]<br>    x = pad_sequences(maxlen=input_shape, sequences=x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br>    y = [[label_dictionary[word[<span class="hljs-number">2</span>]] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sentences]<br>    y = pad_sequences(maxlen=input_shape, sequences=y, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br>    y = [np_utils.to_categorical(label, num_classes=label_size + <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> y]<br><br>    <span class="hljs-keyword">return</span> x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary<br><br><br><span class="hljs-comment"># 定义深度学习模型：Bi-LSTM</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_Bi_LSTM</span>(<span class="hljs-params">vocab_size, label_size, input_shape, output_dim, n_units, out_act, activation</span>):<br>    model = Sequential()<br>    model.add(Embedding(input_dim=vocab_size + <span class="hljs-number">1</span>, output_dim=output_dim,<br>                        input_length=input_shape, mask_zero=<span class="hljs-literal">True</span>))<br>    model.add(Bidirectional(LSTM(units=n_units, activation=activation,<br>                                 return_sequences=<span class="hljs-literal">True</span>)))<br>    model.add(TimeDistributed(Dense(label_size + <span class="hljs-number">1</span>, activation=out_act)))<br>    model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>, loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-comment"># 模型训练</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_train</span>():<br><br>    <span class="hljs-comment"># 将数据集分为训练集和测试集，占比为9:1</span><br>    input_shape = <span class="hljs-number">60</span><br>    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = input_data_for_model(input_shape)<br>    train_end = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(x)*<span class="hljs-number">0.9</span>)<br>    train_x, train_y = x[<span class="hljs-number">0</span>:train_end], np.array(y[<span class="hljs-number">0</span>:train_end])<br>    test_x, test_y = x[train_end:], np.array(y[train_end:])<br><br>    <span class="hljs-comment"># 模型输入参数</span><br>    activation = <span class="hljs-string">&#x27;selu&#x27;</span><br>    out_act = <span class="hljs-string">&#x27;softmax&#x27;</span><br>    n_units = <span class="hljs-number">100</span><br>    batch_size = <span class="hljs-number">32</span><br>    epochs = <span class="hljs-number">10</span><br>    output_dim = <span class="hljs-number">20</span><br><br>    <span class="hljs-comment"># 模型训练</span><br>    lstm_model = create_Bi_LSTM(vocab_size, label_size, input_shape, output_dim, n_units, out_act, activation)<br>    lstm_model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=epochs, batch_size=batch_size, verbose=<span class="hljs-number">1</span>)<br><br><br>model_train()<br></code></pre></td></tr></table></figure><p>模型训练的结果如下（中间过程省略）：</p><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs gradle">......<br><span class="hljs-number">12598</span><span class="hljs-regexp">/12598 [==============================] - 26s 2ms/</span><span class="hljs-keyword">step</span> - loss: <span class="hljs-number">0.0075</span> - acc: <span class="hljs-number">0.9981</span> - val_loss: <span class="hljs-number">0.2131</span> - val_acc: <span class="hljs-number">0.9592</span><br></code></pre></td></tr></table></figure><p>我们修改代码，在lstm_model.fit那一行修改代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">lables = [<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-MISC&#x27;</span>, <span class="hljs-string">&#x27;I-MISC&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;sO&#x27;</span>]<br>   id2label = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(lables)), lables))<br>   callbacks = [F1Metrics(id2label)]<br>   lstm_model.fit(train_x, train_y, validation_data=(test_x, test_y), epochs=epochs,<br>                  batch_size=batch_size, verbose=<span class="hljs-number">1</span>, callbacks=callbacks)<br></code></pre></td></tr></table></figure><p>此时输出结果为：</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs stata">12598/12598 [==============================] - 26s 2ms/step - loss: 0.0089 - acc: 0.9978 - val_loss: 0.2145 - val_acc: 0.9560<br> - f1: 95.40<br>           precision    recall  f1-<span class="hljs-keyword">score</span>   support<br><br>     MISC     0.9707    0.9833    0.9769     15844<br>      PER     0.9080    0.8194    0.8614      1157<br>      <span class="hljs-keyword">LOC</span>     0.7517    0.8095    0.7795       677<br>      ORG     0.8290    0.7289    0.7757       745<br>       <span class="hljs-keyword">sO</span>     0.7757    0.8300    0.8019       100<br><br>micro avg     0.9524    0.9556    0.9540     18523<br><span class="hljs-keyword">macro</span> avg     0.9520    0.9556    0.9535     18523<br></code></pre></td></tr></table></figure><p>这就是seqeval的强大之处。</p><p>关于seqeval在Keras的使用，有不清楚的地方可以参考该项目的Github网址：<ahref="https://github.com/chakki-works/seqeval">https://github.com/chakki-works/seqeval</a>。</p><h3 id="总结">总结</h3><p>感谢大家的阅读，本次分享到此结束。</p><p>欢迎大家关注我的微信公众号：<code>NLP奇幻之旅</code>。</p><h3 id="参考网址">参考网址</h3><ol type="1"><li>序列标注的准确率和召回率计算: <ahref="https://zhuanlan.zhihu.com/p/56582082">https://zhuanlan.zhihu.com/p/56582082</a></li><li>seqeval官方文档： <ahref="https://pypi.org/project/seqeval/0.0.3/">https://pypi.org/project/seqeval/0.0.3/</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>NLP工具</tag>
      
      <tag>序列标注</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>知识图谱构建举例</title>
    <link href="/2023/07/09/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA%E4%B8%BE%E4%BE%8B/"/>
    <url>/2023/07/09/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA%E4%B8%BE%E4%BE%8B/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>笔者在去年的时候，给出了利用深度学习来构建知识图谱的一次尝试，文章为：<ahref="https://percent4.github.io/2023/07/08/%E5%88%A9%E7%94%A8%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E6%9E%84%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/">利用关系抽取构建知识图谱的一次尝试</a>，本文将会更出更多的例子，也是笔者近一个星期的忙碌结果。下面为知识图谱构建的例子，由笔者原创，是从新闻或者小说中直接抽取而来，加上大量时间的人工整理而得到，下面的图片是从Neo4J导出并截图。例子1：《平凡的世界》实体关系图（局部）： <img src="/img/kg2_1.png"alt="《平凡的世界》实体关系图（局部）" />例子2：《白鹿原》实体关系图（局部）： <img src="/img/kg2_2.png"alt="《白鹿原》实体关系图（局部）" />例子3：政治新闻实体关系图（局部）： <img src="/img/kg2_3.png"alt="政治新闻实体关系图（局部）" />例子4：《神雕侠侣》实体关系图（局部）： <img src="/img/kg2_4.png"alt="《神雕侠侣》实体关系图（局部）" />例子5：《明朝那些事儿》实体关系图（局部）： <img src="/img/kg2_5.png"alt="《明朝那些事儿》实体关系图（局部）" />例子6：《曾国藩》实体关系图（局部）： <img src="/img/kg2_6.png"alt="《曾国藩》实体关系图（局部）" /></p><p>以上展示的图以及数据放在Github上，网址为：<ahref="https://github.com/percent4/knowledge_graph_demo">https://github.com/percent4/knowledge_graph_demo</a>。关于这方面的技术和数据将会在不久后公开，代码和数据已经放在Github上，网址为：<ahref="https://github.com/percent4/spo_extract_platform">https://github.com/percent4/spo_extract_platform</a>，笔者将会另写文章来介绍。</p><p>感觉大家的阅读，笔者将会在不久之后公开该技术的源代码和数据，敬请期待~</p><blockquote><p>欢迎大家关注我的微信公众号：<code>NLP奇幻之旅</code> 。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>知识图谱</category>
      
    </categories>
    
    
    <tags>
      
      <tag>知识图谱</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（二十二）利用ALBERT实现文本二分类</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%E4%BA%8C%EF%BC%89%E5%88%A9%E7%94%A8ALBERT%E5%AE%9E%E7%8E%B0%E6%96%87%E6%9C%AC%E4%BA%8C%E5%88%86%E7%B1%BB/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%E4%BA%8C%EF%BC%89%E5%88%A9%E7%94%A8ALBERT%E5%AE%9E%E7%8E%B0%E6%96%87%E6%9C%AC%E4%BA%8C%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在文章<ahref="https://percent4.github.io/2023/07/08/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%EF%BC%89%E5%88%A9%E7%94%A8BERT%E5%AE%9E%E7%8E%B0%E6%96%87%E6%9C%AC%E4%BA%8C%E5%88%86%E7%B1%BB/">NLP（二十）利用BERT实现文本二分类</a>中，笔者介绍了如何使用BERT来实现文本二分类功能，以判别是否属于出访类事件为例子。但是呢，利用BERT在做模型预测的时候存在预测时间较长的问题。因此，我们考虑用新出来的预训练模型来加快模型预测速度。</p><p>本文将介绍如何利用ALBERT来实现文本二分类。</p><h3 id="关于albert">关于ALBERT</h3><p>ALBERT的提出时间大约是在2019年10月，其第一作者为谷歌科学家蓝振忠博士。ALBERT的论文地址为：<ahref="https://openreview.net/pdf?id=H1eA7AEtvS">https://openreview.net/pdf?id=H1eA7AEtvS</a>, Github项目地址为： <ahref="https://github.com/brightmart/albert_zh">https://github.com/brightmart/albert_zh</a>。简单说来，ALBERT是BERT的一个精简版，它在BERT模型的基础上进行改造，减少了大量参数，使得其在模型训练和模型预测的速度上有很大提升，而模型的效果只会有微小幅度的下降，具体的效果和速度方面的说明可以参考Github项目。ALBERT相对于BERT的改进如下：</p><ul><li><p>对Embedding因式分解（Factorized embeddingparameterization）；</p></li><li><p>跨层的参数共享（Cross-layer parameter sharing）；</p></li><li><p>句间连贯（Inter-sentence coherence loss）；</p></li><li><p>移除dropout 。</p><p>笔者在北京的时候也写过ALBERT在提升序列标注算法的预测速度方面的一篇文章：<ahref="https://percent4.github.io/2023/07/08/NLP%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%E5%88%A9%E7%94%A8ALBERT%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E9%80%9F%E5%BA%A6%E7%9A%84%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/">NLP（十八）利用ALBERT提升模型预测速度的一次尝试</a>，该项目的Github地址为：<ahref="https://github.com/percent4/ALBERT_4_Time_Recognition">https://github.com/percent4/ALBERT_4_Time_Recognition</a>。</p></li></ul><h3 id="项目说明">项目说明</h3><p>本项目的数据和代码主要参考笔者的文章<ahref="https://percent4.github.io/2023/07/08/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%EF%BC%89%E5%88%A9%E7%94%A8BERT%E5%AE%9E%E7%8E%B0%E6%96%87%E6%9C%AC%E4%BA%8C%E5%88%86%E7%B1%BB/">NLP（二十）利用BERT实现文本二分类</a>，该项目是想判别输入的句子是否属于政治上的出访类事件。笔者一共收集了340条数据，其中280条用作训练集，60条用作测试集。</p><p>项目结构如下图：</p><p><a href="/img/nlp22_1.png">项目结构</a></p><p>在这里我们使用ALBERT已经训练好的文件<code>albert_tiny</code>，借鉴BERT的调用方法，我们在这里给出<code>albert_zh</code>模块，能够让ALBERT提取文本的特征，具体代码不在这里给出，有兴趣的读者可以访问该项目的Github地址：<ahref="https://github.com/percent4/ALBERT_text_classification">https://github.com/percent4/ALBERT_text_classification</a>。</p><p>注意，<code>albert_tiny</code>给出的向量维度为312，我们的模型训练代码（model_train.py）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># author: Jclian91</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><span class="hljs-comment"># time: 2020-03-04 13:37</span><br><br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> load_data <span class="hljs-keyword">import</span> train_df, test_df<br><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> to_categorical<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Model<br><span class="hljs-keyword">from</span> keras.optimizers <span class="hljs-keyword">import</span> Adam<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Input, BatchNormalization, Dense<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">from</span> albert_zh.extract_feature <span class="hljs-keyword">import</span> BertVector<br><br><span class="hljs-comment"># 读取文件并进行转换</span><br>bert_model = BertVector(pooling_strategy=<span class="hljs-string">&quot;REDUCE_MEAN&quot;</span>, max_seq_len=<span class="hljs-number">100</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;begin encoding&#x27;</span>)<br>f = <span class="hljs-keyword">lambda</span> text: bert_model.encode([text])[<span class="hljs-string">&quot;encodes&quot;</span>][<span class="hljs-number">0</span>]<br>train_df[<span class="hljs-string">&#x27;x&#x27;</span>] = train_df[<span class="hljs-string">&#x27;text&#x27;</span>].apply(f)<br>test_df[<span class="hljs-string">&#x27;x&#x27;</span>] = test_df[<span class="hljs-string">&#x27;text&#x27;</span>].apply(f)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;end encoding&#x27;</span>)<br><br>x_train = np.array([vec <span class="hljs-keyword">for</span> vec <span class="hljs-keyword">in</span> train_df[<span class="hljs-string">&#x27;x&#x27;</span>]])<br>x_test = np.array([vec <span class="hljs-keyword">for</span> vec <span class="hljs-keyword">in</span> test_df[<span class="hljs-string">&#x27;x&#x27;</span>]])<br>y_train = np.array([vec <span class="hljs-keyword">for</span> vec <span class="hljs-keyword">in</span> train_df[<span class="hljs-string">&#x27;label&#x27;</span>]])<br>y_test = np.array([vec <span class="hljs-keyword">for</span> vec <span class="hljs-keyword">in</span> test_df[<span class="hljs-string">&#x27;label&#x27;</span>]])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;x_train: &#x27;</span>, x_train.shape)<br><br><span class="hljs-comment"># Convert class vectors to binary class matrices.</span><br>num_classes = <span class="hljs-number">2</span><br>y_train = to_categorical(y_train, num_classes)<br>y_test = to_categorical(y_test, num_classes)<br><br><span class="hljs-comment"># 创建模型</span><br>x_in = Input(shape=(<span class="hljs-number">312</span>, ))<br>x_out = Dense(<span class="hljs-number">32</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)(x_in)<br>x_out = BatchNormalization()(x_out)<br>x_out = Dense(num_classes, activation=<span class="hljs-string">&quot;softmax&quot;</span>)(x_out)<br>model = Model(inputs=x_in, outputs=x_out)<br><span class="hljs-built_in">print</span>(model.summary())<br><br>model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>              optimizer=Adam(),<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br><span class="hljs-comment"># 模型训练以及评估</span><br>history = model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=<span class="hljs-number">8</span>, epochs=<span class="hljs-number">20</span>)<br>model.save(<span class="hljs-string">&#x27;visit_classify.h5&#x27;</span>)<br><span class="hljs-built_in">print</span>(model.evaluate(x_test, y_test))<br><br><span class="hljs-comment"># 绘制loss和acc图像</span><br>plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>epochs = <span class="hljs-built_in">len</span>(history.history[<span class="hljs-string">&#x27;loss&#x27;</span>])<br>plt.plot(<span class="hljs-built_in">range</span>(epochs), history.history[<span class="hljs-string">&#x27;loss&#x27;</span>], label=<span class="hljs-string">&#x27;loss&#x27;</span>)<br>plt.plot(<span class="hljs-built_in">range</span>(epochs), history.history[<span class="hljs-string">&#x27;val_loss&#x27;</span>], label=<span class="hljs-string">&#x27;val_loss&#x27;</span>)<br>plt.legend()<br><br>plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>epochs = <span class="hljs-built_in">len</span>(history.history[<span class="hljs-string">&#x27;acc&#x27;</span>])<br>plt.plot(<span class="hljs-built_in">range</span>(epochs), history.history[<span class="hljs-string">&#x27;acc&#x27;</span>], label=<span class="hljs-string">&#x27;acc&#x27;</span>)<br>plt.plot(<span class="hljs-built_in">range</span>(epochs), history.history[<span class="hljs-string">&#x27;val_acc&#x27;</span>], label=<span class="hljs-string">&#x27;val_acc&#x27;</span>)<br>plt.legend()<br>plt.savefig(<span class="hljs-string">&quot;loss_acc.png&quot;</span>)<br></code></pre></td></tr></table></figure><p>模型训练的效果很不错，在训练集的acc为0.9857,在测试集上的acc为0.9500，具体如下：</p><figure><img src="/img/nlp22_2.png" alt="训练过程中的loss和acc图" /><figcaption aria-hidden="true">训练过程中的loss和acc图</figcaption></figure><h3 id="与bert的预测对比">与BERT的预测对比</h3><p>接下来我们在模型预测上的时间，与BERT的文本二分类模型预测时间做一个对比，这样有助于提升我们对ALBERT的印象。</p><p>BERT的文本二分类模型预测可以参考文章<ahref="https://percent4.github.io/2023/07/08/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%EF%BC%89%E5%88%A9%E7%94%A8BERT%E5%AE%9E%E7%8E%B0%E6%96%87%E6%9C%AC%E4%BA%8C%E5%88%86%E7%B1%BB/">NLP（二十）利用BERT实现文本二分类</a>，本文给出的代码与BERT实现的模型预测代码基本一致，只不过BERT提取特征改成ALBERT提取特征。</p><p>本文的模型预测代码（model_predict.py）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># author: Jclian91</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><span class="hljs-comment"># time: 2020-03-04 17:33</span><br><br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> albert_zh.extract_feature <span class="hljs-keyword">import</span> BertVector<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<br>load_model = load_model(<span class="hljs-string">&quot;visit_classify.h5&quot;</span>)<br><br><span class="hljs-comment"># 预测语句</span><br>texts = [<span class="hljs-string">&#x27;在访问限制中，用户可以选择禁用iPhone的功能，包括Siri、iTunes购买功能、安装/删除应用等，甚至还可以让iPhone变成一台功能手机。以下是访问限制具体可以实现的一些功能&#x27;</span>,<br>         <span class="hljs-string">&#x27;IT之家4月23日消息 近日，谷歌在其官方论坛发布消息表示，他们为Android Auto添加了一项新功能：可以访问完整联系人列表。用户现在可以通过在Auto的电话拨号界面中打开左上角的菜单访问完整的联系人列表。值得注意的是，这一功能仅支持在车辆停止时使用。&#x27;</span>,<br>         <span class="hljs-string">&#x27;要通过telnet 访问路由器，需要先通过console 口对路由器进行基本配置，例如：IP地址、密码等。&#x27;</span>,<br>         <span class="hljs-string">&#x27;IT之家3月26日消息 近日反盗版的国际咨询公司MUSO发布了2017年的年度报告，其中的数据显示，去年盗版资源网站访问量达到了3000亿次，比前一年（2016年）提高了1.6%。美国是访问盗版站点次数最多的国家，共有279亿次访问；其后分别是俄罗斯、印度和巴西，中国位列第18。&#x27;</span>,<br>         <span class="hljs-string">&#x27;目前A站已经恢复了访问，可以直接登录，网页加载正常，视频已经可以正常播放。&#x27;</span>,<br>         <span class="hljs-string">&#x27;Win7电脑提示无线适配器或访问点有问题怎么办?很多用户在使用无线网连接上网时，发现无线网显示已连接，但旁边却出现了一个黄色感叹号，无法进行网络操作，通过诊断提示电脑无线适配器或访问点有问题，且处于未修复状态，这该怎么办呢?下面小编就和大家分享下Win7电脑提示无线适配器或访问点有问题的解决方法。&#x27;</span>,<br>         <span class="hljs-string">&#x27;未开发所有安全组之前访问，FTP可以链接上，但是打开会很慢，需要1-2分钟才能链接上&#x27;</span>,<br>         <span class="hljs-string">&#x27;win7系统电脑的用户，在连接WIFI网络网上时，有时候会遇到突然上不了网，查看连接的WIFI出现“有限的访问权限”的文字提示。&#x27;</span>,<br>         <span class="hljs-string">&#x27;2月28日，唐山曹妃甸蓝色海洋科技有限公司董事长赵力军等一行5人到黄海水产研究所交流访问。黄海水产研究所副所长辛福言及相关部门负责人、专家等参加了会议。&#x27;</span>,<br>         <span class="hljs-string">&#x27;与标准Mozy一样，Stash文件夹为用户提供了对其备份文件的基于云的访问，但是它们还使他们可以随时，跨多个设备(包括所有计算机，智能手机和平板电脑)访问它们。换句话说，使用浏览器的任何人都可以同时查看文件(如果需要)。操作系统和设备品牌无关。&#x27;</span>,<br>         <span class="hljs-string">&#x27;研究表明，每个网页的平均预期寿命为44至100天。当用户通过浏览器访问已消失的网页时，就会看到「Page Not Found」的错误信息。对于这种情况，相信大多数人也只能不了了之。不过有责任心的组织——互联网档案馆为了提供更可靠的Web服务，它联手Brave浏览器专门针对此类网页提供了一键加载存档页面的功能。&#x27;</span>,<br>         <span class="hljs-string">&#x27;3日，根据三星电子的消息，李在镕副会长这天访问了位于韩国庆尚北道龟尾市的三星电子工厂。&#x27;</span>] * <span class="hljs-number">10</span><br><br>labels = []<br><br>bert_model = BertVector(pooling_strategy=<span class="hljs-string">&quot;REDUCE_MEAN&quot;</span>, max_seq_len=<span class="hljs-number">100</span>)<br><br>init_time = time.time()<br><br><span class="hljs-comment"># 对上述句子进行预测</span><br><span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br><br>    <span class="hljs-comment"># 将句子转换成向量</span><br>    vec = bert_model.encode([text])[<span class="hljs-string">&quot;encodes&quot;</span>][<span class="hljs-number">0</span>]<br>    x_train = np.array([vec])<br><br>    <span class="hljs-comment"># 模型预测</span><br>    predicted = load_model.predict(x_train)<br>    y = np.argmax(predicted[<span class="hljs-number">0</span>])<br>    label = <span class="hljs-string">&#x27;Y&#x27;</span> <span class="hljs-keyword">if</span> y <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;N&#x27;</span><br>    labels.append(label)<br><br>cost_time = time.time() - init_time<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Average cost time: %s.&quot;</span> % (cost_time/<span class="hljs-built_in">len</span>(texts)))<br><br><span class="hljs-keyword">for</span> text, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(texts, labels):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;%s\t%s&#x27;</span> % (label, text))<br><br>df = pd.DataFrame(&#123;<span class="hljs-string">&#x27;句子&#x27;</span>:texts, <span class="hljs-string">&quot;是否属于出访类事件&quot;</span>: labels&#125;)<br>df.to_excel(<span class="hljs-string">&#x27;./result.xlsx&#x27;</span>, index=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>输出的平均预测时长为：<code>16.98ms</code>，而BERT版的平均预测时间为：<code>257.31ms</code>。</p><p>我们将模型预测写成HTTP服务，代码（server.py）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># author: Jclian91</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><span class="hljs-comment"># time: 2020-03-04 20:13</span><br><br><span class="hljs-keyword">import</span> tornado.httpserver<br><span class="hljs-keyword">import</span> tornado.ioloop<br><span class="hljs-keyword">import</span> tornado.options<br><span class="hljs-keyword">import</span> tornado.web<br><span class="hljs-keyword">from</span> tornado.options <span class="hljs-keyword">import</span> define, options<br><br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> albert_zh.extract_feature <span class="hljs-keyword">import</span> BertVector<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<br><br><br><span class="hljs-comment"># 定义端口为10008</span><br>define(<span class="hljs-string">&quot;port&quot;</span>, default=<span class="hljs-number">10008</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;run on the given port&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br><br><span class="hljs-comment"># 加载ALBERT</span><br>bert_model = BertVector(pooling_strategy=<span class="hljs-string">&quot;REDUCE_MEAN&quot;</span>, max_seq_len=<span class="hljs-number">100</span>)<br><span class="hljs-comment"># 加载已经训练好的模型</span><br>load_model = load_model(<span class="hljs-string">&quot;visit_classify.h5&quot;</span>)<br><br><br><span class="hljs-comment"># 对句子进行预测</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PredictHandler</span>(tornado.web.RequestHandler):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">post</span>(<span class="hljs-params">self</span>):<br><br>        text = self.get_argument(<span class="hljs-string">&quot;text&quot;</span>)<br><br>        <span class="hljs-comment"># 将句子转换成向量</span><br>        vec = bert_model.encode([text])[<span class="hljs-string">&quot;encodes&quot;</span>][<span class="hljs-number">0</span>]<br>        x_train = np.array([vec])<br><br>        <span class="hljs-comment"># 模型预测</span><br>        predicted = load_model.predict(x_train)<br>        y = np.argmax(predicted[<span class="hljs-number">0</span>])<br>        label = <span class="hljs-string">&#x27;是&#x27;</span> <span class="hljs-keyword">if</span> y <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;否&quot;</span><br><br>        <span class="hljs-comment"># 返回结果</span><br>        result = &#123;<span class="hljs-string">&quot;原文&quot;</span>: text, <span class="hljs-string">&quot;是否属于出访类事件？&quot;</span>: label&#125;<br><br>        self.write(json.dumps(result, ensure_ascii=<span class="hljs-literal">False</span>, indent=<span class="hljs-number">2</span>))<br><br><br><span class="hljs-comment"># 主函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br><br>    <span class="hljs-comment"># 开启tornado服务</span><br>    tornado.options.parse_command_line()<br>    <span class="hljs-comment"># 定义app</span><br>    app = tornado.web.Application(<br>            handlers=[(<span class="hljs-string">r&#x27;/predict&#x27;</span>, PredictHandler)] <span class="hljs-comment">#网页路径控制</span><br>           )<br>    http_server = tornado.httpserver.HTTPServer(app)<br>    http_server.listen(options.port)<br>    tornado.ioloop.IOLoop.instance().start()<br><br><br>main()<br></code></pre></td></tr></table></figure><p>用Postman进行测试，如下图：</p><p><img src="/img/nlp22_3.png" /></p><p>实践证明，用ALBERT做文本特征提取，模型训练的效果基本与BERT差别微小，模型训练速度明显提升，更重要的是，模型预测的速度只有BERT版本的6.6%（不同情况下可能有略微差异），这在生产上是十分有帮助的。</p><h3 id="参考网址">参考网址</h3><ol type="1"><li>中文预训练ALBERT模型来了：小模型登顶GLUE，Base版模型小10倍速度快1倍：<ahref="https://zhuanlan.zhihu.com/p/85037097">https://zhuanlan.zhihu.com/p/85037097</a></li><li>ALBERT一作蓝振忠：预训练模型应用已成熟，ChineseGLUE要对标GLUE基准：<ahref="https://tech.sina.com.cn/roll/2019-11-17/doc-iihnzhfy9804802.shtml">https://tech.sina.com.cn/roll/2019-11-17/doc-iihnzhfy9804802.shtml</a>。</li><li>解读ALBERT：<ahref="https://blog.csdn.net/weixin_37947156/article/details/101529943">https://blog.csdn.net/weixin_37947156/article/details/101529943</a>。</li><li>ALBERT的Github项目地址：<ahref="https://github.com/brightmart/albert_zh">https://github.com/brightmart/albert_zh</a>。</li></ol>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>ALBERT</tag>
      
      <tag>文本分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（二十一）人物关系抽取的一次实战</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B8%80%EF%BC%89%E4%BA%BA%E7%89%A9%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E7%9A%84%E4%B8%80%E6%AC%A1%E5%AE%9E%E6%88%98/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%E4%B8%80%EF%BC%89%E4%BA%BA%E7%89%A9%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E7%9A%84%E4%B8%80%E6%AC%A1%E5%AE%9E%E6%88%98/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>去年，笔者写过一篇文章<ahref="https://percent4.github.io/2023/07/08/%E5%88%A9%E7%94%A8%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E6%9E%84%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/">利用关系抽取构建知识图谱的一次尝试</a>，试图用现在的深度学习办法去做开放领域的关系抽取，但是遗憾的是，目前在开放领域的关系抽取，还没有成熟的解决方案和模型。当时的文章仅作为笔者的一次尝试，在实际使用过程中，效果有限。</p><p>本文将讲述如何利用深度学习模型来进行人物关系抽取。人物关系抽取可以理解为是关系抽取，这是我们构建知识图谱的重要一步。本文人物关系抽取的主要思想是关系抽取的pipeline（管道）模式，因为人名可以使用现成的NER模型提取，因此本文仅解决从文章中抽取出人名后，如何进行人物关系抽取。</p><p>本文采用的深度学习模型是文本分类模型，结合BERT预训练模型，取得了较为不错的效果。</p><p>本项目已经开源，Github地址为：<ahref="https://github.com/percent4/people_relation_extract">https://github.com/percent4/people_relation_extract</a>。</p><p>本项目的项目结构图如下：</p><figure><img src="/img/nlp21_1.png" alt="人物关系抽取项目结构" /><figcaption aria-hidden="true">人物关系抽取项目结构</figcaption></figure><h3 id="数据集介绍">数据集介绍</h3><p>在进行这方面的尝试之前，我们还不得不面对这样一个难题，那就是中文人物关系抽取语料的缺失。数据是模型的前提，没有数据，一切模型无从谈起。因此，笔者不得不花费大量的时间收集数据。</p><p>笔者利用大量自己业余的时间，收集了大约2900条人物关系样本，整理成Excel（文件名称为<code>人物关系表.xlsx</code>），其中几行如下：</p><figure><img src="/img/nlp21_2.png" alt="笔者自己收集的数据集" /><figcaption aria-hidden="true">笔者自己收集的数据集</figcaption></figure><p>人物关系一共有14类，分别为<code>unknown</code>,<code>夫妻</code>,<code>父母</code>,<code>兄弟姐妹</code>,<code>上下级</code>,<code>师生</code>,<code>好友</code>,<code>同学</code>,<code>合作</code>,<code>同人</code>,<code>情侣</code>,<code>祖孙</code>,<code>同门</code>,<code>亲戚</code>，其中<code>unknown</code>类别表示该人物关系不在其余的13类中（人物之间没有关系或者为其他关系），<code>同人</code>关系指的是两个人物其实是同一个人，比如下面的例子：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dns">邵逸夫(<span class="hljs-number">1907年10月4</span>日—<span class="hljs-number">2014年1月7</span>日)，原名邵仁楞，生于浙江省宁波市镇海镇，祖籍浙江宁波。<br></code></pre></td></tr></table></figure><p>上面的例子中，邵逸夫和邵仁楞就是同一个人。<code>亲戚</code>关系指的是除了<code>夫妻</code>,<code>父母</code>,<code>兄弟姐妹</code>,<code>祖孙</code>之外的亲戚关系，比如叔侄，舅甥关系等。</p><p>为了对该数据集的每个关系类别的数量进行统计，我们可以使用脚本<code>data/relation_bar_chart.py</code>，完整的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># 绘制人物关系频数统计条形图</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 读取EXCEL数据</span><br>df = pd.read_excel(<span class="hljs-string">&#x27;人物关系表.xlsx&#x27;</span>)<br>label_list = <span class="hljs-built_in">list</span>(df[<span class="hljs-string">&#x27;关系&#x27;</span>].value_counts().index)<br>num_list= df[<span class="hljs-string">&#x27;关系&#x27;</span>].value_counts().tolist()<br><br><span class="hljs-comment"># Mac系统设置中文字体支持</span><br>plt.rcParams[<span class="hljs-string">&quot;font.family&quot;</span>] = <span class="hljs-string">&#x27;Arial Unicode MS&#x27;</span><br><br><span class="hljs-comment"># 利用Matplotlib绘制条形图</span><br>x = <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(num_list))<br>rects = plt.bar(left=x, height=num_list, width=<span class="hljs-number">0.6</span>, color=<span class="hljs-string">&#x27;blue&#x27;</span>, label=<span class="hljs-string">&quot;频数&quot;</span>)<br>plt.ylim(<span class="hljs-number">0</span>, <span class="hljs-number">500</span>) <span class="hljs-comment"># y轴范围</span><br>plt.ylabel(<span class="hljs-string">&quot;数量&quot;</span>)<br>plt.xticks([index + <span class="hljs-number">0.1</span> <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> x], label_list)<br>plt.xticks(rotation=<span class="hljs-number">45</span>) <span class="hljs-comment"># x轴的标签旋转45度</span><br>plt.xlabel(<span class="hljs-string">&quot;人物关系&quot;</span>)<br>plt.title(<span class="hljs-string">&quot;人物关系频数统计&quot;</span>)<br>plt.legend()<br><br><span class="hljs-comment"># 条形图的文字说明</span><br><span class="hljs-keyword">for</span> rect <span class="hljs-keyword">in</span> rects:<br>    height = rect.get_height()<br>    plt.text(rect.get_x() + rect.get_width() / <span class="hljs-number">2</span>, height+<span class="hljs-number">1</span>, <span class="hljs-built_in">str</span>(height), ha=<span class="hljs-string">&quot;center&quot;</span>, va=<span class="hljs-string">&quot;bottom&quot;</span>)<br><br>plt.show()<br></code></pre></td></tr></table></figure><p>运行后的结果如下：</p><figure><img src="/img/nlp21_3.png" alt="人物关系条形图" /><figcaption aria-hidden="true">人物关系条形图</figcaption></figure><p><code>unknown</code>类别最多，有791条，其余的如<code>祖孙</code>,<code>亲戚</code>,<code>情侣</code>等较少，只有90多条，这是因为这类人物关系的数据缺失不好收集。因此，语料的收集费时费力，需要消耗大量的精力。</p><h3 id="数据预处理">数据预处理</h3><p>收集好数据后，我们需要对数据进行预处理，预处理主要分两步，一步是将人物关系和原文本整合在一起，第二步简单，将数据集划分为训练集和测试集，比例为8:2。</p><p>我们对第一步进行详细说明，将人物关系和原文本整合在一起。一般我们给定原文本和该文本中的两个人物，比如：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dns">邵逸夫(<span class="hljs-number">1907年10月4</span>日—<span class="hljs-number">2014年1月7</span>日)，原名邵仁楞，生于浙江省宁波市镇海镇，祖籍浙江宁波。<br></code></pre></td></tr></table></figure><p>这句话中有两个人物：邵逸夫，邵仁楞，这个容易在语料中找到。然后我们将原文本的这两个人物中的每个字符分别用'#'号代码，并通过'$'符号拼接在一起，形成的整合文本如下：</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clean">邵逸夫$邵仁楞$###(<span class="hljs-number">1907</span>年<span class="hljs-number">10</span>月<span class="hljs-number">4</span>日—<span class="hljs-number">2014</span>年<span class="hljs-number">1</span>月<span class="hljs-number">7</span>日)，原名###，生于浙江省宁波市镇海镇，祖籍浙江宁波。<br></code></pre></td></tr></table></figure><p>处理成这种格式是为了方便文本分类模型进行调用。</p><p>数据预处理的脚本为<code>data/data_into_train_test.py</code>，完整的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> pprint <span class="hljs-keyword">import</span> pprint<br><br>df = pd.read_excel(<span class="hljs-string">&#x27;人物关系表.xlsx&#x27;</span>)<br>relations = <span class="hljs-built_in">list</span>(df[<span class="hljs-string">&#x27;关系&#x27;</span>].unique())<br>relations.remove(<span class="hljs-string">&#x27;unknown&#x27;</span>)<br>relation_dict = &#123;<span class="hljs-string">&#x27;unknown&#x27;</span>: <span class="hljs-number">0</span>&#125;<br>relation_dict.update(<span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(relations, <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(relations)+<span class="hljs-number">1</span>))))<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;rel_dict.json&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> h:<br>    h.write(json.dumps(relation_dict, ensure_ascii=<span class="hljs-literal">False</span>, indent=<span class="hljs-number">2</span>))<br><br>pprint(df[<span class="hljs-string">&#x27;关系&#x27;</span>].value_counts())<br>df[<span class="hljs-string">&#x27;rel&#x27;</span>] = df[<span class="hljs-string">&#x27;关系&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: relation_dict[x])<br><br>texts = []<br><span class="hljs-keyword">for</span> per1, per2, text <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(df[<span class="hljs-string">&#x27;人物1&#x27;</span>].tolist(), df[<span class="hljs-string">&#x27;人物2&#x27;</span>].tolist(), df[<span class="hljs-string">&#x27;文本&#x27;</span>].tolist()):<br>    text = <span class="hljs-string">&#x27;$&#x27;</span>.join([per1, per2, text.replace(per1, <span class="hljs-built_in">len</span>(per1)*<span class="hljs-string">&#x27;#&#x27;</span>).replace(per2, <span class="hljs-built_in">len</span>(per2)*<span class="hljs-string">&#x27;#&#x27;</span>)])<br>    texts.append(text)<br><br>df[<span class="hljs-string">&#x27;text&#x27;</span>] = texts<br><br>train_df = df.sample(frac=<span class="hljs-number">0.8</span>, random_state=<span class="hljs-number">1024</span>)<br>test_df = df.drop(train_df.index)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;train.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    <span class="hljs-keyword">for</span> text, rel <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(train_df[<span class="hljs-string">&#x27;text&#x27;</span>].tolist(), train_df[<span class="hljs-string">&#x27;rel&#x27;</span>].tolist()):<br>        f.write(<span class="hljs-built_in">str</span>(rel)+<span class="hljs-string">&#x27; &#x27;</span>+text+<span class="hljs-string">&#x27;\n&#x27;</span>)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;test.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> g:<br>    <span class="hljs-keyword">for</span> text, rel <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(test_df[<span class="hljs-string">&#x27;text&#x27;</span>].tolist(), test_df[<span class="hljs-string">&#x27;rel&#x27;</span>].tolist()):<br>        g.write(<span class="hljs-built_in">str</span>(rel)+<span class="hljs-string">&#x27; &#x27;</span>+text+<span class="hljs-string">&#x27;\n&#x27;</span>)<br></code></pre></td></tr></table></figure><p>运行完该脚本后，会在<code>data</code>目录下生成train.txt,test.txt和rel_dict.json，该json文件中保存的信息如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;unknown&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;夫妻&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;父母&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">2</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;兄弟姐妹&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">3</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;上下级&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">4</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;师生&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">5</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;好友&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">6</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;同学&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">7</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;合作&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">8</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;同人&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">9</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;情侣&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">10</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;祖孙&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">11</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;同门&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">12</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;亲戚&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">13</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>简单来说，是给每种关系一个id，转化成类别型变量。</p><p>以train.txt为例，其前5行的内容如下：</p><figure class="highlight basic"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs basic"><span class="hljs-symbol">4 </span>方琳$李伟康$在生活中，###则把##看作小辈，常常替她解决难题。<br><span class="hljs-symbol">3 </span>佳子$久仁$<span class="hljs-number">12</span>月，##和弟弟##参加了在东京举行的全国初中生演讲比赛。<br><span class="hljs-symbol">2 </span>钱慧安$钱禄新$###，生卒年不详，海上画家###之子。<br><span class="hljs-symbol">0 </span>吴继坤$邓新生$###还曾对媒体说：“我这个小小的投资商，经常得到###等领导的亲自关注和关照，我觉到受宠若惊。”<br><span class="hljs-symbol">2 </span>洪博培$乔恩·M·亨茨曼$###的父亲########是著名企业家、美国最大化学公司亨茨曼公司创始人。<br><span class="hljs-symbol">10 </span>夏乐$陈飞$两小无猜剧情简介:##和##是一对从小一起长大的青梅竹马。<br></code></pre></td></tr></table></figure><p>在每一行中，空格之前的数字所对应的人物关系可以在<code>rel_dict.json</code>中找到。</p><h3 id="模型训练">模型训练</h3><p>在模型训练前，为了将数据的格式更好地适应模型，需要再对trian.txt和test.txt进行处理。处理脚本为<code>load_data.py</code>，完整的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><br><span class="hljs-comment"># 读取txt文件</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_txt_file</span>(<span class="hljs-params">file_path</span>):<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        content = [_.strip() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> f.readlines()]<br><br>    labels, texts = [], []<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> content:<br>        parts = line.split()<br>        label, text = parts[<span class="hljs-number">0</span>], <span class="hljs-string">&#x27;&#x27;</span>.join(parts[<span class="hljs-number">1</span>:])<br>        labels.append(label)<br>        texts.append(text)<br><br>    <span class="hljs-keyword">return</span> labels, texts<br><br><span class="hljs-comment"># 获取训练数据和测试数据，格式为pandas的DataFrame</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_train_test_pd</span>():<br>    file_path = <span class="hljs-string">&#x27;data/train.txt&#x27;</span><br>    labels, texts = read_txt_file(file_path)<br>    train_df = pd.DataFrame(&#123;<span class="hljs-string">&#x27;label&#x27;</span>: labels, <span class="hljs-string">&#x27;text&#x27;</span>: texts&#125;)<br><br>    file_path = <span class="hljs-string">&#x27;data/test.txt&#x27;</span><br>    labels, texts = read_txt_file(file_path)<br>    test_df = pd.DataFrame(&#123;<span class="hljs-string">&#x27;label&#x27;</span>: labels, <span class="hljs-string">&#x27;text&#x27;</span>: texts&#125;)<br><br>    <span class="hljs-keyword">return</span> train_df, test_df<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br><br>    train_df, test_df = get_train_test_pd()<br>    <span class="hljs-built_in">print</span>(train_df.head())<br>    <span class="hljs-built_in">print</span>(test_df.head())<br><br>    train_df[<span class="hljs-string">&#x27;text_len&#x27;</span>] = train_df[<span class="hljs-string">&#x27;text&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x))<br>    <span class="hljs-built_in">print</span>(train_df.describe())<br></code></pre></td></tr></table></figure><p>本项目所采用的模型为：BERT + 双向GRU + Attention +FC，其中BERT用来提取文本的特征，关于这一部分的介绍，已经在文章<ahref="https://percent4.github.io/2023/07/08/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%EF%BC%89%E5%88%A9%E7%94%A8BERT%E5%AE%9E%E7%8E%B0%E6%96%87%E6%9C%AC%E4%BA%8C%E5%88%86%E7%B1%BB/">NLP（二十）利用BERT实现文本二分类</a>中给出；Attention为注意力机制层，FC为全连接层，模型的结构图如下（利用Keras导出）：</p><figure><img src="/img/nlp21_4.png" alt="模型结构示例图" /><figcaption aria-hidden="true">模型结构示例图</figcaption></figure><p>模型训练的脚本为<code>model_train.py</code>，完整的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># 模型训练</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> load_data <span class="hljs-keyword">import</span> get_train_test_pd<br><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> to_categorical<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Model<br><span class="hljs-keyword">from</span> keras.optimizers <span class="hljs-keyword">import</span> Adam<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Input, Dense<br><span class="hljs-keyword">from</span> bert.extract_feature <span class="hljs-keyword">import</span> BertVector<br><br><span class="hljs-keyword">from</span> att <span class="hljs-keyword">import</span> Attention<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> GRU, Bidirectional<br><br><br><span class="hljs-comment"># 读取文件并进行转换</span><br>train_df, test_df = get_train_test_pd()<br>bert_model = BertVector(pooling_strategy=<span class="hljs-string">&quot;NONE&quot;</span>, max_seq_len=<span class="hljs-number">80</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;begin encoding&#x27;</span>)<br>f = <span class="hljs-keyword">lambda</span> text: bert_model.encode([text])[<span class="hljs-string">&quot;encodes&quot;</span>][<span class="hljs-number">0</span>]<br><br>train_df[<span class="hljs-string">&#x27;x&#x27;</span>] = train_df[<span class="hljs-string">&#x27;text&#x27;</span>].apply(f)<br>test_df[<span class="hljs-string">&#x27;x&#x27;</span>] = test_df[<span class="hljs-string">&#x27;text&#x27;</span>].apply(f)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;end encoding&#x27;</span>)<br><br><span class="hljs-comment"># 训练集和测试集</span><br>x_train = np.array([vec <span class="hljs-keyword">for</span> vec <span class="hljs-keyword">in</span> train_df[<span class="hljs-string">&#x27;x&#x27;</span>]])<br>x_test = np.array([vec <span class="hljs-keyword">for</span> vec <span class="hljs-keyword">in</span> test_df[<span class="hljs-string">&#x27;x&#x27;</span>]])<br>y_train = np.array([vec <span class="hljs-keyword">for</span> vec <span class="hljs-keyword">in</span> train_df[<span class="hljs-string">&#x27;label&#x27;</span>]])<br>y_test = np.array([vec <span class="hljs-keyword">for</span> vec <span class="hljs-keyword">in</span> test_df[<span class="hljs-string">&#x27;label&#x27;</span>]])<br><span class="hljs-comment"># print(&#x27;x_train: &#x27;, x_train.shape)</span><br><br><span class="hljs-comment"># 将类型y值转化为ont-hot向量</span><br>num_classes = <span class="hljs-number">14</span><br>y_train = to_categorical(y_train, num_classes)<br>y_test = to_categorical(y_test, num_classes)<br><br><span class="hljs-comment"># 模型结构：BERT + 双向GRU + Attention + FC</span><br>inputs = Input(shape=(<span class="hljs-number">80</span>, <span class="hljs-number">768</span>,))<br>gru = Bidirectional(GRU(<span class="hljs-number">128</span>, dropout=<span class="hljs-number">0.2</span>, return_sequences=<span class="hljs-literal">True</span>))(inputs)<br>attention = Attention(<span class="hljs-number">32</span>)(gru)<br>output = Dense(<span class="hljs-number">14</span>, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>)(attention)<br>model = Model(inputs, output)<br><br><span class="hljs-comment"># 模型可视化</span><br><span class="hljs-comment"># from keras.utils import plot_model</span><br><span class="hljs-comment"># plot_model(model, to_file=&#x27;model.png&#x27;)</span><br><br>model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>              optimizer=Adam(),<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br><span class="hljs-comment"># 模型训练以及评估</span><br>model.fit(x_train, y_train, batch_size=<span class="hljs-number">8</span>, epochs=<span class="hljs-number">30</span>)<br>model.save(<span class="hljs-string">&#x27;people_relation.h5&#x27;</span>)<br><span class="hljs-built_in">print</span>(model.evaluate(x_test, y_test))<br></code></pre></td></tr></table></figure><p>利用该模型对数据集进行训练，输出的结果如下：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs markdown">begin encoding<br>end encoding<br>Epoch 1/30<br>1433/1433 [==============================] - 15s 10ms/step - loss: 1.5558 - acc: 0.4962<br><span class="hljs-strong">****</span><span class="hljs-strong">****</span><span class="hljs-strong">**(中间部分省略输出)**</span><span class="hljs-strong">****</span><span class="hljs-strong">****</span><span class="hljs-strong">****</span><br>Epoch 30/30<br>1433/1433 [==============================] - 12s 8ms/step - loss: 0.0210 - acc: 0.9951<br>[1.1099, 0.7709]<br></code></pre></td></tr></table></figure><p>整个训练过程持续十来分钟，经过30个epoch的训练，最终在测试集上的loss为1.1099，acc为0.7709，在小数据量下的效果还是不错的。训练过程（加入了earlystopping机制）生成的loss和acc图形如下：</p><figure><img src="/img/nlp21_5.png" alt="加入early stopping后的训练结果" /><figcaption aria-hidden="true">加入earlystopping后的训练结果</figcaption></figure><h3 id="模型预测">模型预测</h3><p>上述模型训练完后，利用保存好的模型文件，对新的数据进行预测。模型预测的脚本为<code>model_predict.py</code>，完整的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># 模型预测</span><br><br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> bert.extract_feature <span class="hljs-keyword">import</span> BertVector<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<br><span class="hljs-keyword">from</span> att <span class="hljs-keyword">import</span> Attention<br><br><span class="hljs-comment"># 加载模型</span><br>model = load_model(<span class="hljs-string">&#x27;people_relation.h5&#x27;</span>, custom_objects=&#123;<span class="hljs-string">&quot;Attention&quot;</span>: Attention&#125;)<br><br><span class="hljs-comment"># 示例语句及预处理</span><br>text = <span class="hljs-string">&#x27;赵金闪#罗玉兄#在这里，赵金闪和罗玉兄夫妇已经生活了大半辈子。他们夫妇都是哈密市伊州区林业和草原局的护林员，扎根东天山脚下，守护着这片绿。&#x27;</span><br>per1, per2, doc = text.split(<span class="hljs-string">&#x27;#&#x27;</span>)<br>text = <span class="hljs-string">&#x27;$&#x27;</span>.join([per1, per2, doc.replace(per1, <span class="hljs-built_in">len</span>(per1)*<span class="hljs-string">&#x27;#&#x27;</span>).replace(per2, <span class="hljs-built_in">len</span>(per2)*<span class="hljs-string">&#x27;#&#x27;</span>)])<br><span class="hljs-built_in">print</span>(text)<br><br><br><span class="hljs-comment"># 利用BERT提取句子特征</span><br>bert_model = BertVector(pooling_strategy=<span class="hljs-string">&quot;NONE&quot;</span>, max_seq_len=<span class="hljs-number">80</span>)<br>vec = bert_model.encode([text])[<span class="hljs-string">&quot;encodes&quot;</span>][<span class="hljs-number">0</span>]<br>x_train = np.array([vec])<br><br><span class="hljs-comment"># 模型预测并输出预测结果</span><br>predicted = model.predict(x_train)<br>y = np.argmax(predicted[<span class="hljs-number">0</span>])<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;data/rel_dict.json&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    rel_dict = json.load(f)<br><br>id_rel_dict = &#123;v:k <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> rel_dict.items()&#125;<br><span class="hljs-built_in">print</span>(id_rel_dict[y])<br></code></pre></td></tr></table></figure><p>该人物关系输出的结果为<code>夫妻</code>。</p><p>接着，我们对更好的数据进行预测，输出的结果如下：</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs clean">原文: 润生#润叶#不过，他对润生的姐姐润叶倒怀有一种亲切的感情。<br>预测人物关系: 兄弟姐妹<br>原文: 孙玉厚#兰花#脑子里把前后村庄未嫁的女子一个个想过去，最后选定了双水村孙玉厚的大女子兰花。<br>预测人物关系: 父母<br>原文: 金波#田福堂#每天来回二十里路，与他一块上学的金波和大队书记田福堂的儿子润生都有自行车，只有他是两条腿走路。<br>预测人物关系: unknown<br>原文: 润生#田福堂#每天来回二十里路，与他一块上学的金波和大队书记田福堂的儿子润生都有自行车，只有他是两条腿走路。<br>预测人物关系: 父母<br>原文: 周山#李自成#周山原是李自成亲手提拔的将领，闯王对他十分信任，叫他担任中军。<br>预测人物关系: 上下级<br>原文: 高桂英#李自成#高桂英是李自成的结发妻子，今年才三十岁。<br>预测人物关系: 夫妻<br>原文: 罗斯福#特德#果然，此后罗斯福的政治旅程与长他<span class="hljs-number">24</span>岁的特德叔叔如出一辙——纽约州议员、助理海军部长、纽约州州长以至美国总统。<br>预测人物关系: 亲戚<br>原文: 詹姆斯#克利夫兰#詹姆斯担任了该公司的经理，作为一名民主党人，他曾资助过克利夫兰的再度竞选，两人私交不错。<br>预测人物关系: 上下级（预测出错，应该是好友关系）<br>原文: 高剑父#关山月#高剑父是关山月在艺术道路上非常重要的导师，同时关山月也是最能够贯彻高剑父“折中中西”理念的得意门生。<br>预测人物关系: 师生<br>原文: 唐怡莹#唐石霞#唐怡莹，姓他他拉氏，名为他他拉·怡莹，又名唐石霞，隶属于满洲镶红旗。<br>预测人物关系: 同人<br></code></pre></td></tr></table></figure><h3 id="总结">总结</h3><p>本文采用的深度学习模型是文本分类模型，结合BERT预训练模型，在小标注数据量下对人物关系抽取这个任务取得了还不错的效果。同时模型的识别准确率和使用范围还有待于提升，提升点笔者认为如下：</p><ul><li><p>标注的数据量需要加大，现在的数据才2900条左右，如果数据量上去了，那么模型的准确率还有使用范围也会提升；</p></li><li><p>其他更多的模型有待于尝试；</p></li><li><p>在预测时，模型的预测时间较长，原因在于用BERT提取特征时耗时较长，可以考虑缩短模型预测的时间（比如使用ALBERT就能大大缩短预测时间）；</p></li><li><p>其他问题欢迎补充。</p><p>感谢大家阅读~</p></li></ul><blockquote><p>本人的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape），欢迎大家关注~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>关系抽取</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（二十）利用BERT实现文本二分类</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%EF%BC%89%E5%88%A9%E7%94%A8BERT%E5%AE%9E%E7%8E%B0%E6%96%87%E6%9C%AC%E4%BA%8C%E5%88%86%E7%B1%BB/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E4%BA%8C%E5%8D%81%EF%BC%89%E5%88%A9%E7%94%A8BERT%E5%AE%9E%E7%8E%B0%E6%96%87%E6%9C%AC%E4%BA%8C%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在我们进行事件抽取的时候，我们需要触发词来确定是否属于某个特定的事件类型，比如我们以政治上的出访类事件为例，这类事件往往会出现“访问”这个词语，但是仅仅通过“访问”这个触发词来判断是否属于出访类事件是不可靠的，比如我们会碰到以下情况：</p><p><img src="/img/nlp20_1.png" /></p><p>通过上面的例子，我们知道，像访问速度，访问量这种文档虽然出现了访问，但却不属于政治上的出访类事件。因此，这时候我们需要借助<code>文本分类</code>模型来判断，显然，这是一个二分类模型。</p><p>本文将会讲述如何利用BERT+DNN模型来判断文档是否属于政治上的出访类事件。</p><h3 id="数据集">数据集</h3><p>笔者找了300个文档，里面的文档都含有“出访”这个词语，标签1表示属于政治上的出访类事件，标签0则不是。将数据集分为训练集（250个样本）和测试集（50个样本），比例为5:1，样本不是很多，但借助BERT，我们可以在小样本上取得不错的效果。</p><p>训练集（部分）的样本如下：</p><figure><img src="/img/nlp20_2.png" alt="训练集部分数据" /><figcaption aria-hidden="true">训练集部分数据</figcaption></figure><h3 id="代码">代码</h3><p>本项目的结构如下：</p><figure><img src="/img/nlp20_3.png" alt="项目结构" /><figcaption aria-hidden="true">项目结构</figcaption></figure><p>因为我们这边是小样本量，所以需要用到BERT。又因为是中文，所以需要下载BERT的中文训练文件<code>chinese_L-12_H-768_A-12</code>，这是已经训练好的模型文件。</p><p>根据我们在文章<ahref="https://percent4.github.io/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89%E9%A6%96%E6%AC%A1%E4%BD%BF%E7%94%A8BERT%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%AF%BC/">NLP（十九）首次使用BERT的可视化指导</a>中的经验，我们需要写代码来调用BERT模型文件，比如tokenizer，padding,masking以及BERT模型产生输出向量等，幸运的是，有人已经帮助我们做好了这件事，我们只需要调用其代码就行了。这部分的代码位于bert文件夹下，读者可以在文章最后的Github地址上找到。因为本文的模型为文本分类模型，所以需要取[CLS]这个token所对应的768维的向量。</p><p>接下来，我们先读取数据集，处理成训练集和测试集，脚本为load_data.py，完整的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># author: Jclian91</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><span class="hljs-comment"># time: 2020-02-12 12:57</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><br><span class="hljs-comment"># 读取txt文件</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_txt_file</span>(<span class="hljs-params">file_path</span>):<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        content = [_.strip() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> f.readlines()]<br><br>    labels, texts = [], []<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> content:<br>        parts = line.split()<br>        label, text = parts[<span class="hljs-number">0</span>], <span class="hljs-string">&#x27;&#x27;</span>.join(parts[<span class="hljs-number">1</span>:])<br>        labels.append(label)<br>        texts.append(text)<br><br>    <span class="hljs-keyword">return</span> labels, texts<br><br><br>file_path = <span class="hljs-string">&#x27;data/train.txt&#x27;</span><br>labels, texts = read_txt_file(file_path)<br>train_df = pd.DataFrame(&#123;<span class="hljs-string">&#x27;label&#x27;</span>: labels, <span class="hljs-string">&#x27;text&#x27;</span>: texts&#125;)<br><br>file_path = <span class="hljs-string">&#x27;data/test.txt&#x27;</span><br>labels, texts = read_txt_file(file_path)<br>test_df = pd.DataFrame(&#123;<span class="hljs-string">&#x27;label&#x27;</span>: labels, <span class="hljs-string">&#x27;text&#x27;</span>: texts&#125;)<br><br><span class="hljs-built_in">print</span>(train_df.head())<br><span class="hljs-built_in">print</span>(test_df.head())<br><br>train_df[<span class="hljs-string">&#x27;text_len&#x27;</span>] = train_df[<span class="hljs-string">&#x27;text&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x))<br><span class="hljs-built_in">print</span>(train_df.describe())<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">  label                                               text<br>0     1  当地时间2月10日，白宫发表声明称，美国总统特朗普及夫人梅拉尼娅将于2月24日至25日访问印...<br>1     0  俄罗斯卫星通讯社11日最新消息，菲律宾总统杜特尔特已下令终止与美国间的《访问部队协定》(VFA)。<br>2     1  据俄罗斯卫星网6日报道，土耳其总统发言人卡林表示，俄罗斯军事代表团将于近日访问安卡拉，讨论叙...<br>3     0  先来说说什么是LPDDR5：要知道，手机中有两种内存颗粒，一种就是DRAM也就是大家常说的“...<br>4     1  在疫情的关键时刻，出现了一件令人感动的事情，让我们明白这才是真正的好朋友，不惧疫情访问我国，...<br>  label                                               text<br>0     1  应巴基斯坦总理伊姆兰·汗、荷兰王国首相吕特、德国联邦政府邀请，国家副主席王岐山将于5月26日...<br>1     1  联邦德国总理默克尔抵达印度进行访问，在雾霾笼罩下的新德里受到军人仪仗队的欢迎。默克尔赞扬了德...<br>2     1  5月6日至12日，省委副书记乌兰率代表团访问韩国、泰国，与韩国国际交流联盟、新村运动中央会等...<br>3     1  国台办发言人马晓光今天（5月22日）表示，新党主席、新中华儿女学会荣誉理事长郁慕明将率台湾各...<br>4     1  6月13日至15日，联合国反恐事务副秘书长沃伦科夫应邀访问北京和新疆，并与中国外交部副部长乐...<br>         text_len<br>count  250.000000<br>mean    77.540000<br>std     36.804493<br>min     11.000000<br>25%     47.500000<br>50%     73.000000<br>75%    100.750000<br>max    192.000000<br></code></pre></td></tr></table></figure><p>可以发现，训练数据集的文本长度的75%分位点为100.75，所以我们在模型训练的时候，padding过程中的统一长度取100。</p><p>数据预处理之后，我们利用BERT提取文档的特征，每个文档的填充长度为100，对应1个768维的向量，然后用Keras创建DNN来进行模型训练，训练完模型后对测试集进行验证，并保存该模型文件，便于后续的模型预测使用。模型训练的脚本为model_train.py，完整的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># author: Jclian91</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><span class="hljs-comment"># time: 2020-02-12 13:37</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-comment"># 是否使用GPU训练</span><br><span class="hljs-comment"># os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;4,5,6,7,8&quot;</span><br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> load_data <span class="hljs-keyword">import</span> train_df, test_df<br><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> to_categorical<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Model<br><span class="hljs-keyword">from</span> keras.optimizers <span class="hljs-keyword">import</span> Adam<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Input, BatchNormalization, Dense<br><span class="hljs-keyword">from</span> bert.extract_feature <span class="hljs-keyword">import</span> BertVector<br><br><span class="hljs-comment"># 读取文件并进行转换</span><br>bert_model = BertVector(pooling_strategy=<span class="hljs-string">&quot;REDUCE_MEAN&quot;</span>, max_seq_len=<span class="hljs-number">100</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;begin encoding&#x27;</span>)<br>f = <span class="hljs-keyword">lambda</span> text: bert_model.encode([text])[<span class="hljs-string">&quot;encodes&quot;</span>][<span class="hljs-number">0</span>]<br>train_df[<span class="hljs-string">&#x27;x&#x27;</span>] = train_df[<span class="hljs-string">&#x27;text&#x27;</span>].apply(f)<br>test_df[<span class="hljs-string">&#x27;x&#x27;</span>] = test_df[<span class="hljs-string">&#x27;text&#x27;</span>].apply(f)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;end encoding&#x27;</span>)<br><br>x_train = np.array([vec <span class="hljs-keyword">for</span> vec <span class="hljs-keyword">in</span> train_df[<span class="hljs-string">&#x27;x&#x27;</span>]])<br>x_test = np.array([vec <span class="hljs-keyword">for</span> vec <span class="hljs-keyword">in</span> test_df[<span class="hljs-string">&#x27;x&#x27;</span>]])<br>y_train = np.array([vec <span class="hljs-keyword">for</span> vec <span class="hljs-keyword">in</span> train_df[<span class="hljs-string">&#x27;label&#x27;</span>]])<br>y_test = np.array([vec <span class="hljs-keyword">for</span> vec <span class="hljs-keyword">in</span> test_df[<span class="hljs-string">&#x27;label&#x27;</span>]])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;x_train: &#x27;</span>, x_train.shape)<br><br><span class="hljs-comment"># Convert class vectors to binary class matrices.</span><br>num_classes = <span class="hljs-number">2</span><br>y_train = to_categorical(y_train, num_classes)<br>y_test = to_categorical(y_test, num_classes)<br><br><span class="hljs-comment"># 创建DNN模型</span><br>x_in = Input(shape=(<span class="hljs-number">768</span>, ))<br>x_out = Dense(<span class="hljs-number">32</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)(x_in)<br>x_out = BatchNormalization()(x_out)<br>x_out = Dense(num_classes, activation=<span class="hljs-string">&quot;softmax&quot;</span>)(x_out)<br>model = Model(inputs=x_in, outputs=x_out)<br><span class="hljs-built_in">print</span>(model.summary())<br><br>model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>,<br>              optimizer=Adam(),<br>              metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br><span class="hljs-comment"># 模型训练、评估以及保存</span><br>model.fit(x_train, y_train, batch_size=<span class="hljs-number">8</span>, epochs=<span class="hljs-number">20</span>)<br>model.save(<span class="hljs-string">&#x27;visit_classify.h5&#x27;</span>)<br><span class="hljs-built_in">print</span>(model.evaluate(x_test, y_test))<br></code></pre></td></tr></table></figure><h3 id="模型训练">模型训练</h3><p>在模型训练中，我们创建的DNN模型结构如下： <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_<br><span class="hljs-section">Layer (type)                 Output Shape              Param #   </span><br><span class="hljs-section">=================================================================</span><br>input<span class="hljs-emphasis">_1 (InputLayer)         (None, 768)               0         </span><br><span class="hljs-emphasis"><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_</span><br>dense<span class="hljs-emphasis">_1 (Dense)              (None, 32)                24608     </span><br><span class="hljs-emphasis"><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_</span><br>batch<span class="hljs-emphasis">_normalization_</span>1 (Batch (None, 32)                128       <br><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_<br><span class="hljs-section">dense<span class="hljs-emphasis">_2 (Dense)              (None, 2)                 66        </span></span><br><span class="hljs-emphasis"><span class="hljs-section">=================================================================</span></span><br><span class="hljs-emphasis"><span class="hljs-section">Total params: 24,802</span></span><br><span class="hljs-emphasis"><span class="hljs-section">Trainable params: 24,738</span></span><br><span class="hljs-emphasis"><span class="hljs-section">Non-trainable params: 64</span></span><br><span class="hljs-emphasis"><span class="hljs-section"><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_</span></span><br></code></pre></td></tr></table></figure>模型训练过程中的输出如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs bash">Epoch 1/20<br><br>  8/250 [..............................] - ETA: 43s - loss: 1.0427 - acc: 0.3750<br>250/250 [==============================] - 1s 6ms/step - loss: 0.3345 - acc: 0.8640<br>Epoch 2/20<br><br>  8/250 [..............................] - ETA: 0s - loss: 0.2664 - acc: 0.8750<br>250/250 [==============================] - 0s 133us/step - loss: 0.2147 - acc: 0.9320<br><br>.........(省略部分输出结果)............<br><br>Epoch 19/20<br><br>  8/250 [..............................] - ETA: 0s - loss: 0.2481 - acc: 0.8750<br>250/250 [==============================] - 0s 136us/step - loss: 0.0716 - acc: 0.9760<br>Epoch 20/20<br><br>  8/250 [..............................] - ETA: 0s - loss: 0.0149 - acc: 1.0000<br>250/250 [==============================] - 0s 140us/step - loss: 0.0560 - acc: 0.9800<br><br>32/50 [==================&gt;...........] - ETA: 0s<br>50/50 [==============================] - 0s 4ms/step<br>[0.3687818288803101, 0.9199999928474426]<br></code></pre></td></tr></table></figure><p>经过20个epoch的训练，模型在训练集上的准确率为0.9800，在测试集上的准确率约为0.9200，BERT的效果如此惊人，后接简单的DNN模型就能取得如此不错的效果。</p><h3 id="模型预测">模型预测</h3><p>为了再次验证模型的预测效果，笔者从网站上又重新找了20个文档，对其进行预测。预测的脚本为model_predict.py，完整的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># author: Jclian91</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><span class="hljs-comment"># time: 2020-02-12 17:33</span><br><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> bert.extract_feature <span class="hljs-keyword">import</span> BertVector<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<br>load_model = load_model(<span class="hljs-string">&quot;visit_classify.h5&quot;</span>)<br><br><span class="hljs-comment"># 预测语句</span><br>texts = [<span class="hljs-string">&#x27;在访问限制中，用户可以选择禁用iPhone的功能，包括Siri、iTunes购买功能、安装/删除应用等，甚至还可以让iPhone变成一台功能手机。以下是访问限制具体可以实现的一些功能&#x27;</span>,<br>         <span class="hljs-string">&#x27;IT之家4月23日消息 近日，谷歌在其官方论坛发布消息表示，他们为Android Auto添加了一项新功能：可以访问完整联系人列表。用户现在可以通过在Auto的电话拨号界面中打开左上角的菜单访问完整的联系人列表。值得注意的是，这一功能仅支持在车辆停止时使用。&#x27;</span>,<br>         <span class="hljs-string">&#x27;要通过telnet 访问路由器，需要先通过console 口对路由器进行基本配置，例如：IP地址、密码等。&#x27;</span>,<br>         <span class="hljs-string">&#x27;IT之家3月26日消息 近日反盗版的国际咨询公司MUSO发布了2017年的年度报告，其中的数据显示，去年盗版资源网站访问量达到了3000亿次，比前一年（2016年）提高了1.6%。美国是访问盗版站点次数最多的国家，共有279亿次访问；其后分别是俄罗斯、印度和巴西，中国位列第18。&#x27;</span>,<br>         <span class="hljs-string">&#x27;应葡萄牙议会邀请，全国人大常委会副委员长吉炳轩率团于12月14日至16日访问葡萄牙，会见副议长费利佩、社会党副总书记卡内罗。&#x27;</span>,<br>         <span class="hljs-string">&#x27;2月26日至3月2日，应香港特区政府“内地贵宾访港计划”邀请，省委常委、常务副省长陈向群赴港考察访问，重点围绕“香港所长、湖南所需”，与特区政府相关部门和机构深入交流，推动湖南与香港交流合作取得新进展。&#x27;</span>,<br>         <span class="hljs-string">&#x27;目前A站已经恢复了访问，可以直接登录，网页加载正常，视频已经可以正常播放。&#x27;</span>,<br>         <span class="hljs-string">&#x27;难民署特使安吉丽娜·朱莉6月8日结束了对哥伦比亚和委内瑞拉边境地区的难民营地为期两天的访问，她对哥伦比亚人民展现的人道主义和勇气表示赞扬。&#x27;</span>,<br>         <span class="hljs-string">&#x27;据《南德意志报》报道，德国总理默克尔计划明年1月就前往安卡拉，和土耳其总统埃尔多安进行会谈。&#x27;</span>,<br>         <span class="hljs-string">&#x27;自9月14日至18日，由越共中央政治局委员、中央书记处书记、中央经济部部长阮文平率领工作代表团对希腊进行工作访问。&#x27;</span>,<br>         <span class="hljs-string">&#x27;Win7电脑提示无线适配器或访问点有问题怎么办?很多用户在使用无线网连接上网时，发现无线网显示已连接，但旁边却出现了一个黄色感叹号，无法进行网络操作，通过诊断提示电脑无线适配器或访问点有问题，且处于未修复状态，这该怎么办呢?下面小编就和大家分享下Win7电脑提示无线适配器或访问点有问题的解决方法。&#x27;</span>,<br>         <span class="hljs-string">&#x27;2019年10月13日至14日，外交部副部长马朝旭访问智利，会见智利外长里韦拉，同智利总统外事顾问萨拉斯举行会谈，就智利举办亚太经合组织（APEC）第二十七次领导人非正式会议等深入交换意见。&#x27;</span>,<br>         <span class="hljs-string">&#x27;未开发所有安全组之前访问，FTP可以链接上，但是打开会很慢，需要1-2分钟才能链接上&#x27;</span>,<br>         <span class="hljs-string">&#x27;win7系统电脑的用户，在连接WIFI网络网上时，有时候会遇到突然上不了网，查看连接的WIFI出现“有限的访问权限”的文字提示。&#x27;</span>,<br>         <span class="hljs-string">&#x27;联合国秘书长潘基文８日访问了日本福岛县，与当地灾民交流并访问了一所高中。&#x27;</span>,<br>         <span class="hljs-string">&#x27;国务院总理温家宝当地时间23日下午乘专机抵达布宜诺斯艾利斯，开始对阿根廷进行正式访问。&#x27;</span>,<br>         <span class="hljs-string">&#x27;正在中国访问的巴巴多斯总理斯图尔特１５日在陕西西安参观访问。&#x27;</span>,<br>         <span class="hljs-string">&#x27;据外媒报道,当地时间10日,美国白宫发声明称,美国总统特朗普将于2月底访问印度,与印度总理莫迪进行战略对话。&#x27;</span>,<br>         <span class="hljs-string">&#x27;2月28日，唐山曹妃甸蓝色海洋科技有限公司董事长赵力军等一行5人到黄海水产研究所交流访问。黄海水产研究所副所长辛福言及相关部门负责人、专家等参加了会议。&#x27;</span>,<br>         <span class="hljs-string">&#x27;2018年7月2日，莫斯科孔子文化促进会会长姜彦彬，常务副会长陈国建，在中国著名留俄油画大师牟克教授的陪同下，访问了莫斯科国立苏里科夫美术学院，受到第一副校长伊戈尔·戈尔巴秋克先生接待。&#x27;</span><br>         ]<br><br>labels = []<br><br>bert_model = BertVector(pooling_strategy=<span class="hljs-string">&quot;REDUCE_MEAN&quot;</span>, max_seq_len=<span class="hljs-number">100</span>)<br><br><span class="hljs-comment"># 对上述句子进行预测</span><br><span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br><br>    <span class="hljs-comment"># 将句子转换成向量</span><br>    vec = bert_model.encode([text])[<span class="hljs-string">&quot;encodes&quot;</span>][<span class="hljs-number">0</span>]<br>    x_train = np.array([vec])<br><br>    <span class="hljs-comment"># 模型预测</span><br>    predicted = load_model.predict(x_train)<br>    y = np.argmax(predicted[<span class="hljs-number">0</span>])<br>    label = <span class="hljs-string">&#x27;Y&#x27;</span> <span class="hljs-keyword">if</span> y <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;N&#x27;</span><br>    labels.append(label)<br><br><span class="hljs-keyword">for</span> text,label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(texts, labels):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;%s\t%s&#x27;</span>%(label, text))<br><br><span class="hljs-comment"># 将结果保存为xlsx文件</span><br>df = pd.DataFrame(&#123;<span class="hljs-string">&#x27;句子&#x27;</span>:texts, <span class="hljs-string">&quot;是否属于出访类事件&quot;</span>: labels&#125;)<br>df.to_excel(<span class="hljs-string">&#x27;./result.xlsx&#x27;</span>, index=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure><p>模型预测的结果会输出，同时也会保存至Excel，文件的内容如下：</p><figure><img src="/img/nlp20_4.png" alt="excel文件中的内容" /><figcaption aria-hidden="true">excel文件中的内容</figcaption></figure><p>所有预测的文档完全正确！</p><h3 id="预测">预测</h3><p>本项目已开源，Github地址为：<ahref="https://github.com/percent4/bert_doc_binary_classification">https://github.com/percent4/bert_doc_binary_classification</a>。</p><p>通过笔者自己的试验，BERT在小标注样本量的效果确实很不错，后续我们还将继续接触BERT！</p><p>感谢大家的阅读~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BERT</tag>
      
      <tag>NLP</tag>
      
      <tag>文本分类</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（十九）首次使用BERT的可视化指导</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89%E9%A6%96%E6%AC%A1%E4%BD%BF%E7%94%A8BERT%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%AF%BC/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89%E9%A6%96%E6%AC%A1%E4%BD%BF%E7%94%A8BERT%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%AF%BC/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文（部分内容）翻译自文章<ahref="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/">AVisual Guide to Using BERT for the First Time</a>，其作者为JayAlammar，访问网址为：<ahref="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/">http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/</a>，可以作为那些不熟悉BERT的读者首次阅读。文章中如有翻译不当之处，还请批评指正。</p><p><img src="/img/nlp19_1.png" /></p><p>本文是关于如何使用BERT的变异版本来进行句子分类的简单教程。该例子足够简单，因此可以作为首次使用BERT的介绍，当然，它也包含了一些关键性的概念。</p><h3 id="数据集sst2">数据集：SST2</h3><p>本文中使用的数据集为<ahref="https://nlp.stanford.edu/sentiment/index.html">SST2</a>，它包含了电影评论的句子，每一句带有一个标签，或者标注为<code>正面情感</code>（取值为1），或者标注为<code>负面情感</code>（取值为0）。</p><p><img src="/img/nlp19_2.png" /></p><h3 id="模型句子情感分类">模型：句子情感分类</h3><p>我们的目标是创建一个模型，它能够处理一个句子（就行我们数据集中的句子那样）并且输出1（表明该句子具有正面情感）或者0（表明该句子具有负面情感）。我们设想它长这样：</p><figure><img src="/img/nlp19_3.png" alt="模型描述" /><figcaption aria-hidden="true">模型描述</figcaption></figure><p>事实上，该模型包含两个模型：</p><ul><li><code>DistillBERT</code>会处理句子并把它提取后的信息传递给下一个模型。<code>DistillBERT</code>是<code>BERT</code>的变异版本，由<code>HuggingFace</code>小组开发和开源。它是<code>BERT</code>的更轻量、更快速的版本，同时它的表现基本与<code>BERT</code>相近。</li><li>下一个模型，从scikitlearn中导入的一个基本的<code>逻辑回归模型</code>（Logistic Regressionmodel），它会利用<code>DistillBERT</code>的处理结果，然后将句子进行分类成<code>正面情感</code>或者<code>负面情感</code>（分别为1或者0）。</li></ul><p>在两个模型之间传递的数据为1个768维的向量。我们可以把这个向量理解为这个句子的嵌入向量（EmbeddingVector），用于分类。</p><p><img src="/img/nlp19_4.png" /></p><h3 id="模型训练">模型训练</h3><p>尽管我们用了两个模型，但是我们只会训练<code>逻辑回归模型</code>。对于<code>DistillBERT</code>，我们会使用已经预训练好的英语模型。该模型，既不会被训练也不会做<code>微调（fine-tuned）</code>，直接进行句子分类。这是因为，我们可以从<code>BERT</code>中获得句子分类的能力。这尤其适合<code>BERT</code>输出的第一个位置（跟[CLS]标志相关）。我相信这是由于<code>BERT</code>的第二个训练模型——<code>下一句分类（Next sentence classification）</code>。该模型的目标在于封装句子级别的语料进行训练，并输出第一个位置。<code>transformers</code>库已经提供了<code>DistillBERT</code>的操作，作为其预训练模型版本。</p><figure><img src="/img/nlp19_5.png" alt="模型训练" /><figcaption aria-hidden="true">模型训练</figcaption></figure><h3 id="教程总览">教程总览</h3><p>以下是该教程的计划安排。首先我们会使用<code>DistillBERT</code>来产生2000个句子的句子向量。</p><figure><img src="/img/nlp19_6.png" alt="利用DistillBERT产生句子向量" /><figcaption aria-hidden="true">利用DistillBERT产生句子向量</figcaption></figure><p>这一步之后我们不会接触<code>DistillBERT</code>。接下去只是ScikitLearn的操作。我们将数据集分为训练集和测试集。</p><figure><img src="/img/nlp19_7.png"alt="将数据集经过Distilll处理后划分为训练集和测试集，注意sklearn的划分是将数据集打乱(shuffle)后再进行划分，所以不是取数据集的前75%作为训练集。" /><figcaptionaria-hidden="true">将数据集经过Distilll处理后划分为训练集和测试集，注意sklearn的划分是将数据集打乱(shuffle)后再进行划分，所以不是取数据集的前75%作为训练集。</figcaption></figure><p>接下来我们在训练集上使用<code>逻辑回归模型</code>进行训练。</p><p><img src="/img/nlp19_8.png" /></p><h3 id="单次预测如何计算">单次预测如何计算</h3><p>在我们讲解代码和解释如何训练模型之前，让我们看一下已预训练好的模型如何进行预测。我们尝试着预测句子“a visually stunning rumination onlove”。第一步是使用BERT tokenizer将句子划分成tokens。然后加上句子分类的特殊tokens（[CLS]在开始位置，[SEP]在句子结尾）。</p><p><img src="/img/nlp19_9.png" /></p><p>第三步是通过已预训练好的模型的嵌入表（embeddingtable）将每一个tokens映射成各自的id。这一步可以参考<code>word embedding</code>，参考阅读文章<ahref="http://jalammar.github.io/illustrated-word2vec/">The IllustratedWord2vec</a>。</p><p><img src="/img/nlp19_10.png" /></p><p>我们注意到，tokenizer仅需要一行代码就能完成以上步骤。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">tokenizer.encode(<span class="hljs-string">&quot;a visually stunning rumination on love&quot;</span>, add_special_tokens=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>我们的输入句子现在已经处理成<code>DistilBERT</code>可以处理的格式了。如果你已经读过<ahref="http://jalammar.github.io/illustrated-bert/">IllustratedBERT</a>，那么这一步的可视化如下：</p><p><img src="/img/nlp19_11.png" /></p><h3 id="distilbert处理流程">DistilBERT处理流程</h3><p><code>DistilBERT</code>处理输入向量的流程类似于<code>BERT</code>。输出是每一个token对应一个向量。每个向量由768个浮点型数字组成。</p><p><img src="/img/nlp19_12.png" /></p><p>因为这是一个句子分类任务，故我们忽略其他向量而只取第一个向量（跟[CLS]相关的那个）。这个向量我们会作为<code>逻辑回归模型</code>的输入。</p><p><img src="/img/nlp19_13.png" /></p><p>从这里开始，就是<code>逻辑回归模型</code>的事儿了，它负责将输入的向量进行分类。我们设想一个预测的流程长这样：</p><p><img src="/img/nlp19_14.png" /></p><h3 id="代码">代码</h3><p>文章中用到的数据集下载网址为：<ahref="https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv">https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv</a>。下载<code>DistillBERT</code>模型文件，网址为：<ahref="https://www.kaggle.com/abhishek/distilbertbaseuncased">https://www.kaggle.com/abhishek/distilbertbaseuncased</a>。</p><p>原文中这部分的代码讲解比较多，我这边忽略过去了，笔者想按自己的思路来处理，因此这部分内容会有调整。完整的思路如下：</p><p>下载数据集和模型文件，与代码放在同一目录下。建立jupyter脚本，先载入必要的模块：</p><p><img src="/img/nlp19_15.png" /></p><p>接着我们利用pandas读取训练集数据，并统计标签值的频数：</p><p><img src="/img/nlp19_16.png" /></p><p>读取<code>DistillBERT</code>模型文件并创建tokenizer：</p><p><img src="/img/nlp19_17.png" /></p><p>通过tokenizer完成句子切分成tokens，并映射到id:</p><p><img src="/img/nlp19_18.png" /></p><p>由于每个句子的长度可能会不同，因此需要对句子进行填充（Padding），保持每个句子的输入维度一致，句子填充的长度为该数据集中句子长度的最大值。</p><p><img src="/img/nlp19_19.png" /></p><p>对句子进行填充后，然后再进行Masking。这是因为如果我们直接将padded传入<code>BERT</code>，这会造成一定的困扰。我们需要创建另一个变量，来告诉模型去mask之前的填充结果。这就是attention_mask的作用：</p><p><img src="/img/nlp19_20.png" /></p><p>我们的输入已经准备完毕，接下来我们尝试着用<code>DistillBERT</code>来获取向量，也就是之前说的第一步。这一步的处理结果会返回<code>last_hidden_states</code>，而我们的分类模型只需要获取<code>[CLS]</code>这个token对应的输出向量。</p><p><img src="/img/nlp19_21.png" /></p><p>可视化的操作说明如下图：</p><p><img src="/img/nlp19_22.png" /></p><p>这样，我们就把之前的每一个句子映射成了1个768维的句子向量，然后就利用<code>逻辑回归模型</code>直接进行训练就可以了。</p><p><img src="/img/nlp19_23.png" /></p><p>最后，我们来看一下这个模型在测试集上的效果：</p><p><img src="/img/nlp19_24.png" /></p><h3 id="总结">总结</h3><p>本文主要介绍了如何利用<code>DistillBERT</code>和已经封装好的<code>transformers</code>模块，结合<code>逻辑回归模型</code>对英文句子进行文本二分类。后续笔者还会研究在中文上的文本分类以及如何进行微调（Fine_tuning）。</p><p>本项目的Gitlab地址为：<ahref="https://gitlab.com/jclian91/sentence_classify_using_distillBERT_LR">https://gitlab.com/jclian91/sentence_classify_using_distillBERT_LR</a>，原文章作者的Github地址为<ahref="https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb">https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb</a>。</p><p>感谢大家阅读~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>利用关系抽取构建知识图谱的一次尝试</title>
    <link href="/2023/07/08/%E5%88%A9%E7%94%A8%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E6%9E%84%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/"/>
    <url>/2023/07/08/%E5%88%A9%E7%94%A8%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E6%9E%84%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="关系抽取">关系抽取</h3><p>信息抽取（Information Extraction,IE）旨在从大规模非结构或半结构的自然语言文本中抽取结构化信息。关系抽取（RelationExtraction,RE）是其中的重要子任务之一，主要目的是从文本中识别实体并抽取实体之间的语义关系，是自然语言处理（NLP）中的一项基本任务。比如，我们可以从下面的一段话中，</p><blockquote><p>鸿海集团董事长郭台铭25日表示，阿里巴巴集团董事局主席马云提的新零售、新制造中的「新制造」，是他给加上的。网易科技报导，郭台铭在2018深圳IT领袖峰会谈到工业互联网时表示，眼睛看的、脑筋想的、嘴巴吃的、耳朵听的，都在随着互联网的发展而蓬勃发展，当然互联网不是万能的，比如说刚才李小加要水喝，在手机上一按就能出一瓶水吗？当然做不到，还是得有实体经济。</p></blockquote><p>可以抽取出如下三元组，用来表示实体之间的关系：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scheme">[<span class="hljs-symbol">&#x27;鸿海集团</span>&#x27;, <span class="hljs-symbol">&#x27;董事长</span>&#x27;, <span class="hljs-symbol">&#x27;郭台铭</span>&#x27;]<br>[<span class="hljs-symbol">&#x27;阿里巴巴集团</span>&#x27;, <span class="hljs-symbol">&#x27;主席</span>&#x27;, <span class="hljs-symbol">&#x27;马云</span>&#x27;]<br></code></pre></td></tr></table></figure><p>并且能够形成如下的简单的知识图谱（Knowledge Graph）。</p><p><img src="/img/kg1_1.png" /></p><p>关于知识图谱，笔者已经在文章<ahref="https://www.jianshu.com/p/286eeef0e0c3">SPARQL入门（一）SPARQL简介与简单使用</a>中给出了一些介绍，而利用关系抽取，我们可以从一些非结构化数据中，提取出实体之间的关系，形成知识图谱，这在很大程度上可以帮助我们减轻构建知识图谱的成本。非结构化数据越多，关系抽取效果越好，我们构建的知识图谱就会越庞大，实体之间的关系也会越丰富。</p><h3 id="如何做好关系抽取">如何做好关系抽取？</h3><p>目前，网络上有许多与关系抽取相关的公开比赛，比如：</p><ul><li><p>CCKS 2019 人物关系抽取，网址为：<ahref="https://biendata.com/competition/ccks_2019_ipre/">https://biendata.com/competition/ccks_2019_ipre/</a>；</p></li><li><p>2019语言与智能技术竞赛信息抽取：<ahref="http://lic2019.ccf.org.cn/kg">http://lic2019.ccf.org.cn/kg</a>。</p><p>常用的关系抽取语料如下：</p></li><li><p>MUC关系抽取任务数据集；</p></li><li><p>ACE关系抽取任务数据集；</p></li><li><p>TAC-KBP数据集。</p><p>现阶段，关系抽取的办法主要如下：</p></li><li><p>基于规则的模式匹配；</p></li><li><p>基于监督学习的方法；</p></li><li><p>半监督和无监督学习方法；</p></li><li><p>远程监督的方法；</p></li><li><p>深度学习模型。</p><p>接着，笔者想说下，为什么最近会研究关系抽取。在一个偶然的机会，笔者看到了这个网站：<ahref="https://www.wisers.ai/zh-cn/browse/relation-extraction/demo/">https://www.wisers.ai/zh-cn/browse/relation-extraction/demo/</a>，截图如下：</p></li></ul><p><img src="/img/kg1_2.png" /></p><p>这个图给人以一种非常炫酷的感觉，因此，笔者就被它所吸引了。但笔者在这个demo网站上尝试了几篇新的语料，有些效果好，有些效果不尽如人意，因此，笔者决定自己动手实现一个关系抽取的模型！</p><p>虽然网上已经有许多现成的很好的关系抽取的模型，但笔者还是希望能够按照自己的意愿和想法来实现一下，当然，仅仅是作为一次尝试。笔者的思路如下：</p><ul><li>以句子级别进行标注，标注出句子中的主语，谓语，宾语，形成标注序列；</li><li>利用标注好的语料，采用bert+dl的方法进行训练；</li><li>对新的语料，预测主语，谓语，宾语，然后利用一定的策略，形成实体关系；</li><li>对新语料的实体关系进行可视化展示。</li></ul><p>如果你对笔者的尝试感兴趣，请尝试这阅读下去。</p><h3 id="如何标注">如何标注？</h3><p>按照笔者的惯例，还是自己进行标注。那么，对于关系抽取，该如何进行标注呢？比如，下面这句话：</p><blockquote><p>应日本国首相安倍晋三邀请，出席二十国集团领导人第十四次峰会。</p></blockquote><p>我们需要的实体关系应该是：日本国--&gt;首相--&gt;安倍晋三，那么我们可以选择主语为日本，谓语为首相，宾语为安倍晋三，形成的标注序列如下：</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs mathematica">应<span class="hljs-built_in">O</span><br>日<span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>本<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>国<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>首<span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">PRED</span><br>相<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">PRED</span><br>安<span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">OBJ</span><br>倍<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">OBJ</span><br>晋<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">OBJ</span><br>三<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">OBJ</span><br>邀<span class="hljs-built_in">O</span><br>请<span class="hljs-built_in">O</span><br>，<span class="hljs-built_in">O</span><br>出<span class="hljs-built_in">O</span><br>席<span class="hljs-built_in">O</span><br>二<span class="hljs-built_in">O</span><br>十<span class="hljs-built_in">O</span><br>国<span class="hljs-built_in">O</span><br>集<span class="hljs-built_in">O</span><br>团<span class="hljs-built_in">O</span><br>领<span class="hljs-built_in">O</span><br>导<span class="hljs-built_in">O</span><br>人<span class="hljs-built_in">O</span><br>第<span class="hljs-built_in">O</span><br>十<span class="hljs-built_in">O</span><br>四<span class="hljs-built_in">O</span><br>次<span class="hljs-built_in">O</span><br>峰<span class="hljs-built_in">O</span><br>会<span class="hljs-built_in">O</span><br>。<span class="hljs-built_in">O</span><br></code></pre></td></tr></table></figure><p>对于句子中出现多主语，多谓语，多宾语的情况，也可以照此进行标注，比如下面这句：</p><blockquote><p>齐鹏飞同志任中共中国人民大学委员会常委、副书记。</p></blockquote><p>形成的标注序列如下：</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs mathematica">齐<span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">OBJ</span><br>鹏<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">OBJ</span><br>飞<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">OBJ</span><br>同<span class="hljs-built_in">O</span><br>志<span class="hljs-built_in">O</span><br>任<span class="hljs-built_in">O</span><br>中<span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>共<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>中<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>国<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>人<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>民<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>大<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>学<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>委<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>员<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>会<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>常<span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">PRED</span><br>委<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">PRED</span><br>、<span class="hljs-built_in">O</span><br>副<span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">PRED</span><br>书<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">PRED</span><br>记<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">PRED</span><br>。<span class="hljs-built_in">O</span><br></code></pre></td></tr></table></figure><p>对此，我们希望形成两个三元组，分别为：中共中国人民大学委员会--&gt;常委--&gt;齐鹏飞,中共中国人民大学委员会--&gt;副书记--&gt;齐鹏飞。</p><p>笔者利用自己的标注平台（后续会在Github开源），一共标注了950分语料，其中80%作为训练集，10%作为验证集，另外10%作为测试集。当然，标注的过程是很痛苦的，这些标注量也还远远不够，后续会持续不断地更新。</p><h3 id="模型训练">模型训练</h3><p>由于是小样本量的标注数量，因此，在模型的选择上，需要预训练模型，笔者的预训练模型选择BERT。在预训练的基础上，选择BiLSTM+CRF深度学习模型，对上述语料进行训练，共训练100次，在验证集和测试集上的效果如下：</p><p>验证集：</p><table><thead><tr class="header"><th>项目</th><th>precision</th><th>recall</th><th>f1</th></tr></thead><tbody><tr class="odd"><td>全部</td><td>71.08%</td><td>78.27%</td><td>74.50%</td></tr><tr class="even"><td>宾语</td><td>78.95%</td><td>88.24%</td><td>83.33%</td></tr><tr class="odd"><td>谓语</td><td>68.00%</td><td>74.56%</td><td>71.13%</td></tr><tr class="even"><td>主语</td><td>67.18%</td><td>73.33%</td><td>70.12%</td></tr></tbody></table><p>测试集</p><table><thead><tr class="header"><th>项目</th><th>precision</th><th>recall</th><th>f1</th></tr></thead><tbody><tr class="odd"><td>全部</td><td>75.07%</td><td>82.18%</td><td>78.46%</td></tr><tr class="even"><td>宾语</td><td>78.33%</td><td>85.45%</td><td>81.74%</td></tr><tr class="odd"><td>谓语</td><td>73.23%</td><td>82.30%</td><td>77.50%</td></tr><tr class="even"><td>主语</td><td>73.88%</td><td>79.20%</td><td>76.45%</td></tr></tbody></table><p>效果并没有达到很好，一方面是标注策略的问题，另一方面是标注的数量问题（因为这是一个通用模型），后续我们可以看看，当标注数量提上去后，模型训练的效果是否会有提升。</p><h3 id="模型预测">模型预测</h3><p>接着，我们利用刚才训练好的模型，对新的句子进行预测，记住，预测的级别为句子。当然，预测的结果，只是序列标注模型识别出的结果，我们还要采用一定的策略，将其形成三元组。比如以下的句子：</p><blockquote><p>英媒称，美国农业部长桑尼·珀杜在6月25日播出的一个访谈节目中承认，美国农民是特朗普总统对华贸易战的“受害者”。</p></blockquote><p>预测的结果如下：</p><blockquote><p>[{'word': '美国', 'start': 4, 'end': 6, 'type': 'SUBJ'}, {'word':'农业部长', 'start': 6, 'end': 10, 'type': 'PRED'}, {'word':'桑尼·珀杜', 'start': 10, 'end': 15, 'type': 'OBJ'}, {'word': '美国','start': 34, 'end': 36, 'type': 'SUBJ'}]</p></blockquote><p>可以看到，模型识别出主语为美国，谓语为农业部长，宾语为桑尼·珀杜，这是一个完美的三元组。</p><p>我们再来对下面的语句进行预测：</p><blockquote><p>6月25日，华为常务董事、运营商事业部总裁丁耘表示，华为已在全球范围内获得50个5G商用合同，其中2/3是由华为协助其构建的。</p></blockquote><p>预测结果为：</p><blockquote><p>[{'word': '华为', 'start': 6, 'end': 8, 'type': 'SUBJ'}, {'word':'常务董事', 'start': 8, 'end': 12, 'type': 'PRED'}, {'word':'运营商事业部', 'start': 13, 'end': 19, 'type': 'SUBJ'}, {'word':'总裁', 'start': 19, 'end': 21, 'type': 'PRED'}, {'word': '丁耘','start': 21, 'end': 23, 'type': 'OBJ'}, {'word': '华为', 'start': 26,'end': 28, 'type': 'SUBJ'}, {'word': '华为', 'start': 54, 'end': 56,'type': 'SUBJ'}]</p></blockquote><p>这就需要一定的策略，才能识别出具体的三元组了。笔者采用的策略如下：</p><ul><li>按主语，谓语，宾语进行归类，形成主体集合<code>&#123;华为, 运营商事业部&#125;</code>，谓语集合<code>&#123;常务董事, 总裁&#125;</code>以及宾语集合<code>&#123;丁耘&#125;</code>；</li><li>接着，按照各个元素在句子出现的位置进行组合，比如<code>华为</code>的位置，离<code>常务董事</code>挨得近，那么形成一个三元组['华为','常务董事', '丁耘']，同理，形成另一个三元组['运营商事业部', '总裁','丁耘'];</li><li>将句子按照逗号进行分割，形成<code>小句子集合</code>，看三元组的三个元素是否都在一个小句子中，如果是，则提取该三元组，如果不是，则放弃该三元组。</li></ul><h3 id="关系抽取可视化">关系抽取可视化</h3><p>对于关系抽取后的节后，我们将三元组导入至Neo4J中，查看可视化的效果。我们一共选择三篇文章进行测试，为了取得较好的效果，我们选择了程序处理+人工check（过滤）的过程，稍微有点工作量。</p><p>第一篇文章来自微信公众号，标题为：<code>哈工大社会计算与信息检索研究中心（HIT-SCIR）拟于7月20日在哈工大举办首届事理图谱研讨会</code>,访问网址为：https://mp.weixin.qq.com/s/9H7rxsPdo5S5trwz_CASZw，我们抽取出来的实体关系（带原文）如下：</p><blockquote><p>原文,s,p,o2017年10月，研究中心主任刘挺教授在中国计算机大会（CNCC）上正式提出事理图谱的概念，2018年9月，在研究中心丁效老师的主持下，研制出中文金融事理图谱1.0版本。,研究中心,老师,丁效<br />2017年10月，研究中心主任刘挺教授在中国计算机大会（CNCC）上正式提出事理图谱的概念，2018年9月，在研究中心丁效老师的主持下，研制出中文金融事理图谱1.0版本。,研究中心,教授,刘挺<br />2017年10月，研究中心主任刘挺教授在中国计算机大会（CNCC）上正式提出事理图谱的概念，2018年9月，在研究中心丁效老师的主持下，研制出中文金融事理图谱1.0版本。,研究中心,主任,刘挺<br />白硕（上海证券交易所前任总工程师，中科院计算所博导）,上海证券交易所,前任总工程师,白硕<br />荀恩东（北京语言大学信息学院院长）,北京语言大学信息学院,院长,荀恩东<br />赵军（中科院自动化所研究员）,中科院自动化所,研究员,赵军<br />吴华（百度技术委员会主席）,百度技术,主席,吴华<br />吴华（百度技术委员会主席）,百度技术,委员,吴华<br />宋阳秋（香港科技大学助理教授）,香港科技大学,助理教授,宋阳秋<br />李金龙（招商银行人工智能实验室负责人）,招商银行人工智能实验室,负责人,李金龙<br />李世奇（北京西亚财信人工智能科技有限责任公司CEO）,北京西亚财信人工智能科技有限责任公司,CEO,李世奇</p></blockquote><p>对于这篇文章，我们没有抽取出<code>李斌阳（国际关系学院副教授）</code>中的实体关系，并且<code>吴华（百度技术委员会主席</code>这句为抽取有误，正确的应为：百度技术委员会,主席,吴华。</p><p>将上述关系修改下，导入至Neo4J中，得到的实体关系图如下：</p><p><img src="/img/kg1_3.png" /></p><p>第二篇文章为凤凰网的新闻，标题为<code>南阳“水氢车”风波：一个中部城市的招商突围战</code>，访问网址为：<ahref="https://news.ifeng.com/c/7ntawxhCDvj">https://news.ifeng.com/c/7ntawxhCDvj</a>，我们抽取出来的实体关系（带原文）如下表：</p><blockquote><p>原文,s,p,o2017年，因巴铁所属企业北京华赢凯来资产管理有限公司涉嫌非法集资活动，北京警方将“巴铁之父”白丹青依法刑拘。,巴铁,之父,白丹青<br />南阳“神车”下线之后，界面新闻约访南阳市委书记张文深，被告知张文深与市长双双出差，工作人员并不确定张文深何时回到南阳，他的手机则处于忙线状态。,南阳,市委书记,张文深<br />南阳洛特斯新能源汽车有限公司实际控制人庞青年说，水氢汽车并未下线，媒体的报道使他措手不及。,南阳洛特斯新能源汽车有限公司,实际控制人,庞青年<br />从2006年开始，前湖北工业大学学者董仕节带领的团队开始研发一项车载铝合金水解制氢技术，并获得国家973前期研究项目和国家自然基金的支持。,湖北工业大学,学者,董仕节<br />南阳市高新区投资公司负责人尹召翼在接受央视采访时表示，庞青年经常拿“水氢”来混淆“水解制氢”的概念。,南阳市高新区投资公司,负责人,尹召翼<br />南阳市招商局招商二科科长赵怿接受界面新闻采访时表示，他只知道这个项目不是招商科引进的。,南阳市招商局招商二科,科长,赵怿<br />庞青年告诉界面新闻，南阳市高新区投资有限公司已经为他提供了9600万元，用途是南阳高新区投资有限公司给南阳市洛特斯新能源汽车有限公司的注册资金，占股49%。,南阳高新区投资有限公司,南阳市,洛特斯新能<br />曾先后在南阳市委党校、南阳市发改委任职的退休干部张一江（化名）说，“走工业突围道路的冲动在南阳早已有之，所以这几年的巴铁神车项目、加水就能跑的神车项目能被引进南阳，我觉得算不上奇怪。”,南阳市发改委,退休干部,张一江<br />以此次南阳神车项目为例，南阳市科技局局长张梅明确告诉界面新闻，庞青年的企业进入南阳时未有任何部门邀请科技局鉴别其“新能源技术”。,南阳市科技局,局长,张梅<br />官方报道显示，2012年6月18日，一位时任南阳市委主要领导在南阳宾馆会见了青年汽车董事局主席庞青年一行，双方就如何发挥自身优势，谋求合作共赢进行了交流，“南阳的发展需要大项目的带动和支撑，我们欢迎中国青年汽车集团这样有实力、有影响的大企业来南阳投资兴业。,青年汽车,董事局主席,庞青年<br />早在当年5月，在第十九届中国北京国际科技博览会上，时任南阳市副市长郑茂杰与巴铁科技发展有限公司总工程师宋有洲签署战略合作协议。,巴铁科技发展有限公司,总工程师,宋有洲<br />早在当年5月，在第十九届中国北京国际科技博览会上，时任南阳市副市长郑茂杰与巴铁科技发展有限公司总工程师宋有洲签署战略合作协议。,南阳市,副市长,郑茂杰</p></blockquote><p>对于这篇文章，我们没有抽取出一些关系，比如<code>南阳市发展和改革委员会主任乔长恩受访时承认，招商引入南阳洛斯特之前“掌握这个情况。”</code>等，并且<code>庞青年告诉界面新闻，南阳市高新区投资有限公司已经为他提供了9600万元，用途是南阳高新区投资有限公司给南阳市洛特斯新能源汽车有限公司的注册资金，占股49%。</code>这句为抽取有误，应当删除。</p><p>将上述关系修改下，导入至Neo4J中，得到的实体关系图如下：</p><p><img src="/img/kg1_4.png" /></p><p>最后一篇为长篇小说——著名作家路遥的《平凡的世界》第一部。利用我们的关系抽取模型，一共在该小说中抽取了169对实体关系，其中有效实体关系100对。由于我们在该小说中抽取的实体关系过多，因此只展示前10条原文及抽取的实体关系：</p><blockquote><p>原文,s,p,o每天来回二十里路，与他一块上学的金波和大队书记田福堂的儿子润生都有自行车，只有他是两条腿走路。,田福堂,儿子,润生<br />不过，他对润生的姐姐润叶倒怀有一种亲切的感情。,润生,姐姐,润叶<br />“金波是金俊海的小子。”,金俊海,小子,“金波<br />脑子里把前后村庄未嫁的女子一个个想过去，最后选定了双水村孙玉厚的大女子兰花。,双水村孙玉厚,大女子,兰花<br />玉亭是大队党支部委员、农田基建队队长、贫下中农管理学校委员会主任，一身三职，在村里也是一个人物。,贫下中农管理学校,主任,玉亭<br />玉亭是大队党支部委员、农田基建队队长、贫下中农管理学校委员会主任，一身三职，在村里也是一个人物。,农田基建队,队长,玉亭<br />玉亭是大队党支部委员、农田基建队队长、贫下中农管理学校委员会主任，一身三职，在村里也是一个人物。,大队,党支部委员,玉亭<br />会战总指挥是公社副主任徐治功，副总指挥是公社武装专干杨高虎。,公社,武装,杨高虎<br />会战总指挥是公社副主任徐治功，副总指挥是公社武装专干杨高虎。,公社,副主任,徐治功<br />这时候，双水村妇女主任贺凤英，正领着本村和外村的一些“铁姑娘”，忙碌地布置会场。,双水村,妇女主任,贺凤英<br />……</p></blockquote><p>将上述关系修改下，导入至Neo4J中，得到的实体关系图如下：</p><p><img src="/img/kg1_5.png" /></p><p><img src="/img/kg1_6.png" /></p><p><img src="/img/kg1_7.png" /></p><h3 id="总结">总结</h3><p>本次关系抽取仅仅作为笔者的一次尝试，在实际的应用中还存在着许多的不足之处，比如：</p><ul><li><p>对语料的标注，是否可以采用其他更好的办法；</p></li><li><p>作为通用模型，标注的数量还远远不够；</p></li><li><p>模型的选择方面，是否可以其他更好的模型；</p></li><li><p>对预测的结果，如何能更好地提取出三元组；</p></li><li><p>将三元组扫入至图数据库中，能否做到实体对齐，且能做一些实体关系的分析与推理。</p><p>本文用到的语料以及模型会在后续的文章中公开，希望大家能继续关注～</p><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注~</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>知识图谱</category>
      
    </categories>
    
    
    <tags>
      
      <tag>关系抽取</tag>
      
      <tag>知识图谱</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（十八）利用ALBERT提升模型预测速度的一次尝试</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%E5%88%A9%E7%94%A8ALBERT%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E9%80%9F%E5%BA%A6%E7%9A%84%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%E5%88%A9%E7%94%A8ALBERT%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E9%80%9F%E5%BA%A6%E7%9A%84%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="前沿">前沿</h3><p>在文章<ahref="https://percent4.github.io/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%E5%88%A9%E7%94%A8tensorflow-serving%E9%83%A8%E7%BD%B2kashgari%E6%A8%A1%E5%9E%8B/">NLP（十七）利用tensorflow-serving部署kashgari模型</a>中，笔者介绍了如何利用tensorflow-serving部署来部署深度模型模型，在那篇文章中，笔者利用kashgari模块实现了经典的BERT+Bi-LSTM+CRF模型结构，在标注了时间的文本语料（大约2000多个训练句子）中也达到了很好的识别效果，但是也存在着不足之处，那就是模型的预测时间过长，平均预测一个句子中的时间耗时约400毫秒，这种预测速度在生产环境或实际应用中是不能忍受的。</p><p>查看该模型的耗时原因，很大一部分原因在于BERT的调用。BERT是当下最火，知名度最高的预训练模型，虽然会使得模型的训练、预测耗时增加，但也是小样本语料下的最佳模型工具之一，因此，BERT在模型的架构上是不可缺少的。那么，该如何避免使用预训练模型带来的模型预测耗时过长的问题呢？</p><p>本文决定尝试使用ALBERT，来验证ALBERT在提升模型预测速度方面的应用，同时，也算是本人对于使用ALBERT的一次实战吧~</p><h3 id="albert简介">ALBERT简介</h3><p>我们不妨花一些时间来简单地了解一下ALBERT。ALBERT是最近一周才开源的预训练模型，其Github的网址为：https://github.com/brightmart/albert_zh，其论文可以参考网址：https://arxiv.org/pdf/1909.11942.pdf 。</p><p>根据ALBERT的Github介绍，ALBERT在海量中文语料上进行了预训练，模型的参数更少，效果更好。以albert_tiny_zh为例，其文件大小16M、参数为1.8M，模型大小仅为BERT的1/25，效果仅比BERT略差或者在某些NLP任务上更好。在本文的预训练模型中，将采用albert_tiny_zh。</p><h3 id="利用albert训练时间识别模型">利用ALBERT训练时间识别模型</h3><p>我们以Github中的bertNER为本次项目的代码模板，在该项目中，实现的模型为BERT+Bi-LSTM+CRF，我们将BERT替换为ALBERT，也就是说笔者的项目中模型为ALBERT+Bi-LSTM+CRF，同时替换bert文件夹的代码为alert_zh，替换预训练模型文件夹chinese_L-12_H-768_A-12（BERT中文预训练模型文件）为albert_tiny。当然，也需要修改一部分的项目源代码，来适应ALBERT的模型训练。</p><p>数据集采用笔者自己标注的时间语料，即标注了时间的句子，大概2000+句子，其中75%作为训练集（time.train文件），10%作为验证集（time.dev文件），15%作为测试集（time.test文件）。在这里笔者不打算给出具体的Python代码，因为工程比较复杂，有兴趣的额读者可以去查看该项目的Github地址：<ahref="https://github.com/percent4/ALBERT_4_Time_Recognition">https://github.com/percent4/ALBERT_4_Time_Recognition</a>。</p><p>一些模型的参数可以如下：</p><ul><li><p>预训练模型：ALBERT（tiny）</p></li><li><p>训练样本的最大字符长度： 128</p></li><li><p>batch_size: 8</p></li><li><p>epoch: 100</p></li><li><p>双向LSTM的个数：100</p><p>ALBERT的模型训练时间也会显著提高，我们耐心地等待模型训练完毕。在time.dev和time.test数据集上的表现如下表：</p></li></ul><table><thead><tr class="header"><th>数据集</th><th>precision</th><th>recall</th><th>f1</th></tr></thead><tbody><tr class="odd"><td>time.dev</td><td>81.41%</td><td>84.95%</td><td>83.14%</td></tr><tr class="even"><td>time.test</td><td>83.03%</td><td>86.38%</td><td>84.67%</td></tr></tbody></table><p>接着笔者利用训练好的模型，用tornado封装了一个模型预测的HTTP服务，具体的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> traceback<br><br><span class="hljs-keyword">import</span> tornado.httpserver<br><span class="hljs-keyword">import</span> tornado.ioloop<br><span class="hljs-keyword">import</span> tornado.options<br><span class="hljs-keyword">import</span> tornado.web<br><span class="hljs-keyword">from</span> tornado.options <span class="hljs-keyword">import</span> define, options<br><br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> create_model, get_logger<br><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> Model<br><span class="hljs-keyword">from</span> loader <span class="hljs-keyword">import</span> input_from_line<br><span class="hljs-keyword">from</span> train <span class="hljs-keyword">import</span> FLAGS, load_config, train<br><br><span class="hljs-comment"># 定义端口为12306</span><br>define(<span class="hljs-string">&quot;port&quot;</span>, default=<span class="hljs-number">12306</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;run on the given port&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br><span class="hljs-comment"># 导入模型</span><br>config = load_config(FLAGS.config_file)<br>logger = get_logger(FLAGS.log_file)<br><span class="hljs-comment"># limit GPU memory</span><br>tf_config = tf.ConfigProto()<br>tf_config.gpu_options.allow_growth = <span class="hljs-literal">False</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(FLAGS.map_file, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    tag_to_id, id_to_tag = pickle.load(f)<br><br>sess = tf.Session(config=tf_config)<br>model = create_model(sess, Model, FLAGS.ckpt_path, config, logger)<br><br><span class="hljs-comment"># 模型预测的HTTP接口</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResultHandler</span>(tornado.web.RequestHandler):<br>    <span class="hljs-comment"># post函数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">post</span>(<span class="hljs-params">self</span>):<br>        event = self.get_argument(<span class="hljs-string">&#x27;event&#x27;</span>)<br>        result = model.evaluate_line(sess, input_from_line(event, FLAGS.max_seq_len, tag_to_id), id_to_tag)<br>        self.write(json.dumps(result, ensure_ascii=<span class="hljs-literal">False</span>))<br><br><span class="hljs-comment"># 主函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    <span class="hljs-comment"># 开启tornado服务</span><br>    tornado.options.parse_command_line()<br>    <span class="hljs-comment"># 定义app</span><br>    app = tornado.web.Application(<br>            handlers=[<br>                      (<span class="hljs-string">r&#x27;/subj_extract&#x27;</span>, ResultHandler)<br>                     ], <span class="hljs-comment">#网页路径控制</span><br>           )<br>    http_server = tornado.httpserver.HTTPServer(app)<br>    http_server.listen(options.port)<br>    tornado.ioloop.IOLoop.instance().start()<br><br>main()<br></code></pre></td></tr></table></figure><h3 id="模型预测提速了吗">模型预测提速了吗？</h3><p>将模型预测封装成HTTP服务后，我们利用Postman来测试模型预测的效果和时间，如下图所示：</p><p><img src="/img/nlp18_1.png" /></p><p>可以看到，模型预测的结果正确，且耗时仅为38ms。</p><p>接着我们尝试多测试几个句子的测试，测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Daxing, Beijing</span><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> time<br><br>url = <span class="hljs-string">&#x27;http://localhost:12306/subj_extract&#x27;</span><br><br>texts = [<span class="hljs-string">&#x27;记者从国家发展改革委、商务部相关方面获悉，日前美方已决定对拟于10月1日实施的中国输美商品加征关税措施做出调整，中方支持相关企业从即日起按照市场化原则和WTO规则，自美采购一定数量大豆、猪肉等农产品，国务院关税税则委员会将对上述采购予以加征关税排除。&#x27;</span>,<br>         <span class="hljs-string">&#x27;据印度Zee新闻网站12日报道，亚洲新闻国际通讯社援引印度军方消息人士的话说，9月11日的对峙事件发生在靠近班公错北岸的实际控制线一带。&#x27;</span>,<br>         <span class="hljs-string">&#x27;儋州市决定，从9月开始，对城市低保、农村低保、特困供养人员、优抚对象、领取失业保险金人员、建档立卡未脱贫人口等低收入群体共3万多人，发放猪肉价格补贴，每人每月发放不低于100元补贴，以后发放标准，将根据猪肉价波动情况进行动态调整。&#x27;</span>,<br>         <span class="hljs-string">&#x27;9月11日，华为心声社区发布美国经济学家托马斯.弗里德曼在《纽约时报》上的专栏内容，弗里德曼透露，在与华为创始人任正非最近一次采访中，任正非表示华为愿意与美国司法部展开话题不设限的讨论。&#x27;</span>,<br>         <span class="hljs-string">&#x27;造血干细胞移植治疗白血病技术已日益成熟，然而，通过该方法同时治愈艾滋病目前还是一道全球尚在攻克的难题。&#x27;</span>,<br>         <span class="hljs-string">&#x27;英国航空事故调查局（AAIB）近日披露，今年2月6日一趟由德国法兰克福飞往墨西哥坎昆的航班上，因飞行员打翻咖啡使操作面板冒烟，导致飞机折返迫降爱尔兰。&#x27;</span>,<br>         <span class="hljs-string">&#x27;当地时间周四（9月12日），印度尼西亚财政部长英卓华（Sri Mulyani Indrawati）明确表示：特朗普的推特是风险之一。&#x27;</span>,<br>         <span class="hljs-string">&#x27;华中科技大学9月12日通过其官方网站发布通报称，9月2日，我校一硕士研究生不幸坠楼身亡。&#x27;</span>,<br>         <span class="hljs-string">&#x27;微博用户@ooooviki 9月12日下午公布发生在自己身上的惊悚遭遇：一个自称网警、名叫郑洋的人利用职务之便，查到她的完备的个人信息，包括但不限于身份证号、家庭地址、电话号码、户籍变动情况等，要求她做他女朋友。&#x27;</span>,<br>         <span class="hljs-string">&#x27;今天，贵阳取消了汽车限购，成为目前全国实行限购政策的9个省市中，首个取消限购的城市。&#x27;</span>,<br>         <span class="hljs-string">&#x27;据悉，与全球同步，中国区此次将于9月13日于iPhone官方渠道和京东正式开启预售，京东成Apple中国区唯一官方授权预售渠道。&#x27;</span>,<br>         <span class="hljs-string">&#x27;根据央行公布的数据，截至2019年6月末，存款类金融机构住户部门短期消费贷款规模为9.11万亿元，2019年上半年该项净增3293.19亿元，上半年增量看起来并不乐观。&#x27;</span>,<br>         <span class="hljs-string">&#x27;9月11日，一段拍摄浙江万里学院学生食堂的视频走红网络，视频显示该学校食堂不仅在用餐区域设置了可以看电影、比赛的大屏幕，还推出了“一人食”餐位。&#x27;</span>,<br>         <span class="hljs-string">&#x27;当日，在北京举行的2019年国际篮联篮球世界杯半决赛中，西班牙队对阵澳大利亚队。&#x27;</span>,<br>         ]<br><br>t1 = time.time()<br><span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>    data = &#123;<span class="hljs-string">&#x27;event&#x27;</span>: text.replace(<span class="hljs-string">&#x27; &#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)&#125;<br>    req = requests.post(url, data)<br>    <span class="hljs-keyword">if</span> req.status_code == <span class="hljs-number">200</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;原文：%s&#x27;</span> % text)<br>        res = json.loads(req.content)[<span class="hljs-string">&#x27;entities&#x27;</span>]<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;抽取结果：%s&#x27;</span> % <span class="hljs-built_in">str</span>([_[<span class="hljs-string">&#x27;word&#x27;</span>] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> res]))<br><br><br>t2 = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;一共耗时：%ss.&#x27;</span> % <span class="hljs-built_in">str</span>(<span class="hljs-built_in">round</span>(t2-t1, <span class="hljs-number">4</span>)))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs prolog">原文：记者从国家发展改革委、商务部相关方面获悉，日前美方已决定对拟于<span class="hljs-number">10</span>月<span class="hljs-number">1</span>日实施的中国输美商品加征关税措施做出调整，中方支持相关企业从即日起按照市场化原则和<span class="hljs-symbol">WTO</span>规则，自美采购一定数量大豆、猪肉等农产品，国务院关税税则委员会将对上述采购予以加征关税排除。<br>抽取结果：[<span class="hljs-string">&#x27;日前&#x27;</span>, <span class="hljs-string">&#x27;10月1日&#x27;</span>]<br>原文：据印度<span class="hljs-symbol">Zee</span>新闻网站<span class="hljs-number">12</span>日报道，亚洲新闻国际通讯社援引印度军方消息人士的话说，<span class="hljs-number">9</span>月<span class="hljs-number">11</span>日的对峙事件发生在靠近班公错北岸的实际控制线一带。<br>抽取结果：[<span class="hljs-string">&#x27;12日&#x27;</span>, <span class="hljs-string">&#x27;9月11日&#x27;</span>]<br>原文：儋州市决定，从<span class="hljs-number">9</span>月开始，对城市低保、农村低保、特困供养人员、优抚对象、领取失业保险金人员、建档立卡未脱贫人口等低收入群体共<span class="hljs-number">3</span>万多人，发放猪肉价格补贴，每人每月发放不低于<span class="hljs-number">100</span>元补贴，以后发放标准，将根据猪肉价波动情况进行动态调整。<br>抽取结果：[<span class="hljs-string">&#x27;9月&#x27;</span>]<br>原文：<span class="hljs-number">9</span>月<span class="hljs-number">11</span>日，华为心声社区发布美国经济学家托马斯.弗里德曼在《纽约时报》上的专栏内容，弗里德曼透露，在与华为创始人任正非最近一次采访中，任正非表示华为愿意与美国司法部展开话题不设限的讨论。<br>抽取结果：[<span class="hljs-string">&#x27;9月11日&#x27;</span>]<br>原文：造血干细胞移植治疗白血病技术已日益成熟，然而，通过该方法同时治愈艾滋病目前还是一道全球尚在攻克的难题。<br>抽取结果：[]<br>原文：英国航空事故调查局（<span class="hljs-symbol">AAIB</span>）近日披露，今年<span class="hljs-number">2</span>月<span class="hljs-number">6</span>日一趟由德国法兰克福飞往墨西哥坎昆的航班上，因飞行员打翻咖啡使操作面板冒烟，导致飞机折返迫降爱尔兰。<br>抽取结果：[<span class="hljs-string">&#x27;近日&#x27;</span>, <span class="hljs-string">&#x27;今年2月6日&#x27;</span>]<br>原文：当地时间周四（<span class="hljs-number">9</span>月<span class="hljs-number">12</span>日），印度尼西亚财政部长英卓华（<span class="hljs-symbol">Sri</span> <span class="hljs-symbol">Mulyani</span> <span class="hljs-symbol">Indrawati</span>）明确表示：特朗普的推特是风险之一。<br>抽取结果：[<span class="hljs-string">&#x27;当地时间周四（9月12日）&#x27;</span>]<br>原文：华中科技大学<span class="hljs-number">9</span>月<span class="hljs-number">12</span>日通过其官方网站发布通报称，<span class="hljs-number">9</span>月<span class="hljs-number">2</span>日，我校一硕士研究生不幸坠楼身亡。<br>抽取结果：[<span class="hljs-string">&#x27;9月12日&#x27;</span>, <span class="hljs-string">&#x27;9月2日&#x27;</span>]<br>原文：微博用户@ooooviki <span class="hljs-number">9</span>月<span class="hljs-number">12</span>日下午公布发生在自己身上的惊悚遭遇：一个自称网警、名叫郑洋的人利用职务之便，查到她的完备的个人信息，包括但不限于身份证号、家庭地址、电话号码、户籍变动情况等，要求她做他女朋友。<br>抽取结果：[<span class="hljs-string">&#x27;9月12日下午&#x27;</span>]<br>原文：今天，贵阳取消了汽车限购，成为目前全国实行限购政策的<span class="hljs-number">9</span>个省市中，首个取消限购的城市。<br>抽取结果：[<span class="hljs-string">&#x27;今天&#x27;</span>, <span class="hljs-string">&#x27;目前&#x27;</span>]<br>原文：据悉，与全球同步，中国区此次将于<span class="hljs-number">9</span>月<span class="hljs-number">13</span>日于iPhone官方渠道和京东正式开启预售，京东成<span class="hljs-symbol">Apple</span>中国区唯一官方授权预售渠道。<br>抽取结果：[<span class="hljs-string">&#x27;9月13日&#x27;</span>]<br>原文：根据央行公布的数据，截至<span class="hljs-number">2019</span>年<span class="hljs-number">6</span>月末，存款类金融机构住户部门短期消费贷款规模为<span class="hljs-number">9.11</span>万亿元，<span class="hljs-number">2019</span>年上半年该项净增<span class="hljs-number">3293.19</span>亿元，上半年增量看起来并不乐观。<br>抽取结果：[<span class="hljs-string">&#x27;2019年6月末&#x27;</span>, <span class="hljs-string">&#x27;2019年上半年&#x27;</span>, <span class="hljs-string">&#x27;上半年&#x27;</span>]<br>原文：<span class="hljs-number">9</span>月<span class="hljs-number">11</span>日，一段拍摄浙江万里学院学生食堂的视频走红网络，视频显示该学校食堂不仅在用餐区域设置了可以看电影、比赛的大屏幕，还推出了“一人食”餐位。<br>抽取结果：[<span class="hljs-string">&#x27;9月11日&#x27;</span>]<br>原文：当日，在北京举行的<span class="hljs-number">2019</span>年国际篮联篮球世界杯半决赛中，西班牙队对阵澳大利亚队。<br>抽取结果：[<span class="hljs-string">&#x27;当日&#x27;</span>, <span class="hljs-string">&#x27;2019年&#x27;</span>]<br>一共耗时：<span class="hljs-number">0.5314</span>s.<br></code></pre></td></tr></table></figure><p>可以看到，对于测试的14个句子，识别的准确率很高，且预测耗时为531ms，平均每个话的预测时间不超过40ms。相比较而言，文章<ahref="https://percent4.github.io/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%E5%88%A9%E7%94%A8tensorflow-serving%E9%83%A8%E7%BD%B2kashgari%E6%A8%A1%E5%9E%8B/">NLP（十七）利用tensorflow-serving部署kashgari模型</a>中的模型，该模型的预测时间为每句话1秒多，模型预测的速度为带ALBERT模型的25倍多。</p><p>因此，ALBERT模型确实提升了模型预测的时间，而且效果非常显著。</p><h3 id="总结">总结</h3><p>由于ALBERT开源不到一周，而且笔者的学识、才能有限，因此，在代码方面可能会存在不足。但是，作为一次使用ALBERT的历经，希望能够与大家分享。</p><p>本文绝不是上述项目代码的抄袭和堆砌，该项目融入了笔者自己的思考，希望不要被误解为是抄袭。笔者使用上述的bertNER和ALBERT，只是为了验证ALBERT在模型预测耗时方面的提速效果，而事实是，ALBERT确实给我带来了很大惊喜，感受源代码作者们～</p><p>最后，附上本文中笔者项目的Github地址：<ahref="https://github.com/percent4/ALBERT_4_Time_Recognition">https://github.com/percent4/ALBERT_4_Time_Recognition</a>。</p><p>众里寻他千百度。蓦然回首，那人却在，灯火阑珊处。</p><h3 id="参考文献">参考文献</h3><ol type="1"><li>超小型BERT中文版横空出世！模型只有16M，训练速度提升10倍：https://mp.weixin.qq.com/s/eVlNpejrxdE4ctDTBM-fiA</li><li>ALBERT的Github地址：https://github.com/brightmart/albert_zh</li><li>bertNER项目的Github地址：https://github.com/yumath/bertNER</li><li>NLP（十七）利用tensorflow-serving部署kashgari模型：https://www.cnblogs.com/jclian91/p/11526547.html</li></ol>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>ALBERT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（十七）利用tensorflow-serving部署kashgari模型</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%E5%88%A9%E7%94%A8tensorflow-serving%E9%83%A8%E7%BD%B2kashgari%E6%A8%A1%E5%9E%8B/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%E5%88%A9%E7%94%A8tensorflow-serving%E9%83%A8%E7%BD%B2kashgari%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在文章<ahref="https://percent4.github.io/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%E8%AE%A9%E6%A8%A1%E5%9E%8B%E6%9D%A5%E5%91%8A%E8%AF%89%E4%BD%A0%E6%96%87%E6%9C%AC%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4/">NLP（十五）让模型来告诉你文本中的时间</a>中，我们已经学会了如何利用kashgari模块来完成序列标注模型的训练与预测，在本文中，我们将会了解如何tensorflow-serving来部署模型。</p><p>在kashgari的官方文档中，已经有如何利用tensorflow-serving来部署模型的说明了，网址为：<ahref="https://kashgari.bmio.net/advance-use/tensorflow-serving/">https://kashgari.bmio.net/advance-use/tensorflow-serving/</a>。</p><p>下面，本文将介绍tensorflow-serving以及如何利用tensorflow-serving来部署kashgari的模型。</p><h3 id="tensorflow-serving">tensorflow-serving</h3><p>TensorFlow Serving 是一个用于机器学习模型 serving的高性能开源库。它可以将训练好的机器学习模型部署到线上，使用 gRPC作为接口接受外部调用。更加让人眼前一亮的是，它支持模型热更新与自动模型版本管理。这意味着一旦部署TensorFlow Serving后，你再也不需要为线上服务操心，只需要关心你的线下模型训练。</p><p>TensorFlowServing可以方便我们部署TensorFlow模型，本文将使用TensorFlowServing的Docker镜像来使用TensorFlow Serving，安装的命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker pull tensorflow/serving<br></code></pre></td></tr></table></figure><h3 id="工程实践">工程实践</h3><p>本项目将演示如何利用tensorflow/serving来部署kashgari中的模型，项目结构如下：</p><p><img src="/img/nlp17_1.png" /></p><p>本项目的data来自之前笔者标注的时间数据集，即标注出文本中的时间，采用BIO标注系统。chinese_wwm_ext文件夹为哈工大的预训练模型文件。</p><p>model_train.py为模型训练的代码，主要功能是完成时间序列标注模型的训练，完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># time: 2019-09-12</span><br><span class="hljs-comment"># place: Huangcun Beijing</span><br><br><span class="hljs-keyword">import</span> kashgari<br><span class="hljs-keyword">from</span> kashgari <span class="hljs-keyword">import</span> utils<br><span class="hljs-keyword">from</span> kashgari.corpus <span class="hljs-keyword">import</span> DataReader<br><span class="hljs-keyword">from</span> kashgari.embeddings <span class="hljs-keyword">import</span> BERTEmbedding<br><span class="hljs-keyword">from</span> kashgari.tasks.labeling <span class="hljs-keyword">import</span> BiLSTM_CRF_Model<br><br><span class="hljs-comment"># 模型训练</span><br><br>train_x, train_y = DataReader().read_conll_format_file(<span class="hljs-string">&#x27;./data/time.train&#x27;</span>)<br>valid_x, valid_y = DataReader().read_conll_format_file(<span class="hljs-string">&#x27;./data/time.dev&#x27;</span>)<br>test_x, test_y = DataReader().read_conll_format_file(<span class="hljs-string">&#x27;./data/time.test&#x27;</span>)<br><br>bert_embedding = BERTEmbedding(<span class="hljs-string">&#x27;chinese_wwm_ext_L-12_H-768_A-12&#x27;</span>,<br>                               task=kashgari.LABELING,<br>                               sequence_length=<span class="hljs-number">128</span>)<br><br>model = BiLSTM_CRF_Model(bert_embedding)<br><br>model.fit(train_x, train_y, valid_x, valid_y, batch_size=<span class="hljs-number">16</span>, epochs=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># Save model</span><br>utils.convert_to_saved_model(model,<br>                             model_path=<span class="hljs-string">&#x27;saved_model/time_entity&#x27;</span>,<br>                             version=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>运行该代码，模型训练完后会生成saved_model文件夹，里面含有模型训练好后的文件，方便我们利用tensorflow/serving进行部署。接着我们利用tensorflow/serving来完成模型的部署，命令如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">docker <span class="hljs-built_in">run</span> -t --rm -p 8501:8501 -v <span class="hljs-string">&quot;/Users/jclian/PycharmProjects/kashgari_tf_serving/saved_model:/models/&quot;</span> -e <span class="hljs-attribute">MODEL_NAME</span>=time_entity tensorflow/serving<br></code></pre></td></tr></table></figure><p>其中需要注意该模型所在的路径，路径需要写完整路径，以及模型的名称（MODEL_NAME），这在训练代码（train.py）中已经给出（saved_model/time_entity）。</p><p>接着我们使用tornado来搭建HTTP服务，帮助我们方便地进行模型预测，runServer.py的完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> kashgari <span class="hljs-keyword">import</span> utils<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> model_predict <span class="hljs-keyword">import</span> get_predict<br><br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> tornado.httpserver<br><span class="hljs-keyword">import</span> tornado.ioloop<br><span class="hljs-keyword">import</span> tornado.options<br><span class="hljs-keyword">import</span> tornado.web<br><span class="hljs-keyword">from</span> tornado.options <span class="hljs-keyword">import</span> define, options<br><span class="hljs-keyword">import</span> traceback<br><br><span class="hljs-comment"># tornado高并发</span><br><span class="hljs-keyword">import</span> tornado.web<br><span class="hljs-keyword">import</span> tornado.gen<br><span class="hljs-keyword">import</span> tornado.concurrent<br><span class="hljs-keyword">from</span> concurrent.futures <span class="hljs-keyword">import</span> ThreadPoolExecutor<br><br><span class="hljs-comment"># 定义端口为12333</span><br>define(<span class="hljs-string">&quot;port&quot;</span>, default=<span class="hljs-number">16016</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;run on the given port&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br><br><span class="hljs-comment"># 模型预测</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ModelPredictHandler</span>(tornado.web.RequestHandler):<br>    executor = ThreadPoolExecutor(max_workers=<span class="hljs-number">5</span>)<br><br>    <span class="hljs-comment"># get 函数</span><br><span class="hljs-meta">    @tornado.gen.coroutine</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self</span>):<br>        origin_text = self.get_argument(<span class="hljs-string">&#x27;text&#x27;</span>)<br>        result = <span class="hljs-keyword">yield</span> self.function(origin_text)<br>        self.write(json.dumps(result, ensure_ascii=<span class="hljs-literal">False</span>))<br><br><span class="hljs-meta">    @tornado.concurrent.run_on_executor</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">function</span>(<span class="hljs-params">self, text</span>):<br>        <span class="hljs-keyword">try</span>:<br>            text = text.replace(<span class="hljs-string">&#x27; &#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>            x = [_ <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> text]<br><br>            <span class="hljs-comment"># Pre-processor data</span><br>            processor = utils.load_processor(model_path=<span class="hljs-string">&#x27;saved_model/time_entity/1&#x27;</span>)<br>            tensor = processor.process_x_dataset([x])<br><br>            <span class="hljs-comment"># only for bert Embedding</span><br>            tensor = [&#123;<br>                <span class="hljs-string">&quot;Input-Token:0&quot;</span>: i.tolist(),<br>                <span class="hljs-string">&quot;Input-Segment:0&quot;</span>: np.zeros(i.shape).tolist()<br>            &#125; <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tensor]<br><br>            <span class="hljs-comment"># predict</span><br>            r = requests.post(<span class="hljs-string">&quot;http://localhost:8501/v1/models/time_entity:predict&quot;</span>, json=&#123;<span class="hljs-string">&quot;instances&quot;</span>: tensor&#125;)<br>            preds = r.json()[<span class="hljs-string">&#x27;predictions&#x27;</span>]<br><br>            <span class="hljs-comment"># Convert result back to labels</span><br>            labels = processor.reverse_numerize_label_sequences(np.array(preds).argmax(-<span class="hljs-number">1</span>))<br><br>            entities = get_predict(<span class="hljs-string">&#x27;TIME&#x27;</span>, text, labels[<span class="hljs-number">0</span>])<br><br>            <span class="hljs-keyword">return</span> entities<br><br>        <span class="hljs-keyword">except</span> Exception:<br>            self.write(traceback.format_exc().replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&lt;br&gt;&#x27;</span>))<br><br><br><span class="hljs-comment"># get请求</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">HelloHandler</span>(tornado.web.RequestHandler):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self</span>):<br>        self.write(<span class="hljs-string">&#x27;Hello from lmj from Daxing Beijing!&#x27;</span>)<br><br><br><span class="hljs-comment"># 主函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    <span class="hljs-comment"># 开启tornado服务</span><br>    tornado.options.parse_command_line()<br>    <span class="hljs-comment"># 定义app</span><br>    app = tornado.web.Application(<br>            handlers=[(<span class="hljs-string">r&#x27;/model_predict&#x27;</span>, ModelPredictHandler),<br>                      (<span class="hljs-string">r&#x27;/hello&#x27;</span>, HelloHandler),<br>                      ], <span class="hljs-comment">#网页路径控制</span><br>          )<br>    http_server = tornado.httpserver.HTTPServer(app)<br>    http_server.listen(options.port)<br>    tornado.ioloop.IOLoop.instance().start()<br><br>main()<br></code></pre></td></tr></table></figure><p>我们定义了tornado封装HTTP服务来进行模型预测，运行该脚本，启动模型预测的HTTP服务。接着我们再使用Python脚本才测试下模型的预测效果以及预测时间，预测的代码脚本的完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> requests<br><br>t1 = time.time()<br>texts = [<span class="hljs-string">&#x27;记者从国家发展改革委、商务部相关方面获悉，日前美方已决定对拟于10月1日实施的中国输美商品加征关税措施做出调整，中方支持相关企业从即日起按照市场化原则和WTO规则，自美采购一定数量大豆、猪肉等农产品，国务院关税税则委员会将对上述采购予以加征关税排除。&#x27;</span>,<br>         <span class="hljs-string">&#x27;据印度Zee新闻网站12日报道，亚洲新闻国际通讯社援引印度军方消息人士的话说，9月11日的对峙事件发生在靠近班公错北岸的实际控制线一带。&#x27;</span>,<br>         <span class="hljs-string">&#x27;儋州市决定，从9月开始，对城市低保、农村低保、特困供养人员、优抚对象、领取失业保险金人员、建档立卡未脱贫人口等低收入群体共3万多人，发放猪肉价格补贴，每人每月发放不低于100元补贴，以后发放标准，将根据猪肉价波动情况进行动态调整。&#x27;</span>,<br>         <span class="hljs-string">&#x27;9月11日，华为心声社区发布美国经济学家托马斯.弗里德曼在《纽约时报》上的专栏内容，弗里德曼透露，在与华为创始人任正非最近一次采访中，任正非表示华为愿意与美国司法部展开话题不设限的讨论。&#x27;</span>,<br>         <span class="hljs-string">&#x27;造血干细胞移植治疗白血病技术已日益成熟，然而，通过该方法同时治愈艾滋病目前还是一道全球尚在攻克的难题。&#x27;</span>,<br>         <span class="hljs-string">&#x27;英国航空事故调查局（AAIB）近日披露，今年2月6日一趟由德国法兰克福飞往墨西哥坎昆的航班上，因飞行员打翻咖啡使操作面板冒烟，导致飞机折返迫降爱尔兰。&#x27;</span>,<br>         <span class="hljs-string">&#x27;当地时间周四（9月12日），印度尼西亚财政部长英卓华（Sri Mulyani Indrawati）明确表示：特朗普的推特是风险之一。&#x27;</span>,<br>         <span class="hljs-string">&#x27;华中科技大学9月12日通过其官方网站发布通报称，9月2日，我校一硕士研究生不幸坠楼身亡。&#x27;</span>,<br>         <span class="hljs-string">&#x27;微博用户@ooooviki 9月12日下午公布发生在自己身上的惊悚遭遇：一个自称网警、名叫郑洋的人利用职务之便，查到她的完备的个人信息，包括但不限于身份证号、家庭地址、电话号码、户籍变动情况等，要求她做他女朋友。&#x27;</span>,<br>         <span class="hljs-string">&#x27;今天，贵阳取消了汽车限购，成为目前全国实行限购政策的9个省市中，首个取消限购的城市。&#x27;</span>,<br>         <span class="hljs-string">&#x27;据悉，与全球同步，中国区此次将于9月13日于iPhone官方渠道和京东正式开启预售，京东成Apple中国区唯一官方授权预售渠道。&#x27;</span>,<br>         <span class="hljs-string">&#x27;根据央行公布的数据，截至2019年6月末，存款类金融机构住户部门短期消费贷款规模为9.11万亿元，2019年上半年该项净增3293.19亿元，上半年增量看起来并不乐观。&#x27;</span>,<br>         <span class="hljs-string">&#x27;9月11日，一段拍摄浙江万里学院学生食堂的视频走红网络，视频显示该学校食堂不仅在用餐区域设置了可以看电影、比赛的大屏幕，还推出了“一人食”餐位。&#x27;</span>,<br>         <span class="hljs-string">&#x27;当日，在北京举行的2019年国际篮联篮球世界杯半决赛中，西班牙队对阵澳大利亚队。&#x27;</span>,<br>         ]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(texts))<br><br><span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>    url = <span class="hljs-string">&#x27;http://localhost:16016/model_predict?text=%s&#x27;</span> % text<br>    req = requests.get(url)<br>    <span class="hljs-built_in">print</span>(json.loads(req.content))<br><br>t2 = time.time()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">round</span>(t2-t1, <span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure><p>运行该代码，输出的结果如下：（预测文本中的时间）</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs prolog">一共预测<span class="hljs-number">14</span>个句子。<br>[<span class="hljs-string">&#x27;日前&#x27;</span>, <span class="hljs-string">&#x27;10月1日&#x27;</span>, <span class="hljs-string">&#x27;即日&#x27;</span>]<br>[<span class="hljs-string">&#x27;12日&#x27;</span>, <span class="hljs-string">&#x27;9月11日&#x27;</span>]<br>[<span class="hljs-string">&#x27;9月&#x27;</span>]<br>[<span class="hljs-string">&#x27;9月11日&#x27;</span>]<br>[]<br>[<span class="hljs-string">&#x27;近日&#x27;</span>, <span class="hljs-string">&#x27;今年2月6日&#x27;</span>]<br>[<span class="hljs-string">&#x27;当地时间周四（9月12日）&#x27;</span>]<br>[<span class="hljs-string">&#x27;9月12日&#x27;</span>, <span class="hljs-string">&#x27;9月2日&#x27;</span>]<br>[<span class="hljs-string">&#x27;9月12日下午&#x27;</span>]<br>[<span class="hljs-string">&#x27;今天&#x27;</span>, <span class="hljs-string">&#x27;目前&#x27;</span>]<br>[<span class="hljs-string">&#x27;9月13日&#x27;</span>]<br>[<span class="hljs-string">&#x27;2019年6月末&#x27;</span>, <span class="hljs-string">&#x27;2019年上半年&#x27;</span>, <span class="hljs-string">&#x27;上半年&#x27;</span>]<br>[<span class="hljs-string">&#x27;9月11日&#x27;</span>]<br>[<span class="hljs-string">&#x27;当日&#x27;</span>, <span class="hljs-string">&#x27;2019年&#x27;</span>]<br>预测耗时: <span class="hljs-number">15.1085</span>s.<br></code></pre></td></tr></table></figure><p>模型预测的效果还是不错的，但平均每句话的预测时间为1秒多，模型预测时间还是稍微偏长，后续笔者将会研究如何缩短模型预测的时间。</p><h3 id="总结">总结</h3><p>本项目主要是介绍了如何利用tensorflow-serving部署kashgari模型，该项目已经上传至github，地址为：<ahref="https://github.com/percent4/tensorflow-serving_4_kashgari">https://github.com/percent4/tensorflow-serving_4_kashgari</a>。</p><p>至于如何缩短模型预测的时间，笔者还需要再继续研究，欢迎大家关注～</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（十六）轻松上手文本分类</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%E8%BD%BB%E6%9D%BE%E4%B8%8A%E6%89%8B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%E8%BD%BB%E6%9D%BE%E4%B8%8A%E6%89%8B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="背景介绍">背景介绍</h3><p>文本分类是NLP中的常见的重要任务之一，它的主要功能就是将输入的文本以及文本的类别训练出一个模型，使之具有一定的泛化能力，能够对新文本进行较好地预测。它的应用很广泛，在很多领域发挥着重要作用，例如垃圾邮件过滤、舆情分析以及新闻分类等。</p><p>现阶段的文本分类模型频出，种类繁多，花样百变，既有机器学习中的朴素贝叶斯模型、SVM等，也有深度学习中的各种模型，比如经典的CNN,RNN，以及它们的变形，如CNN-LSTM，还有各种高大上的Attention模型。</p><p>无疑，文本分类是一个相对比较成熟的任务，我们尽可以选择自己喜欢的模型来完成该任务。本文以kashgari-tf为例，它能够支持各种文本分类模型，比如BiLSTM，CNN_LSTM，AVCNN等，且对预训练模型，比如BERT的支持较好，它能让我们轻松地完成文本分类任务。</p><p>下面，让我们一起走进文本分类的世界，分分钟搞定textclassification！</p><h3 id="项目">项目</h3><p>首先，我们需要找一份数据作为例子。我们选择THUCNews，THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19GB），均为UTF-8纯文本格式。我们在原始新浪新闻分类体系的基础上，从中选择10个候选分类类别：体育、娱乐、家居、房产、教育、时尚、时政、游戏、科技、财经。</p><p>数据总量一共为6.5万条，其中训练集数据5万条，每个类别5000条，验证集数据0.5万条，每个类别500条，测试集数据1万条，每个类别1000条。笔者已将数据放在Github上，读者可以在最后的总结中找到。</p><p>项目结构，如下图：</p><p><img src="/img/nlp16_1.png" /></p><p>接着，我们尝试着利用kashgari-tf来训练一个文本分类模型，其中模型我们采用CNN-LSTM，完整的Python代码（text_classification_model_train.py）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># time: 2019-08-13 11:16</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><br><span class="hljs-keyword">from</span> kashgari.tasks.classification <span class="hljs-keyword">import</span> CNN_LSTM_Model<br><br><span class="hljs-comment"># 获取数据集</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>(<span class="hljs-params">data_type</span>):<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./data/cnews.%s.txt&#x27;</span> % data_type, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        content = [_.strip() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> f.readlines() <span class="hljs-keyword">if</span> _.strip()]<br><br>    x, y = [], []<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> content:<br>        label, text = line.split(maxsplit=<span class="hljs-number">1</span>)<br>        y.append(label)<br>        x.append([_ <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> text])<br><br>    <span class="hljs-keyword">return</span> x, y<br><br><span class="hljs-comment"># 获取数据</span><br>train_x, train_y = load_data(<span class="hljs-string">&#x27;train&#x27;</span>)<br>valid_x, valid_y = load_data(<span class="hljs-string">&#x27;val&#x27;</span>)<br>test_x, test_y = load_data(<span class="hljs-string">&#x27;test&#x27;</span>)<br><br><span class="hljs-comment"># 训练模型</span><br>model = CNN_LSTM_Model()<br>model.fit(train_x, train_y, valid_x, valid_y, batch_size=<span class="hljs-number">16</span>, epochs=<span class="hljs-number">5</span>)<br><br><span class="hljs-comment"># 评估模型</span><br>model.evaluate(test_x, test_y)<br><br><span class="hljs-comment"># 保存模型</span><br>model.save(<span class="hljs-string">&#x27;text_classification_model&#x27;</span>)<br></code></pre></td></tr></table></figure><p>输出的模型结果如下：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_<br><span class="hljs-section">Layer (type)                 Output Shape              Param #</span><br><span class="hljs-section">=================================================================</span><br>input (InputLayer)           (None, 2544)              0<br><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_<br>layer<span class="hljs-emphasis">_embedding (Embedding)  (None, 2544, 100)         553200</span><br><span class="hljs-emphasis"><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_</span><br>conv1d (Conv1D)              (None, 2544, 32)          9632<br><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_<br>max<span class="hljs-emphasis">_pooling1d (MaxPooling1D) (None, 1272, 32)          0</span><br><span class="hljs-emphasis"><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_</span><br>cu<span class="hljs-emphasis">_dnnlstm (CuDNNLSTM)       (None, 100)               53600</span><br><span class="hljs-emphasis"><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_</span><br><span class="hljs-section">dense (Dense)                (None, 10)                1010</span><br><span class="hljs-section">=================================================================</span><br>Total params: 617,442<br>Trainable params: 617,442<br>Non-trainable params: 0<br></code></pre></td></tr></table></figure><p>设定模型训练次数为5个epoch，batch_size为16。模型训练完后，在训练集、验证集上的结果如下：</p><table><thead><tr class="header"><th>数据集</th><th>accuracy</th><th>loss</th></tr></thead><tbody><tr class="odd"><td>训练集</td><td>0.9661</td><td>0.1184</td></tr><tr class="even"><td>验证集</td><td>0.9204</td><td>0.2567</td></tr></tbody></table><p>在测试集上的结果如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs yaml">             <span class="hljs-string">precision</span>    <span class="hljs-string">recall</span>  <span class="hljs-string">f1-score</span>   <span class="hljs-string">support</span><br><br>          <span class="hljs-string">体育</span>     <span class="hljs-number">0.9852</span>    <span class="hljs-number">0.9970</span>    <span class="hljs-number">0.9911</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">娱乐</span>     <span class="hljs-number">0.9938</span>    <span class="hljs-number">0.9690</span>    <span class="hljs-number">0.9813</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">家居</span>     <span class="hljs-number">0.9384</span>    <span class="hljs-number">0.8830</span>    <span class="hljs-number">0.9098</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">房产</span>     <span class="hljs-number">0.9490</span>    <span class="hljs-number">0.9680</span>    <span class="hljs-number">0.9584</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">教育</span>     <span class="hljs-number">0.9650</span>    <span class="hljs-number">0.8820</span>    <span class="hljs-number">0.9216</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">时尚</span>     <span class="hljs-number">0.9418</span>    <span class="hljs-number">0.9710</span>    <span class="hljs-number">0.9562</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">时政</span>     <span class="hljs-number">0.9732</span>    <span class="hljs-number">0.9450</span>    <span class="hljs-number">0.9589</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">游戏</span>     <span class="hljs-number">0.9454</span>    <span class="hljs-number">0.9700</span>    <span class="hljs-number">0.9576</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">科技</span>     <span class="hljs-number">0.8910</span>    <span class="hljs-number">0.9560</span>    <span class="hljs-number">0.9223</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">财经</span>     <span class="hljs-number">0.9566</span>    <span class="hljs-number">0.9920</span>    <span class="hljs-number">0.9740</span>      <span class="hljs-number">1000</span><br><br>    <span class="hljs-string">accuracy</span>                         <span class="hljs-number">0.9533</span>     <span class="hljs-number">10000</span><br>   <span class="hljs-string">macro</span> <span class="hljs-string">avg</span>     <span class="hljs-number">0.9539</span>    <span class="hljs-number">0.9533</span>    <span class="hljs-number">0.9531</span>     <span class="hljs-number">10000</span><br><span class="hljs-string">weighted</span> <span class="hljs-string">avg</span>     <span class="hljs-number">0.9539</span>    <span class="hljs-number">0.9533</span>    <span class="hljs-number">0.9531</span>     <span class="hljs-number">10000</span><br></code></pre></td></tr></table></figure><p>总的来说，上述模型训练的效果还是很不错的。接下来，是考验模型的预测能力的时刻了，看看它是否具体文本分类的泛化能力。</p><h3 id="测试">测试</h3><p>我们已经有了训练好的模型<code>text_classification_model</code>，接着让我们利用该模型来对新的数据进行预测，预测的代码（model_predict.py）如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># time: 2019-08-14 00:21</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><br><span class="hljs-keyword">import</span> kashgari<br><br><span class="hljs-comment"># 加载模型</span><br>loaded_model = kashgari.utils.load_model(<span class="hljs-string">&#x27;text_classification_model&#x27;</span>)<br><br>text = <span class="hljs-string">&#x27;华夏幸福成立于 1998 年，前身为廊坊市华夏房地产开发有限公司，初始注册资本 200 万元，其中王文学出资 160 万元，廊坊市融通物资贸易有限公司出资 40 万元，后经多次股权转让和增资，公司于 2007 年整体改制为股份制公司，2011 年完成借壳上市。&#x27;</span><br><br>x = [[_ <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> text]]<br><br>label = loaded_model.predict(x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;预测分类:%s&#x27;</span> % label)<br></code></pre></td></tr></table></figure><p>以下是测试结果：</p><blockquote><p>原文1: 华夏幸福成立于 1998年，前身为廊坊市华夏房地产开发有限公司，初始注册资本 200万元，其中王文学出资 160 万元，廊坊市融通物资贸易有限公司出资 40万元，后经多次股权转让和增资，公司于 2007 年整体改制为股份制公司，2011年完成借壳上市。 分类结果：预测分类:['财经']</p></blockquote><blockquote><p>原文2:现今常见的短袖衬衫大致上可以分为：夏威夷衬衫、古巴衬衫、保龄球衫，三者之间虽有些微分别，但其实有些时候，一件衬衫也可能包含了多种款式的特色。而‘古巴（领）衬衫’最显而易见的特点在于‘领口’，通常会设计为V领，且呈现微微的外翻，也因此缺少衬衫领口常见的‘第一颗钮扣’，衣服到领子的剪裁为一体成形，整体较宽松舒适。分类结果：预测分类:['时尚']</p></blockquote><blockquote><p>原文3:周琦2014年加盟新疆广汇篮球俱乐部，当年就代表俱乐部青年队接连拿下全国篮球青年联赛冠军和全国俱乐部青年联赛冠军。升入一队后，周琦2016年随队出战第25届亚冠杯，获得冠军。2016-2017赛季，周琦为新疆广汇队夺得队史首座总冠军奖杯立下汗马功劳，他在总决赛中带伤出战，更是传为佳话。分类结果：预测分类:['体育']</p></blockquote><blockquote><p>原文4:周杰伦[微博]监制赛车电影《叱咤风云》13日释出花絮导演篇，不仅真实赛车竞速画面大量曝光，几十辆百万赛车在国际专业赛道、山路飙速，场面浩大震撼，更揭开不少现场拍摄的幕后画面。监制周杰伦在现场与导演讨论剧本、范逸臣[微博]与高英轩大打出手、甚至有眼尖网友发现在花絮中闪过“男神”李玉玺[微博]的画面。分类结果：预测分类:['娱乐']</p></blockquote><blockquote><p>原文5:北京时间8月13日上午消息，据《韩国先驱报》网站报道，近日美国知识产权所有者协会（Intellectual Property Owners Association）发布的一份报告显示，在获得的美国专利数量方面，IBM、微软和通用电气等美国企业名列前茅，排在后面的韩国科技巨头三星、LG与之竞争激烈。分类结果：预测分类:['科技']</p></blockquote><h3 id="总结">总结</h3><p>虽然我们上述测试的文本分类效果还不错，但也存在着一些分类错误的情况。</p><p>本文讲述了如何利用kashgari-tf模块来快速地搭建文本分类任务，其实，也没那么难！</p><p>本文代码和数据及已上传至Github, 网址为： <ahref="https://github.com/percent4/cnews_text_classification">https://github.com/percent4/cnews_text_classification</a></p><blockquote><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape），欢迎大家关注~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Kashgari</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（十五）让模型来告诉你文本中的时间</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%E8%AE%A9%E6%A8%A1%E5%9E%8B%E6%9D%A5%E5%91%8A%E8%AF%89%E4%BD%A0%E6%96%87%E6%9C%AC%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%E8%AE%A9%E6%A8%A1%E5%9E%8B%E6%9D%A5%E5%91%8A%E8%AF%89%E4%BD%A0%E6%96%87%E6%9C%AC%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="背景介绍">背景介绍</h3><p>在文章<ahref="https://percent4.github.io/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%E4%BB%8E%E6%96%87%E6%9C%AC%E4%B8%AD%E6%8F%90%E5%8F%96%E6%97%B6%E9%97%B4/">NLP入门（十一）从文本中提取时间</a>中，笔者演示了如何利用分词、词性标注的方法从文本中获取时间。当时的想法比较简单快捷，只是利用了词性标注这个功能而已，因此，在某些地方，时间的识别效果并不太好。比如以下的两个例子：</p><p>原文1:</p><blockquote><p>苏北大量农村住房建于上世纪80年代之前。去年9月，江苏省决定全面改善苏北农民住房条件，计划3年内改善30万户，作为决胜全面建成小康社会补短板的重要举措。</p></blockquote><p>用笔者之前的代码，提取的时间结果为：</p><blockquote><p>提取时间： ['去年9月']</p></blockquote><p>但实际上，我们提取的时间应该是：</p><blockquote><p>上世纪80年代之前， 去年9月，3年内</p></blockquote><p>原文2:</p><blockquote><p>南宋绍兴十年，金分兵两路向陕西和河南大举进攻，在很快夺回了河南、陕西之后，又率大军向淮南大举进攻。</p></blockquote><p>用笔者之前的代码，提取的时间结果为：</p><blockquote><p>提取时间： ['南宋']</p></blockquote><p>但实际上，我们提取的时间应该是：</p><blockquote><p>南宋绍兴十年</p></blockquote><p>因此，利用简单的词性标注功能来提取文本中的时间会存在漏提、错提的情况，鉴于此，笔者想到能否用深度学习模型来实现文本中的时间提取呢？</p><p>该功能类似于命名实体识别（NER）功能，只不过NER是识别文本中的人名、地名、组织机构名，而我们这次需要识别文本中的时间。但是，它们背后的算法原理都是一样的，即采用序列标注模型来解决。</p><h3 id="项目">项目</h3><p>在文章<ahref="https://percent4.github.io/2023/07/08/NLP%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%E8%87%AA%E5%88%B6%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E5%B9%B3%E5%8F%B0/">NLP（十四）自制序列标注平台</a>中，笔者提出了一种自制的序列标注平台，利用该标注平台，笔者从新闻网站中标注了大约2000份语料，标注出文本中的时间，其中75%作为训练集（time.train文件），10%作为验证集（time.dev文件），15%作为测试集（time.test文件）。</p><p>虽然我们现在已经有了深度学习框架方便我们来训练模型，比如TensorFlow,Keras,PyTorch等，但目前已有某大神开源了一个序列标注和文本分类的模块，名称为kashgari-tf，它能够方便快速地用几行命令就可以训练一个序列标注或文本分类的模型，容易上手，而且集中了多种模型（BiGRU，CNN，BiLSTM，CRF）以及多种预训练模型（BERT，ERNIE，wwm-ext），对于用户来说算是十分友好了。该模块的参考网址为：<ahref="https://kashgari.bmio.net/">https://kashgari.bmio.net/</a> 。</p><p>笔者自己花了几天的时间来标注数据，目前已累计标注2000+数据，后续将放到Github供大家参考。我们训练的数据，比如time.train的前几行如下：（每一行中间用空格隔开）</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs mathematica"><span class="hljs-number">1</span> <span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">6</span> <span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">0</span> <span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">9</span> <span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>年 <span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>， <span class="hljs-built_in">O</span><br>日 <span class="hljs-built_in">O</span><br>本 <span class="hljs-built_in">O</span><br>萨 <span class="hljs-built_in">O</span><br>摩 <span class="hljs-built_in">O</span><br>藩 <span class="hljs-built_in">O</span><br>入 <span class="hljs-built_in">O</span><br>侵 <span class="hljs-built_in">O</span><br>琉 <span class="hljs-built_in">O</span><br>球 <span class="hljs-built_in">O</span><br>国 <span class="hljs-built_in">O</span><br>， <span class="hljs-built_in">O</span><br>并 <span class="hljs-built_in">O</span><br>在 <span class="hljs-built_in">O</span><br>一 <span class="hljs-built_in">O</span><br>个 <span class="hljs-built_in">O</span><br>时 <span class="hljs-built_in">O</span><br>期 <span class="hljs-built_in">O</span><br>内 <span class="hljs-built_in">O</span><br>控 <span class="hljs-built_in">O</span><br>制 <span class="hljs-built_in">O</span><br>琉 <span class="hljs-built_in">O</span><br>球 <span class="hljs-built_in">O</span><br>国 <span class="hljs-built_in">O</span><br><span class="hljs-operator">...</span><br></code></pre></td></tr></table></figure><p>接着是模型这块，我们采用经典的BERT+Bi-LSTM+CRF模型，训练1个epoch，batch_size为16，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># time: 2019-08-09 16:47</span><br><span class="hljs-comment"># place: Zhichunlu Beijing</span><br><br><span class="hljs-keyword">import</span> kashgari<br><span class="hljs-keyword">from</span> kashgari.corpus <span class="hljs-keyword">import</span> DataReader<br><span class="hljs-keyword">from</span> kashgari.embeddings <span class="hljs-keyword">import</span> BERTEmbedding<br><span class="hljs-keyword">from</span> kashgari.tasks.labeling <span class="hljs-keyword">import</span> BiLSTM_CRF_Model<br><br>train_x, train_y = DataReader().read_conll_format_file(<span class="hljs-string">&#x27;./data/time.train&#x27;</span>)<br>valid_x, valid_y = DataReader().read_conll_format_file(<span class="hljs-string">&#x27;./data/time.dev&#x27;</span>)<br>test_x, test_y = DataReader().read_conll_format_file(<span class="hljs-string">&#x27;./data/time.test&#x27;</span>)<br><br>bert_embedding = BERTEmbedding(<span class="hljs-string">&#x27;chinese_L-12_H-768_A-12&#x27;</span>,<br>                               task=kashgari.LABELING,<br>                               sequence_length=<span class="hljs-number">128</span>)<br><br>model = BiLSTM_CRF_Model(bert_embedding)<br>model.fit(train_x, train_y, valid_x, valid_y, batch_size=<span class="hljs-number">16</span>, epochs=<span class="hljs-number">1</span>)<br><br>model.save(<span class="hljs-string">&#x27;time_ner.h5&#x27;</span>)<br><br>model.evaluate(test_x, test_y)<br></code></pre></td></tr></table></figure><p>模型训练完后，得到的效果如下：</p><table><thead><tr class="header"><th>数据集</th><th>accuracy</th><th>loss</th></tr></thead><tbody><tr class="odd"><td>训练集</td><td>0.9814</td><td>6.7295</td></tr><tr class="even"><td>验证集</td><td>0.6868</td><td>150.8513</td></tr></tbody></table><p>在测试集上的结果如下：</p><table><thead><tr class="header"><th>数据集</th><th>precision</th><th>recall</th><th>f1</th></tr></thead><tbody><tr class="odd"><td>测试集</td><td>0.8547</td><td>0.8934</td><td>0.8736</td></tr></tbody></table><p>由于是小标注量，因此我们选择了用BERT预训练模型。如果不采用BERT预训练模型，在同样的数据集上，即使训练100个epoch，虽然在训练集上的准确率超过95%，但是在测试集上却只有大约50%的准确率，效果不行，因此，需要采用预训练模型。</p><h3 id="测试效果">测试效果</h3><p>在训练完模型后，会在当前目录下生成time_ner.h5模型文件，接着我们需要该模型文件来对新的文件进行预测，提取出文本中的时间。模型预测的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Load saved model</span><br><span class="hljs-keyword">import</span> kashgari<br><br>loaded_model = kashgari.utils.load_model(<span class="hljs-string">&#x27;time_ner.h5&#x27;</span>)<br><br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>    text = <span class="hljs-built_in">input</span>(<span class="hljs-string">&#x27;sentence: &#x27;</span>)<br>    t = loaded_model.predict([[char <span class="hljs-keyword">for</span> char <span class="hljs-keyword">in</span> text]])<br>    <span class="hljs-built_in">print</span>(t)<br></code></pre></td></tr></table></figure><p>接着我们在几条新的数据上进行预测，看看该模型的表现效果：</p><blockquote><p>"原文":"绿地控股2018年年度年报显示，截至2018年12月31日，万科金域中央项目的经营状态为“住宅、办公、商业”，项目用地面积18.90万平方米，规划计容建筑面积79.38万平方米，总建筑面积为105.78万平方米，已竣工面积32.90万平方米，总投资额95亿元，报告期实际投资额为10.18亿元。","预测时间": [ "2018年年度", "2018年12月31日"]</p></blockquote><blockquote><p>"原文":"经过工作人员两天的反复验证、严密测算，记者昨天从上海中心大厦得到确认：被誉为上海中心大厦“定楼神器”的阻尼器，在8月10日出现自2016年正式启用以来的最大摆幅。","预测时间": [ "两天", "昨天", "8月10日", "2016年"]</p></blockquote><blockquote><p>"原文": "不幸的是，在升任内史的同年九月，狄仁杰就在洛阳私宅离世。","预测时间": [ "同年九月"]</p></blockquote><blockquote><p>"原文":"早上9点25分到达北京火车站，火车站在北京市区哦，地铁很方便到达酒店，我们定了王府井大街的锦江之星，409元一晚，有点小贵。下午去了天坛公园，傍晚去了天安门广场。","预测时间": [ "早上9点25分", "下午", "傍晚"],</p></blockquote><h3 id="总结">总结</h3><p>利用深度学习模型，在小标注量数据上，我们对时间识别取得了不错的效果。后续如果我们想要提高时间识别的准确率，可以再多增加标注数据，目前还只有2000+数据～</p><p>本项目已经开源，Github的地址为：<ahref="https://github.com/percent4/Chinese_Time_Recogniztion">https://github.com/percent4/Chinese_Time_Recogniztion</a>。</p><p>另外，强烈推荐kashgari-tf模块，它能够让你在几分钟内搭建一个序列标注模型，而且方便加载各种预训练模型。</p><blockquote><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（十四）自制序列标注平台</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%E8%87%AA%E5%88%B6%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E5%B9%B3%E5%8F%B0/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%E8%87%AA%E5%88%B6%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E5%B9%B3%E5%8F%B0/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="背景介绍">背景介绍</h3><p>在平时的NLP任务中，我们经常用到命名实体识别（NER），常用的识别实体类型为人名、地名、组织机构名，但是我们往往也会有识别其它实体的需求，比如时间、品牌名等。在利用算法做实体识别的时候，我们一般采用序列标注算法，这就对标注的文本格式有一定的要求，因此，一个好的序列标注的平台必不可少，将会大大减少我们标注的工作量，有效提升算法的更新迭代速度。</p><p>本文将介绍笔者的一个工作：自制的序列标注平台。我们以时间识别为例。比如，在下面的文章中：</p><blockquote><p>按计划，2019年8月10日，荣耀智慧屏将在华为开发者大会上正式亮相，在8月6日，荣耀官微表示该产品的预约量已破十万台，8月7日下午，荣耀总裁赵明又在微博上造势率先打出差异化牌，智慧屏没有开关机广告，并表态以后也不会有，消费者体验至上，营销一波接一波，可谓来势汹汹。</p></blockquote><p>我们需要从该文章中标注出三个时间：<code>2019年8月10日</code>，<code>8月6日</code>，<code>8月7日下午</code>，并形成标注序列。</p><p>下面将详细介绍笔者的工作。</p><h3 id="序列标注平台">序列标注平台</h3><p>由于开发时间仓促以及笔者能力有限，因此，序列标注平台的功能还没有很完善，希望笔者的工作能抛砖引玉。</p><p>项目的结构图如下：</p><p><img src="/img/nlp14_1.png" /></p><p>templates中存放静态资源，time_index.html为平台的操作界面，time_output为平台标注完实体后的文件保存路径，time_server.py是用tornado写的服务端路径控制代码，utils.py中是获取某个路径下的txt文件的最大数值的函数。</p><p>其中，utils.py的完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># time: 2019-03-14</span><br><span class="hljs-comment"># place: Xinbeiqiao, Beijing</span><br><br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 获取当前所在目录的txt文本的最大数值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_max_num</span>(<span class="hljs-params">path</span>):<br>    files = os.listdir(path)<br>    <span class="hljs-keyword">if</span> files:<br>        numbers = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">int</span>(x.replace(<span class="hljs-string">&#x27;.txt&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)), files))<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">max</span>(numbers)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>time_server.py的完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># time: 2019-08-08</span><br><span class="hljs-comment"># place: Xinbeiqiao, Beijing</span><br><br><span class="hljs-keyword">import</span> os.path<br><span class="hljs-keyword">import</span> tornado.httpserver<br><span class="hljs-keyword">import</span> tornado.ioloop<br><span class="hljs-keyword">import</span> tornado.options<br><span class="hljs-keyword">import</span> tornado.web<br><span class="hljs-keyword">from</span> tornado.options <span class="hljs-keyword">import</span> define, options<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> get_max_num<br><br><span class="hljs-comment">#定义端口为9005</span><br>define(<span class="hljs-string">&quot;port&quot;</span>, default=<span class="hljs-number">9005</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;run on the given port&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br><br><span class="hljs-comment"># GET请求</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">QueryHandler</span>(tornado.web.RequestHandler):<br>    <span class="hljs-comment"># get函数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self</span>):<br>        self.render(<span class="hljs-string">&#x27;time_index.html&#x27;</span>, data = [<span class="hljs-string">&#x27;&#x27;</span>, []])<br><br><span class="hljs-comment"># POST请求</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PostHandler</span>(tornado.web.RequestHandler):<br>    <span class="hljs-comment"># post函数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">post</span>(<span class="hljs-params">self</span>):<br><br>        <span class="hljs-comment"># 获取前端参数, event, time, index</span><br>        event = self.get_argument(<span class="hljs-string">&#x27;event&#x27;</span>)<br>        times = self.get_arguments(<span class="hljs-string">&#x27;time&#x27;</span>)<br>        indices = self.get_arguments(<span class="hljs-string">&#x27;index&#x27;</span>)<br>        <span class="hljs-built_in">print</span>(event)<br>        <span class="hljs-built_in">print</span>(times)<br>        <span class="hljs-built_in">print</span>(indices)<br><br>        <span class="hljs-comment"># 前端显示序列标注信息</span><br>        tags = [<span class="hljs-string">&#x27;O&#x27;</span>] * <span class="hljs-built_in">len</span>(event)<br><br>        <span class="hljs-keyword">for</span> time, index <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(times, indices):<br>            index = <span class="hljs-built_in">int</span>(index)<br>            tags[index] = <span class="hljs-string">&#x27;B-TIME&#x27;</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(time)):<br>                tags[index+i] = <span class="hljs-string">&#x27;I-TIME&#x27;</span><br><br>        data = [event, tags]<br><br>        self.render(<span class="hljs-string">&#x27;time_index.html&#x27;</span>, data=data)<br><br>        <span class="hljs-comment"># 保存为txt文件</span><br>        dir_path = <span class="hljs-string">&#x27;./time_output&#x27;</span><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./%s/%s.txt&#x27;</span> % (dir_path, get_max_num(dir_path)+<span class="hljs-number">1</span>), <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            <span class="hljs-keyword">for</span> char, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(event, tags):<br>                f.write(char+<span class="hljs-string">&#x27;\t&#x27;</span>+tag+<span class="hljs-string">&#x27;\n&#x27;</span>)<br><br><br><span class="hljs-comment"># 主函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    <span class="hljs-comment"># 开启tornado服务</span><br>    tornado.options.parse_command_line()<br>    <span class="hljs-comment"># 定义app</span><br>    app = tornado.web.Application(<br>            handlers=[(<span class="hljs-string">r&#x27;/query&#x27;</span>, QueryHandler),<br>                      (<span class="hljs-string">r&#x27;/result&#x27;</span>, PostHandler)<br>                      ], <span class="hljs-comment">#网页路径控制</span><br>            template_path=os.path.join(os.path.dirname(__file__), <span class="hljs-string">&quot;templates&quot;</span>) <span class="hljs-comment"># 模板路径</span><br>          )<br>    http_server = tornado.httpserver.HTTPServer(app)<br>    http_server.listen(options.port)<br>    tornado.ioloop.IOLoop.instance().start()<br><br>main()<br></code></pre></td></tr></table></figure><p>time_index.html文件如下：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-meta">&lt;!DOCTYPE <span class="hljs-keyword">html</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">html</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">head</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">charset</span>=<span class="hljs-string">&quot;utf-8&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">title</span>&gt;</span>时间抽取标注平台<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">link</span> <span class="hljs-attr">rel</span>=<span class="hljs-string">&quot;stylesheet&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;https://cdn.staticfile.org/twitter-bootstrap/3.3.7/css/bootstrap.min.css&quot;</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;https://cdn.staticfile.org/twitter-bootstrap/3.3.7/js/bootstrap.min.js&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">style</span>&gt;</span><span class="language-css"></span><br><span class="language-css">        <span class="hljs-selector-tag">mark</span> &#123;</span><br><span class="language-css">            <span class="hljs-attribute">background-color</span>:<span class="hljs-number">#00ff90</span>; <span class="hljs-attribute">font-weight</span>:bold;</span><br><span class="language-css">        &#125;</span><br><span class="language-css"><span class="hljs-selector-tag">p</span>&#123;<span class="hljs-attribute">text-indent</span>:<span class="hljs-number">2em</span>;&#125;</span><br><span class="language-css">    </span><span class="hljs-tag">&lt;/<span class="hljs-name">style</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">script</span>&gt;</span><span class="language-javascript"></span><br><span class="language-javascript">        <span class="hljs-keyword">var</span> click_cnt = <span class="hljs-number">0</span>;</span><br><span class="language-javascript"></span><br><span class="language-javascript">        <span class="hljs-comment">// 双击第i个select, 添加文字的index</span></span><br><span class="language-javascript">        <span class="hljs-keyword">function</span> <span class="hljs-title function_">select_click</span>(<span class="hljs-params">i</span>)&#123;</span><br><span class="language-javascript">        <span class="hljs-keyword">var</span> content = <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">getElementById</span>(<span class="hljs-string">&#x27;event&#x27;</span>).<span class="hljs-property">value</span>;</span><br><span class="language-javascript">        <span class="hljs-keyword">var</span> time = <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">getElementById</span>(<span class="hljs-string">&#x27;time_&#x27;</span>+i.<span class="hljs-title function_">toString</span>()).<span class="hljs-property">value</span>;</span><br><span class="language-javascript"></span><br><span class="language-javascript">        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">var</span> j=<span class="hljs-number">0</span>; j&lt;=content.<span class="hljs-property">length</span>-time.<span class="hljs-property">length</span>; j++)&#123;</span><br><span class="language-javascript">        <span class="hljs-keyword">if</span>(content.<span class="hljs-title function_">substr</span>(j, time.<span class="hljs-property">length</span>) == time)&#123;</span><br><span class="language-javascript">        <span class="hljs-keyword">var</span> select = <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">getElementById</span>(<span class="hljs-string">&#x27;index_&#x27;</span>+i.<span class="hljs-title function_">toString</span>());</span><br><span class="language-javascript">        <span class="hljs-keyword">var</span> option = <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">createElement</span>(<span class="hljs-string">&quot;option&quot;</span>);</span><br><span class="language-javascript">        option.<span class="hljs-property">value</span> = j;</span><br><span class="language-javascript">        option.<span class="hljs-property">innerHTML</span> = j;</span><br><span class="language-javascript">        select.<span class="hljs-title function_">appendChild</span>(option);</span><br><span class="language-javascript">        &#125;</span><br><span class="language-javascript">        &#125;</span><br><span class="language-javascript">        &#125;</span><br><span class="language-javascript"></span><br><span class="language-javascript"><span class="hljs-comment">// 添加输入框和select框</span></span><br><span class="language-javascript">        $(<span class="hljs-variable language_">document</span>).<span class="hljs-title function_">ready</span>(<span class="hljs-keyword">function</span>(<span class="hljs-params"></span>)&#123;</span><br><span class="language-javascript"></span><br><span class="language-javascript">            $(<span class="hljs-string">&quot;#add_time&quot;</span>).<span class="hljs-title function_">click</span>(<span class="hljs-keyword">function</span>(<span class="hljs-params"></span>)&#123;</span><br><span class="language-javascript">                 click_cnt = click_cnt + <span class="hljs-number">1</span>;</span><br><span class="language-javascript">                 <span class="hljs-keyword">var</span> input_id = <span class="hljs-keyword">new</span> <span class="hljs-title class_">String</span>(<span class="hljs-string">&#x27;time_&#x27;</span>+click_cnt.<span class="hljs-title function_">toString</span>());</span><br><span class="language-javascript">                 <span class="hljs-keyword">var</span> index_id = <span class="hljs-keyword">new</span> <span class="hljs-title class_">String</span>(<span class="hljs-string">&#x27;index_&#x27;</span>+click_cnt.<span class="hljs-title function_">toString</span>());</span><br><span class="language-javascript">                 <span class="hljs-keyword">var</span> content = <span class="hljs-string">&quot;&lt;input type=&#x27;text&#x27; id=&quot;</span> + input_id + <span class="hljs-string">&quot; class=&#x27;form-control&#x27; style=&#x27;width:306px;&#x27; name=&#x27;time&#x27; /&gt; \</span></span><br><span class="hljs-string"><span class="language-javascript">                     &lt;select class=&#x27;form-control&#x27; name=&#x27;index&#x27; id=&quot;</span>+ index_id + <span class="hljs-string">&quot; style=&#x27;width:120px;&#x27; \</span></span><br><span class="hljs-string"><span class="language-javascript">                 ondblclick=&#x27;select_click(&quot;</span>+click_cnt.<span class="hljs-title function_">toString</span>()+<span class="hljs-string">&quot;)&#x27;&gt;&lt;/select&gt;&quot;</span>;</span><br><span class="language-javascript">                 $(content).<span class="hljs-title function_">appendTo</span>($(<span class="hljs-string">&quot;#time_column&quot;</span>));</span><br><span class="language-javascript">            &#125;);</span><br><span class="language-javascript"></span><br><span class="language-javascript">        &#125;);</span><br><span class="language-javascript"></span><br><span class="language-javascript"></span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">head</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">body</span>&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">center</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">br</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">br</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">br</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">form</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-horizontal&quot;</span> <span class="hljs-attr">role</span>=<span class="hljs-string">&quot;form&quot;</span> <span class="hljs-attr">method</span>=<span class="hljs-string">&quot;post&quot;</span> <span class="hljs-attr">action</span>=<span class="hljs-string">&quot;/result&quot;</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;width:600px&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-group&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">label</span> <span class="hljs-attr">for</span>=<span class="hljs-string">&quot;event&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;col-sm-2 control-label&quot;</span>&gt;</span>输入语料<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;col-sm-10&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">textarea</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;text&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-control&quot;</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;event&quot;</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;width:490px; height:200px&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;event&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">textarea</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-inline&quot;</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;text-align:left;&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">label</span> <span class="hljs-attr">for</span>=<span class="hljs-string">&quot;time_0&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;col-sm-2 control-label&quot;</span>&gt;</span>时间<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;col-sm-10&quot;</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;time_column&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;text&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-control&quot;</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;time_0&quot;</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;width:306px;&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;time&quot;</span> /&gt;</span><br>               <br>            <span class="hljs-tag">&lt;<span class="hljs-name">select</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-control&quot;</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;index_0&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;index&quot;</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;width:120px;&quot;</span> <span class="hljs-attr">ondblclick</span>=<span class="hljs-string">&quot;select_click(0)&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">select</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-group&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;col-sm-offset-2 col-sm-10&quot;</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">br</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">button</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;button&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-default&quot;</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;add_time&quot;</span>&gt;</span>添加时间<span class="hljs-tag">&lt;/<span class="hljs-name">button</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">button</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;submit&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-success&quot;</span>&gt;</span>显示标签<span class="hljs-tag">&lt;/<span class="hljs-name">button</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;/query&quot;</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">button</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;button&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-danger&quot;</span>&gt;</span>返回<span class="hljs-tag">&lt;/<span class="hljs-name">button</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">button</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;reset&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-warning&quot;</span>&gt;</span>重置<span class="hljs-tag">&lt;/<span class="hljs-name">button</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><br><span class="hljs-tag">&lt;/<span class="hljs-name">form</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">br</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;width:600px&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">p</span>&gt;</span> 原文：&#123;&#123;data[0]&#125;&#125; <span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">table</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;table table-striped&quot;</span>&gt;</span><br>&#123;% for char, tag in zip(data[0], data[1]) %&#125;<br><span class="hljs-tag">&lt;<span class="hljs-name">tr</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">td</span>&gt;</span>&#123;&#123;char&#125;&#125; <span class="hljs-tag">&lt;/<span class="hljs-name">td</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">td</span>&gt;</span>&#123;&#123;tag&#125;&#125; <span class="hljs-tag">&lt;/<span class="hljs-name">td</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">tr</span>&gt;</span><br>&#123;%end%&#125;<br><span class="hljs-tag">&lt;/<span class="hljs-name">table</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">center</span>&gt;</span><br><br><span class="hljs-tag">&lt;/<span class="hljs-name">body</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">html</span>&gt;</span><br></code></pre></td></tr></table></figure><h3 id="平台使用">平台使用</h3><p>运行上述time_server.py后，在浏览器端输入网址: <ahref="http://localhost:9005/query">http://localhost:9005/query</a> ,则会显示如下界面：</p><p><img src="/img/nlp14_2.png" /></p><p>在<code>输入语料框</code>中，我们输入语料：</p><blockquote><p>8月8日是“全民健身日”，推出重磅微视频《我们要赢的，是自己》。</p></blockquote><p>在时间这个输入框中，可以标注语料中的时间，同时双击同一行中的下拉列表，就能显示该标注时间在语料中的起始位置，有时候同样的标注时间会在语料中出现多次，那么我们在下拉列表中选择我们需要的标注的起始位置即可。</p><p>点击<code>添加时间</code>按钮，它会增加一行标注，允许我们在同一份预料中标注多个时间。我们的一个简单的标注例子如下：</p><p><img src="/img/nlp14_3.png" /></p><p>点击<code>显示标注</code>，则会显示我们标注完后形成的序列标注信息，同时将该序列信息保存为txt文件，该txt文件位于time_output目录下。在网页上的序列标注信息如下：</p><p><img src="/img/nlp14_4.png" /></p><p>同时，我们也可以查看保存的txt文档信息，如下：</p><p><img src="/img/nlp14_5.png" /></p><p>点击<code>返回</code>按钮，它会允许我们进行下一次的标注。刚才展示的只是一个简单例子，稍微复杂的标注如下图：</p><p><img src="/img/nlp14_6.png" /></p><p>它形成的标注序列(部分)如下：</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs mathematica">按<span class="hljs-built_in">O</span><br>计<span class="hljs-built_in">O</span><br>划<span class="hljs-built_in">O</span><br>，<span class="hljs-built_in">O</span><br><span class="hljs-number">2</span><span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">0</span><span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">1</span><span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">9</span><span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>年<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">8</span><span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>月<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">1</span><span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">0</span><span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>日<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>，<span class="hljs-built_in">O</span><br>荣<span class="hljs-built_in">O</span><br>耀<span class="hljs-built_in">O</span><br>智<span class="hljs-built_in">O</span><br>慧<span class="hljs-built_in">O</span><br>屏<span class="hljs-built_in">O</span><br>将<span class="hljs-built_in">O</span><br>在<span class="hljs-built_in">O</span><br>华<span class="hljs-built_in">O</span><br>为<span class="hljs-built_in">O</span><br>开<span class="hljs-built_in">O</span><br>发<span class="hljs-built_in">O</span><br>者<span class="hljs-built_in">O</span><br>大<span class="hljs-built_in">O</span><br>会<span class="hljs-built_in">O</span><br>上<span class="hljs-built_in">O</span><br>正<span class="hljs-built_in">O</span><br>式<span class="hljs-built_in">O</span><br>亮<span class="hljs-built_in">O</span><br>相<span class="hljs-built_in">O</span><br>，<span class="hljs-built_in">O</span><br>在<span class="hljs-built_in">O</span><br><span class="hljs-number">8</span><span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>月<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">6</span><span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>日<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>，<span class="hljs-built_in">O</span><br>荣<span class="hljs-built_in">O</span><br>耀<span class="hljs-built_in">O</span><br>官<span class="hljs-built_in">O</span><br>微<span class="hljs-built_in">O</span><br>表<span class="hljs-built_in">O</span><br>示<span class="hljs-built_in">O</span><br>该<span class="hljs-built_in">O</span><br>产<span class="hljs-built_in">O</span><br>品<span class="hljs-built_in">O</span><br><span class="hljs-operator">......</span><br></code></pre></td></tr></table></figure><h3 id="总结">总结</h3><p>本平台仅作为序列标注算法的前期标注工具使用，并不涉及具体的算法。另外，后续该平台也会陆续开放出来，如果大家有好的建议，也可以留言～</p><p>本项目已上传只Github, 网址为： <ahref="https://github.com/percent4/entity_tagging_platform">https://github.com/percent4/entity_tagging_platform</a></p><blockquote><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>标注平台</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（十三）中文分词工具的使用尝试</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8%E5%B0%9D%E8%AF%95/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8%E5%B0%9D%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文将对三种中文分词工具进行使用尝试，这三种工具分别为哈工大的LTP，结巴分词以及北大的pkuseg。</p><p>首先我们先准备好环境，即需要安装三个模块：pyltp, jieba,pkuseg以及LTP的分词模型文件<code>cws.model</code>。在用户字典中添加以下5个词语：</p><blockquote><p>经 少安 贺凤英 F-35战斗机 埃达尔·阿勒坎</p></blockquote><p>测试的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">import</span> pkuseg<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor<br><br>lexicon = [<span class="hljs-string">&#x27;经&#x27;</span>, <span class="hljs-string">&#x27;少安&#x27;</span>, <span class="hljs-string">&#x27;贺凤英&#x27;</span>, <span class="hljs-string">&#x27;F-35战斗机&#x27;</span>, <span class="hljs-string">&#x27;埃达尔·阿勒坎&#x27;</span>] <span class="hljs-comment"># 自定义词典</span><br><br><span class="hljs-comment"># 哈工大LTP分词</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ltp_segment</span>(<span class="hljs-params">sent</span>):<br>    <span class="hljs-comment"># 加载文件</span><br>    cws_model_path = os.path.join(<span class="hljs-string">&#x27;data/cws.model&#x27;</span>) <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>    lexicon_path = os.path.join(<span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>) <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br>    segmentor = Segmentor()<br>    segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br>    words = <span class="hljs-built_in">list</span>(segmentor.segment(sent))<br>    segmentor.release()<br><br>    <span class="hljs-keyword">return</span> words<br><br><span class="hljs-comment"># 结巴分词</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">jieba_cut</span>(<span class="hljs-params">sent</span>):<br>    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> lexicon:<br>        jieba.add_word(word)<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">list</span>(jieba.cut(sent))<br><br><span class="hljs-comment"># pkuseg分词</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pkuseg_cut</span>(<span class="hljs-params">sent</span>):<br>    seg = pkuseg.pkuseg(user_dict=lexicon)<br>    words = seg.cut(sent)<br>    <span class="hljs-keyword">return</span> words<br><br>sent = <span class="hljs-string">&#x27;尽管玉亭成家以后，他老婆贺凤英那些年把少安妈欺负上一回又一回，怕老婆的玉亭连一声也不敢吭，但少安他妈不计较他。&#x27;</span><br><span class="hljs-comment">#sent = &#x27;据此前报道，以色列于去年5月成为世界上第一个在实战中使用F-35战斗机的国家。&#x27;</span><br><span class="hljs-comment">#sent = &#x27;小船4月8日经长江前往小鸟岛。&#x27;</span><br><span class="hljs-comment">#sent = &#x27;1958年，埃达尔·阿勒坎出生在土耳其首都安卡拉，但他的求学生涯多在美国度过。&#x27;</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;ltp:&#x27;</span>, ltp_segment(sent))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;jieba:&#x27;</span>, jieba_cut(sent))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;pkuseg:&#x27;</span>, pkuseg_cut(sent))<br></code></pre></td></tr></table></figure><p>&amp;emsp 对于第一句话，输出结果如下：</p><blockquote><p>原文:尽管玉亭成家以后，他老婆贺凤英那些年把少安妈欺负上一回又一回，怕老婆的玉亭连一声也不敢吭，但少安他妈不计较他。</p></blockquote><blockquote><p>ltp: ['尽管', '玉亭', '成家', '以后', '，', '他', '老婆', '贺凤英','那些', '年', '把', '少安', '妈', '欺负', '上', '一', '回', '又', '一','回', '，', '怕', '老婆', '的', '玉亭', '连', '一', '声', '也', '不','敢', '吭', '，', '但', '少安', '他妈', '不', '计较', '他', '。']</p></blockquote><blockquote><p>jieba: ['尽管', '玉亭', '成家', '以后', '，', '他', '老婆', '贺凤英','那些', '年', '把', '少安', '妈', '欺负', '上', '一回', '又', '一回','，', '怕老婆', '的', '玉亭', '连', '一声', '也', '不敢', '吭', '，','但少安', '他妈', '不', '计较', '他', '。']</p></blockquote><blockquote><p>pkuseg: ['尽管', '玉亭', '成家', '以后', '，', '他', '老婆','贺凤英', '那些', '年', '把', '少安', '妈', '欺负', '上', '一', '回','又', '一', '回', '，', '怕', '老婆', '的', '玉亭', '连', '一', '声','也', '不', '敢', '吭', '，', '但', '少安', '他妈', '不', '计较', '他','。']</p></blockquote><p>对于第二句话，输出结果如下：</p><blockquote><p>原文:据此前报道，以色列于去年5月成为世界上第一个在实战中使用F-35战斗机的国家。</p></blockquote><blockquote><p>ltp: ['据', '此前', '报道', '，', '以色列', '于', '去年', '5月','成为', '世界', '上', '第一', '个', '在', '实战', '中', '使用', 'F-35','战斗机', '的', '国家', '。']</p></blockquote><blockquote><p>jieba: ['据此', '前', '报道', '，', '以色列', '于', '去年', '5','月', '成为', '世界', '上', '第一个', '在', '实战', '中', '使用', 'F','-', '35', '战斗机', '的', '国家', '。']</p></blockquote><blockquote><p>pkuseg: ['据', '此前', '报道', '，', '以色列', '于', '去年', '5月','成为', '世界', '上', '第一', '个', '在', '实战', '中', '使用','F-35战斗机', '的', '国家', '。']</p></blockquote><p>对于第三句话，输出结果如下：</p><blockquote><p>原文: 小船4月8日经长江前往小鸟岛。</p></blockquote><blockquote><p>ltp: ['小船', '4月', '8日', '经长江', '前往', '小鸟岛', '。']</p></blockquote><blockquote><p>jieba: ['小船', '4', '月', '8', '日经', '长江', '前往', '小', '鸟岛','。']</p></blockquote><blockquote><p>pkuseg: ['小船', '4月', '8日', '经', '长江', '前往', '小鸟', '岛','。']</p></blockquote><p>对于第四句话，输出结果如下：</p><blockquote><p>原文:1958年，埃达尔·阿勒坎出生在土耳其首都安卡拉，但他的求学生涯多在美国度过。</p></blockquote><blockquote><p>ltp: ['1958年', '，', '埃达尔·阿勒坎', '出生', '在', '土耳其','首都', '安卡拉', '，', '但', '他', '的', '求学', '生涯', '多', '在','美国', '度过', '。']</p></blockquote><blockquote><p>jieba: ['1958', '年', '，', '埃', '达尔', '·', '阿勒', '坎', '出生','在', '土耳其', '首都', '安卡拉', '，', '但', '他', '的', '求学','生涯', '多', '在', '美国', '度过', '。']</p></blockquote><blockquote><p>pkuseg: ['1958年', '，', '埃达尔·阿勒坎', '出生', '在', '土耳其','首都', '安卡拉', '，', '但', '他', '的', '求学', '生涯', '多', '在','美国', '度过', '。']</p></blockquote><p>接着，对以上的测试情况做一个简单的总结：</p><ol type="1"><li><p>用户词典方面：LTP和pkuseg的效果都很好，jieba的表现不尽如人意，这主要是因为自定义的字典的词语里面含有标点符号，关于该问题的解决办法，可以参考网址：<ahref="https://blog.csdn.net/weixin_42471956/article/details/80795534">https://blog.csdn.net/weixin_42471956/article/details/80795534</a></p></li><li><p>从第二句话的效果来看，pkuseg的分词效果应该是最好的，‘经’应该作为单个的词语切分出来，而LTP和jieba即使加了自定义词典，也没有效果，同理，‘F-35战斗机’也是类似的情形。</p></li></ol><p>总的来说，三者的分词效果都很优秀，差距不是很大，但在自定义词典这块，无疑pkuseg的效果更加稳定些。笔者也会在以后的分词使用中多多考虑pkuseg～有关pkuseg的介绍与使用，可以参考网址：<ahref="https://github.com/lancopku/PKUSeg-python">https://github.com/lancopku/PKUSeg-python</a></p><blockquote><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape），欢迎大家关注~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>分词</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（十二）依存句法分析的可视化及图分析</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E5%8F%8A%E5%9B%BE%E5%88%86%E6%9E%90/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E5%8F%8A%E5%9B%BE%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>依存句法分析的效果虽然没有像分词、NER的效果来的好，但也有其使用价值，在日常的工作中，我们免不了要和其打交道。笔者这几天一直在想如何分析依存句法分析的结果，一个重要的方面便是其可视化和它的图分析。</p><p>我们使用的NLP工具为jieba和LTP，其中jieba用于分词，LTP用于词性标注和句法分析，需要事件下载<code>pos.model</code>和<code>parser.model</code>文件。</p><p>本文使用的示例句子为：</p><blockquote><p>2018年7月26日，华为创始人任正非向5G极化码（Polar码）之父埃尔达尔教授举行颁奖仪式，表彰其对于通信领域做出的贡献。</p></blockquote><p>首先，让我们来看一下没有可视化效果之前的句法分析结果。Python代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span>  Postagger, Parser<br><br>sent = <span class="hljs-string">&#x27;2018年7月26日，华为创始人任正非向5G极化码（Polar码）之父埃尔达尔教授举行颁奖仪式，表彰其对于通信领域做出的贡献。&#x27;</span><br><br>jieba.add_word(<span class="hljs-string">&#x27;Polar码&#x27;</span>)<br>jieba.add_word(<span class="hljs-string">&#x27;5G极化码&#x27;</span>)<br>jieba.add_word(<span class="hljs-string">&#x27;埃尔达尔&#x27;</span>)<br>jieba.add_word(<span class="hljs-string">&#x27;之父&#x27;</span>)<br>words = <span class="hljs-built_in">list</span>(jieba.cut(sent))<br><br><span class="hljs-built_in">print</span>(words)<br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)<br>postagger = Postagger()<br>postagger.load(pos_model_path)<br>postags = postagger.postag(words)<br><br><span class="hljs-comment"># 依存句法分析</span><br>par_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/parser.model&#x27;</span>)<br>parser = Parser()<br>parser.load(par_model_path)<br>arcs = parser.parse(words, postags)<br><br>rely_id = [arc.head <span class="hljs-keyword">for</span> arc <span class="hljs-keyword">in</span> arcs]  <span class="hljs-comment"># 提取依存父节点id</span><br>relation = [arc.relation <span class="hljs-keyword">for</span> arc <span class="hljs-keyword">in</span> arcs]  <span class="hljs-comment"># 提取依存关系</span><br>heads = [<span class="hljs-string">&#x27;Root&#x27;</span> <span class="hljs-keyword">if</span> <span class="hljs-built_in">id</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> words[<span class="hljs-built_in">id</span>-<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> rely_id]  <span class="hljs-comment"># 匹配依存父节点词语</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(words)):<br>    <span class="hljs-built_in">print</span>(relation[i] + <span class="hljs-string">&#x27;(&#x27;</span> + words[i] + <span class="hljs-string">&#x27;, &#x27;</span> + heads[i] + <span class="hljs-string">&#x27;)&#x27;</span>)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;2018&#x27;</span>, <span class="hljs-string">&#x27;年&#x27;</span>, <span class="hljs-string">&#x27;7&#x27;</span>, <span class="hljs-string">&#x27;月&#x27;</span>, <span class="hljs-string">&#x27;26&#x27;</span>, <span class="hljs-string">&#x27;日&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>, <span class="hljs-string">&#x27;华为&#x27;</span>, <span class="hljs-string">&#x27;创始人&#x27;</span>, <span class="hljs-string">&#x27;任正非&#x27;</span>, <span class="hljs-string">&#x27;向&#x27;</span>, <span class="hljs-string">&#x27;5G极化码&#x27;</span>, <span class="hljs-string">&#x27;（&#x27;</span>, <span class="hljs-string">&#x27;Polar码&#x27;</span>, <span class="hljs-string">&#x27;）&#x27;</span>, <span class="hljs-string">&#x27;之父&#x27;</span>, <span class="hljs-string">&#x27;埃尔达尔&#x27;</span>, <span class="hljs-string">&#x27;教授&#x27;</span>, <span class="hljs-string">&#x27;举行&#x27;</span>, <span class="hljs-string">&#x27;颁奖仪式&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>, <span class="hljs-string">&#x27;表彰&#x27;</span>, <span class="hljs-string">&#x27;其&#x27;</span>, <span class="hljs-string">&#x27;对于&#x27;</span>, <span class="hljs-string">&#x27;通信&#x27;</span>, <span class="hljs-string">&#x27;领域&#x27;</span>, <span class="hljs-string">&#x27;做出&#x27;</span>, <span class="hljs-string">&#x27;的&#x27;</span>, <span class="hljs-string">&#x27;贡献&#x27;</span>, <span class="hljs-string">&#x27;。&#x27;</span>]</span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">2018</span>, 年)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(年, 日)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">7</span>, 月)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(月, 日)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">26</span>, 日)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(日, 举行)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(，, 日)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(华为, 创始人)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(创始人, 任正非)</span></span><br><span class="hljs-function"><span class="hljs-title">SBV</span><span class="hljs-params">(任正非, 举行)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(向, 举行)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">5</span>G极化码, 之父)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(（, Polar码)</span></span><br><span class="hljs-function"><span class="hljs-title">COO</span><span class="hljs-params">(Polar码, <span class="hljs-number">5</span>G极化码)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(）, Polar码)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(之父, 埃尔达尔)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(埃尔达尔, 教授)</span></span><br><span class="hljs-function"><span class="hljs-title">POB</span><span class="hljs-params">(教授, 向)</span></span><br><span class="hljs-function"><span class="hljs-title">HED</span><span class="hljs-params">(举行, Root)</span></span><br><span class="hljs-function"><span class="hljs-title">VOB</span><span class="hljs-params">(颁奖仪式, 举行)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(，, 举行)</span></span><br><span class="hljs-function"><span class="hljs-title">COO</span><span class="hljs-params">(表彰, 举行)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(其, 贡献)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(对于, 做出)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(通信, 领域)</span></span><br><span class="hljs-function"><span class="hljs-title">POB</span><span class="hljs-params">(领域, 对于)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(做出, 贡献)</span></span><br><span class="hljs-function"><span class="hljs-title">RAD</span><span class="hljs-params">(的, 做出)</span></span><br><span class="hljs-function"><span class="hljs-title">VOB</span><span class="hljs-params">(贡献, 表彰)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(。, 举行)</span></span><br></code></pre></td></tr></table></figure><p>我们得到了该句子的依存句法分析的结果，但是其可视化效果却不好。</p><p>我们使用Graphviz工具来得到上述依存句法分析的可视化结果，代码（接上述代码）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> graphviz <span class="hljs-keyword">import</span> Digraph<br><br>g = Digraph(<span class="hljs-string">&#x27;测试图片&#x27;</span>)<br><br>g.node(name=<span class="hljs-string">&#x27;Root&#x27;</span>)<br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>    g.node(name=word)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(words)):<br>    <span class="hljs-keyword">if</span> relation[i] <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;HED&#x27;</span>]:<br>        g.edge(words[i], heads[i], label=relation[i])<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">if</span> heads[i] == <span class="hljs-string">&#x27;Root&#x27;</span>:<br>            g.edge(words[i], <span class="hljs-string">&#x27;Root&#x27;</span>, label=relation[i])<br>        <span class="hljs-keyword">else</span>:<br>            g.edge(heads[i], <span class="hljs-string">&#x27;Root&#x27;</span>, label=relation[i])<br><br>g.view()<br></code></pre></td></tr></table></figure><p>得到的依存句法分析的可视化图片如下：</p><p><img src="/img/nlp12_1.png" /></p><p>在这张图片中，我们有了对依存句法分析结果的直观感觉，效果也非常好，但是遗憾的是，我们并不能对上述可视化结果形成的图（Graph）进行图分析，因为Graphviz仅仅只是一个可视化工具。那么，我们该用什么样的工具来进行图分析呢？</p><p>答案就是NetworkX。以下是笔者对于NetworkX应用于依存句法分析的可视化和图分析的展示，其中图分析展示了两个节点之间的最短路径。示例的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 利用networkx绘制句法分析结果</span><br><span class="hljs-keyword">import</span> networkx <span class="hljs-keyword">as</span> nx<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> pylab <span class="hljs-keyword">import</span> mpl<br><br>mpl.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;Arial Unicode MS&#x27;</span>]  <span class="hljs-comment"># 指定默认字体</span><br><br><br>G = nx.Graph()  <span class="hljs-comment"># 建立无向图G</span><br><br><span class="hljs-comment"># 添加节点</span><br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>    G.add_node(word)<br><br>G.add_node(<span class="hljs-string">&#x27;Root&#x27;</span>)<br><br><span class="hljs-comment"># 添加边</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(words)):<br>    G.add_edge(words[i], heads[i])<br><br>source = <span class="hljs-string">&#x27;5G极化码&#x27;</span><br>target1 = <span class="hljs-string">&#x27;任正非&#x27;</span><br>distance1 = nx.shortest_path_length(G, source=source, target=target1)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#x27;%s&#x27;与&#x27;%s&#x27;在依存句法分析图中的最短距离为:  %s&quot;</span> % (source, target1, distance1))<br><br>target2 = <span class="hljs-string">&#x27;埃尔达尔&#x27;</span><br>distance2 = nx.shortest_path_length(G, source=source, target=target2)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#x27;%s&#x27;与&#x27;%s&#x27;在依存句法分析图中的最短距离为:  %s&quot;</span> % (source, target2, distance2))<br><br>nx.draw(G, with_labels=<span class="hljs-literal">True</span>)<br>plt.savefig(<span class="hljs-string">&quot;undirected_graph.png&quot;</span>)<br></code></pre></td></tr></table></figure><p>得到的可视化图片如下：</p><p><img src="/img/nlp12_2.png" /></p><p>输出的结果如下：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scheme"><span class="hljs-symbol">&#x27;5G极化码</span><span class="hljs-symbol">&#x27;与</span><span class="hljs-symbol">&#x27;任正非</span><span class="hljs-symbol">&#x27;在依存句法分析图中的最短距离为:</span>  <span class="hljs-number">6</span><br><span class="hljs-symbol">&#x27;5G极化码</span><span class="hljs-symbol">&#x27;与</span><span class="hljs-symbol">&#x27;埃尔达尔</span><span class="hljs-symbol">&#x27;在依存句法分析图中的最短距离为:</span>  <span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><p>本次到此结束，希望这篇简短的文章能够给读者带来一些启发～</p><blockquote><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>依存句法分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（十一）从文本中提取时间</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%E4%BB%8E%E6%96%87%E6%9C%AC%E4%B8%AD%E6%8F%90%E5%8F%96%E6%97%B6%E9%97%B4/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%E4%BB%8E%E6%96%87%E6%9C%AC%E4%B8%AD%E6%8F%90%E5%8F%96%E6%97%B6%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在我们的日常生活和工作中，从文本中提取时间是一项非常基础却重要的工作，因此，本文将介绍如何从文本中有效地提取时间。</p><p>举个简单的例子，我们需要从下面的文本中提取时间：</p><blockquote><p>6月28日，杭州市统计局权威公布《2019年5月月报》，杭州市医保参保人数达到1006万，相比于2月份的989万，三个月暴涨16万人参保，傲视新一线城市。</p></blockquote><p>我们可以从文本有提取<code>6月28日</code>，<code>2019年5月</code>，<code>2月份</code>这三个有效时间。</p><p>通常情况下，较好的解决思路是利用深度学习模型来识别文本中的时间，通过一定数量的标记文本和合适的模型。本文尝试利用现有的NLP工具来解决如何从文本中提取时间。</p><p>本文使用的工具为哈工大的pyltp，可以在Python的第三方模块中找到，实现下载好分词模型<code>cws.model</code>和词性标注<code>pos.model</code>这两个模型文件。</p><p>话不多说，我们直接上Python代码，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Postagger<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LTP</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>        pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br>        self.segmentor = Segmentor()  <span class="hljs-comment"># 初始化实例</span><br>        self.segmentor.load(cws_model_path) <span class="hljs-comment"># 加载模型</span><br>        self.postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>        self.postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br><br>    <span class="hljs-comment"># 分词</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">segment</span>(<span class="hljs-params">self, text</span>):<br>        words = <span class="hljs-built_in">list</span>(self.segmentor.segment(text))<br>        <span class="hljs-keyword">return</span> words<br><br>    <span class="hljs-comment"># 词性标注</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">postag</span>(<span class="hljs-params">self, words</span>):<br>        postags = <span class="hljs-built_in">list</span>(self.postagger.postag(words))<br>        <span class="hljs-keyword">return</span> postags<br><br>    <span class="hljs-comment"># 获取文本中的时间</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_time</span>(<span class="hljs-params">self, text</span>):<br><br>        <span class="hljs-comment"># 开始分词及词性标注</span><br>        words = self.segment(text)<br>        postags = self.postag(words)<br><br>        time_lst = []<br><br>        i = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> tag, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(postags, words):<br>            <span class="hljs-keyword">if</span> tag == <span class="hljs-string">&#x27;nt&#x27;</span>:<br>                j = i<br>                <span class="hljs-keyword">while</span> postags[j] == <span class="hljs-string">&#x27;nt&#x27;</span> <span class="hljs-keyword">or</span> words[j] <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;至&#x27;</span>, <span class="hljs-string">&#x27;到&#x27;</span>]:<br>                    j += <span class="hljs-number">1</span><br>                time_lst.append(<span class="hljs-string">&#x27;&#x27;</span>.join(words[i:j]))<br>            i += <span class="hljs-number">1</span><br><br>        <span class="hljs-comment"># 去重子字符串的情形</span><br>        remove_lst = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> time_lst:<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> time_lst:<br>                <span class="hljs-keyword">if</span> i != j <span class="hljs-keyword">and</span> i <span class="hljs-keyword">in</span> j:<br>                    remove_lst.append(i)<br><br>        text_time_lst = []<br>        <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> time_lst:<br>            <span class="hljs-keyword">if</span> item <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> remove_lst:<br>                text_time_lst.append(item)<br><br>        <span class="hljs-comment"># print(text_time_lst)</span><br>        <span class="hljs-keyword">return</span> text_time_lst<br><br>    <span class="hljs-comment"># 释放模型</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">free_ltp</span>(<span class="hljs-params">self</span>):<br>        self.segmentor.release()<br>        self.postagger.release()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    ltp = LTP()<br><br>    <span class="hljs-comment"># 输入文本</span><br>    sent = <span class="hljs-string">&#x27;6月28日，杭州市统计局权威公布《2019年5月月报》，杭州市医保参保人数达到1006万，相比于2月份的989万，三个月暴涨16万人参保，傲视新一线城市。&#x27;</span><br>    time_lst = ltp.get_time(sent)<br>    ltp.free_ltp()<br><br>    <span class="hljs-comment"># 输出文本中提取的时间</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;提取时间： %s&#x27;</span> % <span class="hljs-built_in">str</span>(time_lst))<br></code></pre></td></tr></table></figure><p>接着，我们测试几个例子。</p><p>输入文本为：</p><blockquote><p>今天，央行举行了2019年6月份金融统计数据解读吹风会，发布了2019年6月份金融统计数据并就当前的一些热点问题进行了解读和回应。</p></blockquote><p>文本中提取的时间为：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">提取时间： [&#x27;今天&#x27;, &#x27;<span class="hljs-number">2019</span>年6月份&#x27;, &#x27;<span class="hljs-number">2019</span>年6月份&#x27;, &#x27;当前&#x27;]<br></code></pre></td></tr></table></figure><p>输入文本为：</p><blockquote><p>2006年，上海的国内生产总值达到10296.97亿元，是中国内地第一个GDP突破万亿元的城市。2008年，北京GDP破万亿。两年后，广州GDP超过万亿。2011年，深圳、天津、苏州、重庆4城的GDP也进入了万亿行列。武汉、成都在2014年跻身“万亿俱乐部”，杭州、南京和青岛、无锡和长沙的GDP依次在2015年、2016年和2017年过万亿。宁波和郑州则成为2018年万亿俱乐部的新成员。</p></blockquote><p>文本中提取的时间为：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">提取时间： [&#x27;<span class="hljs-number">2006</span>年&#x27;, &#x27;<span class="hljs-number">2008</span>年&#x27;, &#x27;<span class="hljs-number">2011</span>年&#x27;, &#x27;<span class="hljs-number">2014</span>年&#x27;, &#x27;<span class="hljs-number">2015</span>年&#x27;, &#x27;<span class="hljs-number">2016</span>年&#x27;, &#x27;<span class="hljs-number">2018</span>年&#x27;]<br></code></pre></td></tr></table></figure><p>输入文本为：</p><blockquote><p>此后，6月28日、7月9日和7月11日下午，武威市政协、市人大、市政府分别召开坚决全面彻底肃清火荣贵流毒和影响专题民主生活会。</p></blockquote><p>文本中提取的时间为：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">提取时间： <span class="hljs-selector-attr">[<span class="hljs-string">&#x27;此后&#x27;</span>, <span class="hljs-string">&#x27;6月28日&#x27;</span>, <span class="hljs-string">&#x27;7月9日&#x27;</span>, <span class="hljs-string">&#x27;7月11日下午&#x27;</span>]</span><br></code></pre></td></tr></table></figure><p>输入文本为：</p><blockquote><p>姜保红出生于1974年4月，她于2016年11月至2018年9月任武威市副市长，履新时，武威市的一把手正是火荣贵。</p></blockquote><p>文本中提取的时间为：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">提取时间： [&#x27;<span class="hljs-number">1974</span>年4月&#x27;, &#x27;<span class="hljs-number">2016</span>年11月至<span class="hljs-number">2018</span>年9月&#x27;]<br></code></pre></td></tr></table></figure><p>本次分享到此结束，欢迎大家批评指正。</p><blockquote><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BERT的几个可能的应用</title>
    <link href="/2023/07/08/BERT%E7%9A%84%E5%87%A0%E4%B8%AA%E5%8F%AF%E8%83%BD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
    <url>/2023/07/08/BERT%E7%9A%84%E5%87%A0%E4%B8%AA%E5%8F%AF%E8%83%BD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>BERT是谷歌公司于2018年11月发布的一款新模型，它一种预训练语言表示的方法，在大量文本语料（维基百科）上训练了一个通用的“语言理解”模型，然后用这个模型去执行想做的NLP任务。一经公布，它便引爆了整个NLP界，其在11个主流NLP任务中都取得优异的结果，因此成为NLP领域最吸引人的一个模型。简单来说，BERT就是在训练了大量的文本语料（无监督）之后，能够在对英语中的单词（或中文的汉字）给出一个向量表示，使得该单词（或汉字）具有一定的语义表示能力，因此，BERT具有一定的先验知识，在NLP任务中表现十分抢眼。</p><p>在文章<ahref="https://blog.csdn.net/Vancl_Wang/article/details/90349047">利用bert-serving-server搭建bert词向量服务(一)</a>中，作者简洁明了地介绍了如何利用bert-serving-server来获取中文汉字的词向量，这大大降低了一般从业者使用BERT的门槛。</p><p>结合笔者这段时间的工作体会以及思考，笔者尝试着给出BERT的几个可能的应用，如下：</p><ul><li>NLP基本任务</li><li>查找相似词语</li><li>提取文本中的实体</li><li>问答中的实体对齐</li></ul><p>由于笔者才疏学浅且撰写文章时间仓促，文章中有不足之处，请读者多多批评指正！</p><h3 id="nlp基本任务">NLP基本任务</h3><p>BERT公布已经半年多了，现在已经成为NLP中的深度学习模型中必不可少的工具，一般会加载在模型中的Embedding层。由于篇幅原因，笔者不再介绍自己的BERT项目，而是介绍几个BERT在基本任务中的Github项目：</p><ul><li>英语文本分类： <strong><ahref="https://github.com/Socialbird-AILab/BERT-Classification-Tutorial">BERT-Classification-Tutorial</a></strong></li><li>中文情感分类： <strong><ahref="https://github.com/renxingkai/BERT_Chinese_Classification">BERT_Chinese_Classification</a></strong></li><li>中文命名实体识别（NER）: <strong><ahref="https://github.com/yumath/bertNER">bertNER</a></strong></li></ul><p>可以看到，BERT已经广泛应用于NLP基本任务中，在开源项目中导出可以见到它的身影，并且这些项目的作者也写了非常细致的代码工程，便于上手。</p><p>在具体讲述下面的三个应用前，我们先了解下BERT应用的项目结构，如下：</p><p><imgsrc="https://img-blog.csdnimg.cn/20190607111211990.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pjbGlhbjkx,size_16,color_FFFFFF,t_70" /></p><p>其中，bert_client_lmj.py为调用BERT词向量服务，具体可参考文章<ahref="https://blog.csdn.net/Vancl_Wang/article/details/90349047">利用bert-serving-server搭建bert词向量服务(一)</a>，完整的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><span class="hljs-keyword">from</span> bert_serving.client <span class="hljs-keyword">import</span> BertClient<br><span class="hljs-keyword">from</span> sklearn.metrics.pairwise <span class="hljs-keyword">import</span> cosine_similarity<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoding</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.server_ip = <span class="hljs-string">&quot;127.0.0.1&quot;</span><br>        self.bert_client = BertClient(ip=self.server_ip)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, query</span>):<br>        tensor = self.bert_client.encode([query])<br>        <span class="hljs-keyword">return</span> tensor<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query_similarity</span>(<span class="hljs-params">self, query_list</span>):<br>        tensors = self.bert_client.encode(query_list)<br>        <span class="hljs-keyword">return</span> cosine_similarity(tensors)[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    ec = Encoding()<br>    <span class="hljs-built_in">print</span>(ec.encode(<span class="hljs-string">&quot;中国&quot;</span>).shape)<br>    <span class="hljs-built_in">print</span>(ec.encode(<span class="hljs-string">&quot;美国&quot;</span>).shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;中国和美国的向量相似度:&quot;</span>, ec.query_similarity([<span class="hljs-string">&quot;中国&quot;</span>, <span class="hljs-string">&quot;美国&quot;</span>]))<br></code></pre></td></tr></table></figure><h3 id="查找相似词语">查找相似词语</h3><p>利用词向量可以查找文章中与指定词语最相近的几个词语。具体的做法为：现将文章分词，对分词后的每个词，查询其与指定词语的相似度，最后按相似度输出词语即可。我们的示例文章为老舍的《养花》，内容如下：</p><blockquote><p>我爱花，所以也爱养花。我可还没成为养花专家，因为没有工夫去研究和试验。我只把养花当做生活中的一种乐趣，花开得大小好坏都不计较，只要开花，我就高兴。在我的小院子里，一到夏天满是花草，小猫只好上房去玩，地上没有它们的运动场。花虽然多，但是没有奇花异草。珍贵的花草不易养活，看着一棵好花生病要死，是件难过的事。北京的气候，对养花来说不算很好，冬天冷，春天多风，夏天不是干旱就是大雨倾盆，秋天最好，可是会忽然闹霜冻。在这种气候里，想把南方的好花养活，我还没有那么大的本事。因此，我只养些好种易活、自己会奋斗的花草。不过，尽管花草自己会奋斗，我若是置之不理，任其自生自灭，大半还是会死的。我得天天照管它们，像好朋友似的关心它们。一来二去，我摸着一些门道：有的喜阴，就别放在太阳地里；有的喜干，就别多浇水。摸着门道，花草养活了，而且三年五载老活着、开花，多么有意思啊！不是乱吹，这就是知识呀！多得些知识决不是坏事。我不是有腿病吗，不但不利于行，也不利于久坐。我不知道花草们受我的照顾，感谢我不感谢；我可得感谢它们。我工作的时候，我总是写一会儿就到院中去看看，浇浇这棵，搬搬那盆，然后回到屋里再写一会儿，然后再出去。如此循环，让脑力劳动和体力劳动得到适当的调节，有益身心，胜于吃药。要是赶上狂风暴雨或天气突变，就得全家动员，抢救花草，十分紧张。几百盆花，都要很快地抢到屋里去，使人腰酸腿疼，热汗直流。第二天，天气好了，又得把花都搬出去，就又一次腰酸腿疼，热汗直流。可是，这多么有意思呀！不劳动，连棵花也养不活，这难道不是真理吗？送牛奶的同志进门就夸“好香”，这使我们全家都感到骄傲。赶到昙花开放的时候，约几位朋友来看看，更有秉烛夜游的味道——昙花总在夜里开放。花分根了，一棵分为几棵，就赠给朋友们一些。看着友人拿走自己的劳动果实，心里自然特别欢喜。当然，也有伤心的时候，今年夏天就有这么一回。三百棵菊秧还在地上（没到移入盆中的时候），下了暴雨，邻家的墙倒了，菊秧被砸死三十多种，一百多棵。全家人几天都没有笑容。有喜有忧，有笑有泪，有花有果，有香有色，既须劳动，又长见识，这就是养花的乐趣。</p></blockquote><p>指定词语为“开心”，查询《养花》一文中与“开心”最为接近的5个词语，完整的Python代码如下：（find_similar_words.py）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">from</span> bert_client_lmj <span class="hljs-keyword">import</span> Encoding<br><span class="hljs-keyword">from</span> operator <span class="hljs-keyword">import</span> itemgetter<br><br><span class="hljs-comment"># 读取文章</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./doc.txt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    content = f.read().replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br><br>ec = Encoding()<br>similar_word_dict = &#123;&#125;<br><br><span class="hljs-comment"># 查找文章中与&#x27;开心&#x27;的最接近的词语</span><br>words = <span class="hljs-built_in">list</span>(jieba.cut(content))<br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>    <span class="hljs-built_in">print</span>(word)<br>    <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> similar_word_dict.keys():<br>        similar_word_dict[word] = ec.query_similarity([word, <span class="hljs-string">&#x27;开心&#x27;</span>])<br><br><span class="hljs-comment"># 按相似度从高到低排序</span><br>sorted_dict = <span class="hljs-built_in">sorted</span>(similar_word_dict.items(), key=itemgetter(<span class="hljs-number">1</span>), reverse=<span class="hljs-literal">True</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;与%s最接近的5个词语及相似度如下：&#x27;</span> % <span class="hljs-string">&#x27;开心&#x27;</span>)<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> sorted_dict[:<span class="hljs-number">5</span>]:<br>    <span class="hljs-built_in">print</span>(_)<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs 1c">与开心最接近的<span class="hljs-number">5</span>个词语及相似度如下：<br>(&#x27;难过&#x27;, <span class="hljs-number">0.9070794</span>)<br>(&#x27;高兴&#x27;, <span class="hljs-number">0.89517105</span>)<br>(&#x27;乐趣&#x27;, <span class="hljs-number">0.89260685</span>)<br>(&#x27;骄傲&#x27;, <span class="hljs-number">0.87363803</span>)<br>(&#x27;我爱花&#x27;, <span class="hljs-number">0.86954254</span>)<br></code></pre></td></tr></table></figure><h3 id="提取文本中的实体">提取文本中的实体</h3><p>在事件抽取中，我们往往需要抽取一些指定的元素，比如在下面的句子中，</p><blockquote><p>巴基斯坦当地时间2014年12月16日早晨，巴基斯坦塔利班运动武装分子袭击了西北部白沙瓦市一所军人子弟学校，打死141人，其中132人为12岁至16岁的学生。</p></blockquote><p>我们需要抽取袭击者，也就是恐怖组织这个元素。</p><p>直接从句法分析，也许可以得到一定的效果，但由于事件描述方式多变，句法分析会显得比较复杂且效果不一定能保证。这时候，我们尝试BERT词向量，它在一定程度上可以作为补充策略，帮助我们定位到事件的元素。具体的想法如下：</p><ul><li>指定事件元素模板</li><li>句子分词，对词语做n-gram</li><li>查询每个n-gram与模板的相似度</li><li>按相似度对n-gram排序，取相似度最高的n-gram</li></ul><p>在这里，我们的事件元素为恐怖组织，指定的模板为“伊斯兰组织”，完整的Python程序如下（find_similar_entity_in_sentence.py）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">from</span> operator <span class="hljs-keyword">import</span> itemgetter<br><span class="hljs-keyword">from</span> bert_client_lmj <span class="hljs-keyword">import</span> Encoding<br><br><span class="hljs-comment"># 创建n-gram</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_ngrams</span>(<span class="hljs-params">sequence, n</span>):<br>    lst = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(*[sequence[index:] <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)]))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(lst)):<br>        lst[i] = <span class="hljs-string">&#x27;&#x27;</span>.join(lst[i])<br>    <span class="hljs-keyword">return</span> lst<br><br><span class="hljs-comment"># 模板</span><br>template = <span class="hljs-string">&#x27;伊斯兰组织&#x27;</span><br><span class="hljs-comment"># 示例句子</span><br>doc = <span class="hljs-string">&quot;巴基斯坦当地时间2014年12月16日早晨，巴基斯坦塔利班运动武装分子袭击了西北部白沙瓦市一所军人子弟学校，打死141人，其中132人为12岁至16岁的学生。&quot;</span><br><br>words = <span class="hljs-built_in">list</span>(jieba.cut(doc))<br>all_lst = []<br><span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>):<br>    all_lst.extend(compute_ngrams(words, j))<br><br>ec = Encoding()<br>similar_word_dict = &#123;&#125;<br><br><span class="hljs-comment"># 查找文章中与template的最接近的词语</span><br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> all_lst:<br>    <span class="hljs-built_in">print</span>(word)<br>    <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> similar_word_dict.keys():<br>        similar_word_dict[word] = ec.query_similarity([word, template])<br><br><span class="hljs-comment"># 按相似度从高到低排序</span><br>sorted_dict = <span class="hljs-built_in">sorted</span>(similar_word_dict.items(), key=itemgetter(<span class="hljs-number">1</span>), reverse=<span class="hljs-literal">True</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;与%s最接近的实体是: %s，相似度为 %s.&#x27;</span> %(template, sorted_dict[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], sorted_dict[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]))<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dns">与伊斯兰组织最接近的实体是: 塔利班运动武装分子，相似度为 <span class="hljs-number">0.8953854</span>.<br></code></pre></td></tr></table></figure><p>可以看到，该算法成功地帮助我们定位到了恐怖组织：塔利班运动武装分子，效果很好，但是由于是无监督产生的词向量，效果不一定可控，而且该算法运行速度较慢，这点可以从工程上加以改进。</p><h3 id="问答中的实体对齐">问答中的实体对齐</h3><p>在智能问答中，我们往往会采用知识图谱或者数据库存储实体，其中一个难点就是实体对齐。举个例子，我们在数据库中储存的实体如下：（entities.txt）</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-number">094</span>型/晋级<br><span class="hljs-number">052</span>C型（旅洋Ⅱ级）<br>辽宁舰<span class="hljs-regexp">/瓦良格/</span>Varyag<br>杰拉尔德·R·福特号航空母舰<br><span class="hljs-number">052</span>D型（旅洋III级）<br><span class="hljs-number">054</span>A型<br>CVN-<span class="hljs-number">72</span><span class="hljs-regexp">/林肯号/</span>Lincoln<br></code></pre></td></tr></table></figure><p>这样的实体名字很复杂，如果用户想查询实体“辽宁舰”，就会碰到困难，但是由于实体以储存在数据库或知识图谱中，实体不好直接修改。一种办法是通过关键字匹配定位实体，在这里，我们可以借助BERT词向量来实现，完整的Python代码如下：（Entity_Alignment.py）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><span class="hljs-keyword">from</span> bert_client_lmj <span class="hljs-keyword">import</span> Encoding<br><span class="hljs-keyword">from</span> operator <span class="hljs-keyword">import</span> itemgetter<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;entities.txt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    entities = [_.strip() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> f.readlines()]<br><br>ec = Encoding()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">entity_alignment</span>(<span class="hljs-params">query</span>):<br><br>    similar_word_dict = &#123;&#125;<br><br>    <span class="hljs-comment"># 查找已有实体中与query最接近的实体</span><br>    <span class="hljs-keyword">for</span> entity <span class="hljs-keyword">in</span> entities:<br>        <span class="hljs-keyword">if</span> entity <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> similar_word_dict.keys():<br>            similar_word_dict[entity] = ec.query_similarity([entity, query])<br><br>    <span class="hljs-comment"># 按相似度从高到低排序</span><br>    sorted_dict = <span class="hljs-built_in">sorted</span>(similar_word_dict.items(), key=itemgetter(<span class="hljs-number">1</span>), reverse=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">return</span> sorted_dict[<span class="hljs-number">0</span>]<br><br>query = <span class="hljs-string">&#x27;辽宁舰&#x27;</span><br>result = entity_alignment(query)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;查询实体：%s，匹配实体：%s 。&#x27;</span> %(query, result))<br><br>query = <span class="hljs-string">&#x27;林肯号&#x27;</span><br>result = entity_alignment(query)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;查询实体：%s，匹配实体：%s 。&#x27;</span> %(query, result))<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs 1c">查询实体：辽宁舰，匹配实体：(&#x27;辽宁舰/瓦良格/Varyag&#x27;, <span class="hljs-number">0.8534695</span>) 。<br>查询实体：林肯号，匹配实体：(&#x27;CVN-72/林肯号/Lincoln&#x27;, <span class="hljs-number">0.8389378</span>) 。<br></code></pre></td></tr></table></figure><p>在这里，查询的速度应该不是困难，因为我们可以将已储存的实体以离线的方式查询其词向量并储存，这样进来一个查询到实体，只查询一次词向量，并计算其与离线的词向量的相似度。这种方法也存在缺陷，主要是由于词向量的无监督，实体对齐有时候不会很准，但作为一种补充策略，也许可以考虑。</p><h3 id="总结">总结</h3><p>本文介绍了笔者这段时间所思考的BERT词向量的几个应用，由于能力有限，文章中会存在考虑不当的地方，还请读者多多批评指正。</p><p>另外，笔者将会持续调研词向量方面的技术，比如腾讯词向量，百度词向量等，欢迎大家关注～</p><blockquote><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BERT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（十）使用LSTM进行文本情感分析</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%8D%81%EF%BC%89%E4%BD%BF%E7%94%A8LSTM%E8%BF%9B%E8%A1%8C%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%8D%81%EF%BC%89%E4%BD%BF%E7%94%A8LSTM%E8%BF%9B%E8%A1%8C%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h4 id="情感分析简介">情感分析简介</h4><p>文本情感分析（SentimentAnalysis）是自然语言处理（NLP）方法中常见的应用，也是一个有趣的基本任务，尤其是以提炼文本情绪内容为目的的分类。它是对带有情感色彩的主观性文本进行分析、处理、归纳和推理的过程。</p><p>本文将介绍情感分析中的情感极性（倾向）分析。所谓情感极性分析，指的是对文本进行褒义、贬义、中性的判断。在大多应用场景下，只分为两类。例如对于“喜爱”和“厌恶”这两个词，就属于不同的情感倾向。</p><p>本文将详细介绍如何使用深度学习模型中的LSTM模型来实现文本的情感分析。</p><h3 id="文本介绍及语料分析">文本介绍及语料分析</h3><p>我们以某电商网站中某个商品的评论作为语料（corpus.csv），该数据集的下载网址为：<ahref="https://github.com/renjunxiang/Text-Classification/blob/master/TextClassification/data/data_single.csv">https://github.com/renjunxiang/Text-Classification/blob/master/TextClassification/data/data_single.csv</a>，该数据集一共有4310条评论数据，文本的情感分为两类：“正面”和“反面”，该数据集的前几行如下：</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey"><span class="hljs-built_in">evaluation,</span>label<br>用了一段时间，感觉还不错，可以,正面<br>电视非常好，已经是家里的第二台了。第一天下单，第二天就到本地了，可是物流的人说车坏了，一直催，客服也帮着催，到第三天下午<span class="hljs-number">5</span>点才送过来。父母年纪大了，买个大电视画面清晰，趁着耳朵还好使，享受几年。,正面<br>电视比想象中的大好多，画面也很清晰，系统很智能，更多功能还在摸索中,正面<br>不错,正面<br>用了这么多天了，感觉还不错。夏普的牌子还是比较可靠。希望以后比较耐用，现在是考量质量的时候。,正面<br>物流速度很快，非常棒，今天就看了电视，非常清晰，非常流畅，一次非常完美的购物体验,正面<br>非常好，客服还特意打电话做回访,正面<br>物流小哥不错，辛苦了，东西还没用,正面<br>送货速度快，质量有保障，活动价格挺好的。希望用的久，不出问题。,正面<br></code></pre></td></tr></table></figure><p>接着我们需要对语料做一个简单的分析：</p><ul><li><p>数据集中的情感分布；</p></li><li><p>数据集中的评论句子长度分布。</p><p>使用以下Python脚本，我们可以统计出数据集中的情感分布以及评论句子长度分布。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> font_manager<br><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> accumulate<br><br><span class="hljs-comment"># 设置matplotlib绘图时的字体</span><br>my_font = font_manager.FontProperties(fname=<span class="hljs-string">&quot;/Library/Fonts/Songti.ttc&quot;</span>)<br><br><span class="hljs-comment"># 统计句子长度及长度出现的频数</span><br>df = pd.read_csv(<span class="hljs-string">&#x27;./corpus.csv&#x27;</span>)<br><span class="hljs-built_in">print</span>(df.groupby(<span class="hljs-string">&#x27;label&#x27;</span>)[<span class="hljs-string">&#x27;label&#x27;</span>].count())<br><br>df[<span class="hljs-string">&#x27;length&#x27;</span>] = df[<span class="hljs-string">&#x27;evaluation&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x))<br>len_df = df.groupby(<span class="hljs-string">&#x27;length&#x27;</span>).count()<br>sent_length = len_df.index.tolist()<br>sent_freq = len_df[<span class="hljs-string">&#x27;evaluation&#x27;</span>].tolist()<br><br><span class="hljs-comment"># 绘制句子长度及出现频数统计图</span><br>plt.bar(sent_length, sent_freq)<br>plt.title(<span class="hljs-string">&quot;句子长度及出现频数统计图&quot;</span>, fontproperties=my_font)<br>plt.xlabel(<span class="hljs-string">&quot;句子长度&quot;</span>, fontproperties=my_font)<br>plt.ylabel(<span class="hljs-string">&quot;句子长度出现的频数&quot;</span>, fontproperties=my_font)<br>plt.savefig(<span class="hljs-string">&quot;./句子长度及出现频数统计图.png&quot;</span>)<br>plt.close()<br><br><span class="hljs-comment"># 绘制句子长度累积分布函数(CDF)</span><br>sent_pentage_list = [(count/<span class="hljs-built_in">sum</span>(sent_freq)) <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> accumulate(sent_freq)]<br><br><span class="hljs-comment"># 绘制CDF</span><br>plt.plot(sent_length, sent_pentage_list)<br><br><span class="hljs-comment"># 寻找分位点为quantile的句子长度</span><br>quantile = <span class="hljs-number">0.91</span><br><span class="hljs-comment">#print(list(sent_pentage_list))</span><br><span class="hljs-keyword">for</span> length, per <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(sent_length, sent_pentage_list):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">round</span>(per, <span class="hljs-number">2</span>) == quantile:<br>        index = length<br>        <span class="hljs-keyword">break</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n分位点为%s的句子长度:%d.&quot;</span> % (quantile, index))<br><br><span class="hljs-comment"># 绘制句子长度累积分布函数图</span><br>plt.plot(sent_length, sent_pentage_list)<br>plt.hlines(quantile, <span class="hljs-number">0</span>, index, colors=<span class="hljs-string">&quot;c&quot;</span>, linestyles=<span class="hljs-string">&quot;dashed&quot;</span>)<br>plt.vlines(index, <span class="hljs-number">0</span>, quantile, colors=<span class="hljs-string">&quot;c&quot;</span>, linestyles=<span class="hljs-string">&quot;dashed&quot;</span>)<br>plt.text(<span class="hljs-number">0</span>, quantile, <span class="hljs-built_in">str</span>(quantile))<br>plt.text(index, <span class="hljs-number">0</span>, <span class="hljs-built_in">str</span>(index))<br>plt.title(<span class="hljs-string">&quot;句子长度累积分布函数图&quot;</span>, fontproperties=my_font)<br>plt.xlabel(<span class="hljs-string">&quot;句子长度&quot;</span>, fontproperties=my_font)<br>plt.ylabel(<span class="hljs-string">&quot;句子长度累积频率&quot;</span>, fontproperties=my_font)<br>plt.savefig(<span class="hljs-string">&quot;./句子长度累积分布函数图.png&quot;</span>)<br>plt.close()<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">label</span><br><span class="hljs-string">正面</span>    <span class="hljs-number">1908</span><br><span class="hljs-string">负面</span>    <span class="hljs-number">2375</span><br><span class="hljs-attr">Name:</span> <span class="hljs-string">label,</span> <span class="hljs-attr">dtype:</span> <span class="hljs-string">int64</span><br><br><span class="hljs-string">分位点为0.91的句子长度:183.</span><br></code></pre></td></tr></table></figure><p>可以看到，正反面两类情感的比例差不多。句子长度及出现频数统计图如下：</p><p><img src="/img/nlp10_1.png" /></p><p>句子长度累积分布函数图如下：</p><p><img src="/img/nlp10_2.png" /></p><p>可以看到，大多数样本的句子长度集中在1-200之间，句子长度累计频率取0.91分位点，则长度为183左右。</p><h3 id="使用lstm模型">使用LSTM模型</h3><p>接着我们使用深度学习中的LSTM模型来对上述数据集做情感分析，笔者实现的模型框架如下：</p><figure><img src="/img/nlp10_3.png" alt="模型结构图" /><figcaption aria-hidden="true">模型结构图</figcaption></figure><p>完整的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> np_utils, plot_model<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> LSTM, Dense, Embedding, Dropout<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><br><span class="hljs-comment"># 导入数据</span><br><span class="hljs-comment"># 文件的数据中，特征为evaluation, 类别为label.</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>(<span class="hljs-params">filepath, input_shape=<span class="hljs-number">20</span></span>):<br>    df = pd.read_csv(filepath)<br><br>    <span class="hljs-comment"># 标签及词汇表</span><br>    labels, vocabulary = <span class="hljs-built_in">list</span>(df[<span class="hljs-string">&#x27;label&#x27;</span>].unique()), <span class="hljs-built_in">list</span>(df[<span class="hljs-string">&#x27;evaluation&#x27;</span>].unique())<br><br>    <span class="hljs-comment"># 构造字符级别的特征</span><br>    string = <span class="hljs-string">&#x27;&#x27;</span><br>    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> vocabulary:<br>        string += word<br><br>    vocabulary = <span class="hljs-built_in">set</span>(string)<br><br>    <span class="hljs-comment"># 字典列表</span><br>    word_dictionary = &#123;word: i+<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocabulary)&#125;<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;word_dict.pk&#x27;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        pickle.dump(word_dictionary, f)<br>    inverse_word_dictionary = &#123;i+<span class="hljs-number">1</span>: word <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocabulary)&#125;<br>    label_dictionary = &#123;label: i <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels)&#125;<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;label_dict.pk&#x27;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        pickle.dump(label_dictionary, f)<br>    output_dictionary = &#123;i: labels <span class="hljs-keyword">for</span> i, labels <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels)&#125;<br><br>    vocab_size = <span class="hljs-built_in">len</span>(word_dictionary.keys()) <span class="hljs-comment"># 词汇表大小</span><br>    label_size = <span class="hljs-built_in">len</span>(label_dictionary.keys()) <span class="hljs-comment"># 标签类别数量</span><br><br>    <span class="hljs-comment"># 序列填充，按input_shape填充，长度不足的按0补充</span><br>    x = [[word_dictionary[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> df[<span class="hljs-string">&#x27;evaluation&#x27;</span>]]<br>    x = pad_sequences(maxlen=input_shape, sequences=x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br>    y = [[label_dictionary[sent]] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> df[<span class="hljs-string">&#x27;label&#x27;</span>]]<br>    y = [np_utils.to_categorical(label, num_classes=label_size) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> y]<br>    y = np.array([<span class="hljs-built_in">list</span>(_[<span class="hljs-number">0</span>]) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> y])<br><br>    <span class="hljs-keyword">return</span> x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary<br><br><span class="hljs-comment"># 创建深度学习模型， Embedding + LSTM + Softmax.</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_LSTM</span>(<span class="hljs-params">n_units, input_shape, output_dim, filepath</span>):<br>    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = load_data(filepath)<br>    model = Sequential()<br>    model.add(Embedding(input_dim=vocab_size + <span class="hljs-number">1</span>, output_dim=output_dim,<br>                        input_length=input_shape, mask_zero=<span class="hljs-literal">True</span>))<br>    model.add(LSTM(n_units, input_shape=(x.shape[<span class="hljs-number">0</span>], x.shape[<span class="hljs-number">1</span>])))<br>    model.add(Dropout(<span class="hljs-number">0.2</span>))<br>    model.add(Dense(label_size, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br>    model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>, optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>, metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br>    plot_model(model, to_file=<span class="hljs-string">&#x27;./model_lstm.png&#x27;</span>, show_shapes=<span class="hljs-literal">True</span>)<br>    model.summary()<br><br>    <span class="hljs-keyword">return</span> model<br><br><span class="hljs-comment"># 模型训练</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_train</span>(<span class="hljs-params">input_shape, filepath, model_save_path</span>):<br><br>    <span class="hljs-comment"># 将数据集分为训练集和测试集，占比为9:1</span><br>    <span class="hljs-comment"># input_shape = 100</span><br>    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = load_data(filepath, input_shape)<br>    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = <span class="hljs-number">0.1</span>, random_state = <span class="hljs-number">42</span>)<br><br>    <span class="hljs-comment"># 模型输入参数，需要自己根据需要调整</span><br>    n_units = <span class="hljs-number">100</span><br>    batch_size = <span class="hljs-number">32</span><br>    epochs = <span class="hljs-number">5</span><br>    output_dim = <span class="hljs-number">20</span><br><br>    <span class="hljs-comment"># 模型训练</span><br>    lstm_model = create_LSTM(n_units, input_shape, output_dim, filepath)<br>    lstm_model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 模型保存</span><br>    lstm_model.save(model_save_path)<br><br>    N = test_x.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 测试的条数</span><br>    predict = []<br>    label = []<br>    <span class="hljs-keyword">for</span> start, end <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, N, <span class="hljs-number">1</span>), <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, N+<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)):<br>        sentence = [inverse_word_dictionary[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> test_x[start] <span class="hljs-keyword">if</span> i != <span class="hljs-number">0</span>]<br>        y_predict = lstm_model.predict(test_x[start:end])<br>        label_predict = output_dictionary[np.argmax(y_predict[<span class="hljs-number">0</span>])]<br>        label_true = output_dictionary[np.argmax(test_y[start:end])]<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#x27;</span>.join(sentence), label_true, label_predict) <span class="hljs-comment"># 输出预测结果</span><br>        predict.append(label_predict)<br>        label.append(label_true)<br><br>    acc = accuracy_score(predict, label) <span class="hljs-comment"># 预测准确率</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;模型在测试集上的准确率为: %s.&#x27;</span> % acc)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    filepath = <span class="hljs-string">&#x27;./corpus.csv&#x27;</span><br>    input_shape = <span class="hljs-number">180</span><br>    model_save_path = <span class="hljs-string">&#x27;./corpus_model.h5&#x27;</span><br>    model_train(input_shape, filepath, model_save_path)<br></code></pre></td></tr></table></figure><p>对上述模型，共训练5次，训练集和测试集比例为9:1，输出的结果为：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">......</span><br><span class="hljs-string">Epoch</span> <span class="hljs-number">5</span><span class="hljs-string">/5</span><br><span class="hljs-string">......</span><br><span class="hljs-number">3424</span><span class="hljs-string">/3854</span> [<span class="hljs-string">=========================&gt;....</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 2s - loss: 0.1280 - acc:</span> <span class="hljs-number">0.9565</span><br><span class="hljs-number">3456</span><span class="hljs-string">/3854</span> [<span class="hljs-string">=========================&gt;....</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1274 - acc:</span> <span class="hljs-number">0.9569</span><br><span class="hljs-number">3488</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==========================&gt;...</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1274 - acc:</span> <span class="hljs-number">0.9570</span><br><span class="hljs-number">3520</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==========================&gt;...</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1287 - acc:</span> <span class="hljs-number">0.9568</span><br><span class="hljs-number">3552</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==========================&gt;...</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1290 - acc:</span> <span class="hljs-number">0.9564</span><br><span class="hljs-number">3584</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==========================&gt;...</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1284 - acc:</span> <span class="hljs-number">0.9568</span><br><span class="hljs-number">3616</span><span class="hljs-string">/3854</span> [<span class="hljs-string">===========================&gt;..</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1284 - acc:</span> <span class="hljs-number">0.9569</span><br><span class="hljs-number">3648</span><span class="hljs-string">/3854</span> [<span class="hljs-string">===========================&gt;..</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1278 - acc:</span> <span class="hljs-number">0.9572</span><br><span class="hljs-number">3680</span><span class="hljs-string">/3854</span> [<span class="hljs-string">===========================&gt;..</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1271 - acc:</span> <span class="hljs-number">0.9576</span><br><span class="hljs-number">3712</span><span class="hljs-string">/3854</span> [<span class="hljs-string">===========================&gt;..</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1268 - acc:</span> <span class="hljs-number">0.9580</span><br><span class="hljs-number">3744</span><span class="hljs-string">/3854</span> [<span class="hljs-string">============================&gt;.</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1279 - acc:</span> <span class="hljs-number">0.9575</span><br><span class="hljs-number">3776</span><span class="hljs-string">/3854</span> [<span class="hljs-string">============================&gt;.</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1272 - acc:</span> <span class="hljs-number">0.9579</span><br><span class="hljs-number">3808</span><span class="hljs-string">/3854</span> [<span class="hljs-string">============================&gt;.</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1279 - acc:</span> <span class="hljs-number">0.9580</span><br><span class="hljs-number">3840</span><span class="hljs-string">/3854</span> [<span class="hljs-string">============================&gt;.</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1281 - acc:</span> <span class="hljs-number">0.9581</span><br><span class="hljs-number">3854</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==============================</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">18s 5ms/step - loss: 0.1298 - acc:</span> <span class="hljs-number">0.9577</span><br><span class="hljs-string">......</span><br><span class="hljs-string">给父母买的，特意用了一段时间再来评价，电视非常好，没有坏点和损坏，界面也很简洁，便于操作，稍微不足就是开机会比普通电视慢一些，这应该是智能电视的通病吧，如果可以希望微鲸大大可以更新系统优化下开机时间~电视真的很棒，性价比爆棚，值得大家考虑购买。</span> <span class="hljs-string">客服很细心，快递小哥很耐心的等我通电验货，态度非常好。</span> <span class="hljs-string">负面</span> <span class="hljs-string">正面</span><br><span class="hljs-string">长须鲸和海狮回答都很及时，虽然物流不够快但是服务不错电视不错，对比了乐视小米和微鲸论性价比还是微鲸好点</span> <span class="hljs-string">负面</span> <span class="hljs-string">负面</span><br><span class="hljs-string">所以看不到4k效果，但是应该可以。</span> <span class="hljs-string">自带音响，中规中矩吧，好像没有别人说的好。而且，到现在没连接上我的漫步者，这个非常不满意，因为看到网上说好像普通3.5mm的连不上或者连上了声音小。希望厂家接下来开发的电视有改进。不知道我要不要换个音响。其他的用用再说。</span> <span class="hljs-string">放在地上的是跟我混了两年的tcl，天气受潮，修了一次，下岗了。</span> <span class="hljs-string">最后，我也觉得底座不算太稳，凑合着用。</span> <span class="hljs-string">负面</span> <span class="hljs-string">负面</span><br><span class="hljs-string">电视机一般，低端机不要求那么高咯。</span> <span class="hljs-string">负面</span> <span class="hljs-string">负面</span><br><span class="hljs-string">很好，两点下单上午就到了，服务很好。</span> <span class="hljs-string">正面</span> <span class="hljs-string">正面</span><br><span class="hljs-string">帮朋友买的，好好好好好好好好</span> <span class="hljs-string">正面</span> <span class="hljs-string">正面</span><br><span class="hljs-string">......</span><br><span class="hljs-string">模型在测试集上的准确率为:</span> <span class="hljs-number">0.9020979020979021</span><span class="hljs-string">.</span><br></code></pre></td></tr></table></figure><p>可以看到，该模型在训练集上的准确率为95%以上，在测试集上的准确率为90%以上，效果还是相当不错的。</p><h3 id="模型预测">模型预测</h3><p>接着，我们利用刚刚训练好的模型，对新的数据进行测试。笔者随机改造上述样本的评论，然后预测其情感倾向。情感预测的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-comment"># Import the necessary modules</span><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><br><br><span class="hljs-comment"># 导入字典</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;word_dict.pk&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    word_dictionary = pickle.load(f)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;label_dict.pk&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    output_dictionary = pickle.load(f)<br><br><span class="hljs-keyword">try</span>:<br>    <span class="hljs-comment"># 数据预处理</span><br>    input_shape = <span class="hljs-number">180</span><br>    sent = <span class="hljs-string">&quot;电视刚安装好，说实话，画质不怎么样，很差！&quot;</span><br>    x = [[word_dictionary[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent]]<br>    x = pad_sequences(maxlen=input_shape, sequences=x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># 载入模型</span><br>    model_save_path = <span class="hljs-string">&#x27;./sentiment_analysis.h5&#x27;</span><br>    lstm_model = load_model(model_save_path)<br><br>    <span class="hljs-comment"># 模型预测</span><br>    y_predict = lstm_model.predict(x)<br>    label_dict = &#123;v:k <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> output_dictionary.items()&#125;<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;输入语句: %s&#x27;</span> % sent)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;情感预测结果: %s&#x27;</span> % label_dict[np.argmax(y_predict)])<br><br><span class="hljs-keyword">except</span> KeyError <span class="hljs-keyword">as</span> err:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;您输入的句子有汉字不在词汇表中，请重新输入！&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;不在词汇表中的单词为：%s.&quot;</span> % err)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><span class="hljs-section">输入语句: 电视刚安装好，说实话，画质不怎么样，很差！</span><br><span class="hljs-section">情感预测结果: 负面</span><br></code></pre></td></tr></table></figure><p>让我们再尝试着测试一些其他的评论：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><span class="hljs-section">输入语句: 物超所值，真心不错</span><br><span class="hljs-section">情感预测结果: 正面</span><br><span class="hljs-section">输入语句: 很大很好，方便安装！</span><br><span class="hljs-section">情感预测结果: 正面</span><br><span class="hljs-section">输入语句: 卡，慢，死机，闪退。</span><br><span class="hljs-section">情感预测结果: 负面</span><br><span class="hljs-section">输入语句: 这种货色就这样吧，别期待怎样。</span><br><span class="hljs-section">情感预测结果: 负面</span><br><span class="hljs-section">输入语句: 啥服务态度码，出了事情一个推一个，送货安装还收我50</span><br><span class="hljs-section">情感预测结果: 负面</span><br><span class="hljs-section">输入语句: 京东服务很好！但我买的这款电视两天后就出现这样的问题，很后悔买了这样的电视</span><br><span class="hljs-section">情感预测结果: 负面</span><br><span class="hljs-section">输入语句: 产品质量不错，就是这位客服的态度十分恶劣，对相关服务不予解释说明，缺乏耐心，</span><br><span class="hljs-section">情感预测结果: 负面</span><br><span class="hljs-section">输入语句: 很满意，电视非常好。护眼模式，很好，也很清晰。</span><br><span class="hljs-section">情感预测结果: 负面</span><br></code></pre></td></tr></table></figure><h3 id="总结">总结</h3><p>当然，该模型并不是对一切该商品的评论都会有好的效果，还是应该针对特定的语料去训练，去预测。</p><p>本文主要介绍了LSTM模型在文本情感分析方面的应用，该项目已上传Github，地址为：<ahref="https://github.com/percent4/Sentiment_Analysis">https://github.com/percent4/Sentiment_Analysis</a>。</p><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注~</p><h3 id="参考文献">参考文献</h3><ol type="1"><li>Python机器学习 -- NLP情感分析：<ahref="https://blog.csdn.net/qq_38328378/article/details/81198322">https://blog.csdn.net/qq_38328378/article/details/81198322</a></li><li>数据集来源：<ahref="https://github.com/renjunxiang/Text-Classification/blob/master/TextClassification/data/data_single.csv">https://github.com/renjunxiang/Text-Classification/blob/master/TextClassification/data/data_single.csv</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>情感分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（九）词义消岐（WSD）的简介与实现</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B9%9D%EF%BC%89%E8%AF%8D%E4%B9%89%E6%B6%88%E5%B2%90%EF%BC%88WSD%EF%BC%89%E7%9A%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%9E%E7%8E%B0/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B9%9D%EF%BC%89%E8%AF%8D%E4%B9%89%E6%B6%88%E5%B2%90%EF%BC%88WSD%EF%BC%89%E7%9A%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="词义消岐简介">词义消岐简介</h3><p>词义消岐，英文名称为Word SenseDisambiguation，英语缩写为WSD，是自然语言处理（NLP）中一个非常有趣的基本任务。</p><p>那么，什么是词义消岐呢？通常，在我们的自然语言中，不管是英语，还是中文，都有多义词存在。这些多义词的存在，会让人对句子的意思产生混淆，但人通过学习又是可以正确地区分出来的。</p><p>以<strong>“小米”</strong>这个词为例，如果仅仅只是说“小米”这个词语，你并不知道它实际指的到底是小米科技公司还是谷物。但当我们把词语置于某个特定的语境中，我们能很好地区分出这个词语的意思。比如，</p><blockquote><p>雷军是小米的创始人。</p></blockquote><p>在这个句子中，我们知道这个“小米”指的是小米科技公司。比如</p><blockquote><p>我今天早上喝了一碗小米粥。</p></blockquote><p>在这个句子中，“小米”指的是谷物、农作物。</p><p>所谓词义消岐，指的是在特定的语境中，识别出某个歧义词的正确含义。</p><p>那么，词义消岐有什么作用呢？词义消岐可以很好地服务于语言翻译和智能问答领域，当然，还有许多应用有待开发～</p><h3 id="词义消岐实现">词义消岐实现</h3><p>在目前的词义消岐算法中，有不少原创算法，有些实现起来比较简单，有些想法较为复杂，但实现的效果普遍都不是很好。比较经典的词义消岐的算法为Lesk算法，该算法的想法很简单，通过对某个歧义词构建不同含义的语料及待判别句子中该词语与语料的重合程度来实现，具体的算法原理可参考网址：<ahref="https://en.wikipedia.org/wiki/Lesk_algorithm">https://en.wikipedia.org/wiki/Lesk_algorithm</a>.</p><p>在下面的部分中，笔者将会介绍自己想的一种实现词义消岐的算法，仅仅是一个想法，仅供参考。</p><p>我们以词语“火箭”为例，选取其中的两个<strong>义项</strong>（同一个词语的不同含义）：<ahref="https://baike.baidu.com/item/%E7%81%AB%E7%AE%AD/8794081#viewPageContent"title="NBA球队名">NBA球队名</a> 和 <ahref="https://baike.baidu.com/item/%E7%81%AB%E7%AE%AD/6308#viewPageContent"title="燃气推进装置">燃气推进装置</a> ，如下：</p><p><img src="/img/nlp9_1.png" /></p><h4 id="获取语料">获取语料</h4><p>首先，我们利用爬虫爬取这两个义项的百度百科网页，以句子为单位，只要句子中出现该词语，则把这句话加入到这个义项的预料中。爬虫的完整Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> SentenceSplitter<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WebScrape</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, word, url</span>):<br>        self.url = url<br>        self.word = word<br><br>    <span class="hljs-comment"># 爬取百度百科页面</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">web_parse</span>(<span class="hljs-params">self</span>):<br>        headers = &#123;<span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \</span><br><span class="hljs-string">                                             (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36&#x27;</span>&#125;<br>        req = requests.get(url=self.url, headers=headers)<br><br>        <span class="hljs-comment"># 解析网页，定位到main-content部分</span><br>        <span class="hljs-keyword">if</span> req.status_code == <span class="hljs-number">200</span>:<br>            soup = BeautifulSoup(req.text.encode(req.encoding), <span class="hljs-string">&#x27;lxml&#x27;</span>)<br>            <span class="hljs-keyword">return</span> soup<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># 获取该词语的义项</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_gloss</span>(<span class="hljs-params">self</span>):<br>        soup = self.web_parse()<br>        <span class="hljs-keyword">if</span> soup:<br>            lis = soup.find(<span class="hljs-string">&#x27;ul&#x27;</span>, class_=<span class="hljs-string">&quot;polysemantList-wrapper cmn-clearfix&quot;</span>)<br>            <span class="hljs-keyword">if</span> lis:<br>                <span class="hljs-keyword">for</span> li <span class="hljs-keyword">in</span> lis(<span class="hljs-string">&#x27;li&#x27;</span>):<br>                    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;&lt;a&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">str</span>(li):<br>                        gloss = li.text.replace(<span class="hljs-string">&#x27;▪&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>                        <span class="hljs-keyword">return</span> gloss<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># 获取该义项的语料，以句子为单位</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_content</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 发送HTTP请求</span><br>        result = []<br>        soup = self.web_parse()<br>        <span class="hljs-keyword">if</span> soup:<br>            paras = soup.find(<span class="hljs-string">&#x27;div&#x27;</span>, class_=<span class="hljs-string">&#x27;main-content&#x27;</span>).text.split(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>            <span class="hljs-keyword">for</span> para <span class="hljs-keyword">in</span> paras:<br>                <span class="hljs-keyword">if</span> self.word <span class="hljs-keyword">in</span> para:<br>                    sents = <span class="hljs-built_in">list</span>(SentenceSplitter.split(para))<br>                    <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>                        <span class="hljs-keyword">if</span> self.word <span class="hljs-keyword">in</span> sent:<br>                            sent = sent.replace(<span class="hljs-string">&#x27;\xa0&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>).replace(<span class="hljs-string">&#x27;\u3000&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>                            result.append(sent)<br><br>        result = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(result))<br><br>        <span class="hljs-keyword">return</span> result<br><br>    <span class="hljs-comment"># 将该义项的语料写入到txt</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">write_2_file</span>(<span class="hljs-params">self</span>):<br>        gloss = self.get_gloss()<br>        result = self.get_content()<br>        <span class="hljs-built_in">print</span>(gloss)<br>        <span class="hljs-built_in">print</span>(result)<br>        <span class="hljs-keyword">if</span> result <span class="hljs-keyword">and</span> gloss:<br>            <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./%s_%s.txt&#x27;</span>% (self.word, gloss), <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>                f.writelines([_+<span class="hljs-string">&#x27;\n&#x27;</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> result])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>(<span class="hljs-params">self</span>):<br>        self.write_2_file()<br><br><span class="hljs-comment"># NBA球队名</span><br><span class="hljs-comment">#url = &#x27;https://baike.baidu.com/item/%E4%BC%91%E6%96%AF%E6%95%A6%E7%81%AB%E7%AE%AD%E9%98%9F/370758?fromtitle=%E7%81%AB%E7%AE%AD&amp;fromid=8794081#viewPageContent&#x27;</span><br><span class="hljs-comment"># 燃气推进装置</span><br>url = <span class="hljs-string">&#x27;https://baike.baidu.com/item/%E7%81%AB%E7%AE%AD/6308#viewPageContent&#x27;</span><br>WebScrape(<span class="hljs-string">&#x27;火箭&#x27;</span>, url).run()<br></code></pre></td></tr></table></figure><p>利用这个爬虫，我们爬取了“火箭”这个词语的两个义项的语料，生成了火箭_燃气推进装置.txt文件和火箭_NBA球队名.txt文件，这两个文件分别含有361和171个句子。以火箭_燃气推进装置.txt文件为例，前10个句子如下：</p><blockquote><p>火箭技术的飞速发展，不仅可提供更加完善的各类导弹和推动相关科学的发展，还将使开发空间资源、建立空间产业、空间基地及星际航行等成为可能。火箭技术是一项十分复杂的综合性技术，主要包括火箭推进技术、总体设计技术、火箭结构技术、控制和制导技术、计划管理技术、可靠性和质量控制技术、试验技术，对导弹来说还有弹头制导和控制、1903年，俄国的К.E.齐奥尔科夫斯基提出了制造大型液体火箭的设想和设计原理。火箭有很多种，原始的火箭是用引火物附在弓箭头上，然后射到敌人身上引起焚烧的一种箭矢。“长征三号丙”火箭是在 “长征三号乙”火箭的基础上，减少了两个助推器并取消了助推器上的尾翼。 火箭与导弹有什么区别为了能够在未来大规模的将人类送入太空，不可能依赖传统的火箭和飞船。火箭V2火箭探测高层大气的物理特征（如气压、温度、湿度等）和现象的探空火箭。可一次发射一发至数十发火箭弹。</p></blockquote><h4 id="实现算法">实现算法</h4><p>我们以句子为单位进行词义消岐，即输入一句话，识别出该句子中某个歧义词的含义。笔者使用的算法比较简单，是以TF-IDF为权重的频数判别。以句子</p><blockquote><p>赛季初的时候，火箭是众望所归的西部决赛球队。</p></blockquote><p>为例，对该句子分词后，去掉停用词（stopwords），然后分别统计除了“火箭”这个词以外的TF-IDF值，累加起来,比较在两个义项下这个值的大小即可。</p><p>实现这个算法的完整Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> log2<br><br><span class="hljs-comment"># 读取每个义项的语料</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_file</span>(<span class="hljs-params">path</span>):<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(path, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        lines = [_.strip() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> f.readlines()]<br>        <span class="hljs-keyword">return</span> lines<br><br><span class="hljs-comment"># 对示例句子分词</span><br>sent = <span class="hljs-string">&#x27;赛季初的时候，火箭是众望所归的西部决赛球队。&#x27;</span><br>wsd_word = <span class="hljs-string">&#x27;火箭&#x27;</span><br><br>jieba.add_word(wsd_word)<br>sent_words = <span class="hljs-built_in">list</span>(jieba.cut(sent, cut_all=<span class="hljs-literal">False</span>))<br><br><span class="hljs-comment"># 去掉停用词</span><br>stopwords = [wsd_word, <span class="hljs-string">&#x27;我&#x27;</span>, <span class="hljs-string">&#x27;你&#x27;</span>, <span class="hljs-string">&#x27;它&#x27;</span>, <span class="hljs-string">&#x27;他&#x27;</span>, <span class="hljs-string">&#x27;她&#x27;</span>, <span class="hljs-string">&#x27;了&#x27;</span>, <span class="hljs-string">&#x27;是&#x27;</span>, <span class="hljs-string">&#x27;的&#x27;</span>, <span class="hljs-string">&#x27;啊&#x27;</span>, <span class="hljs-string">&#x27;谁&#x27;</span>, <span class="hljs-string">&#x27;什么&#x27;</span>,<span class="hljs-string">&#x27;都&#x27;</span>,\<br>             <span class="hljs-string">&#x27;很&#x27;</span>, <span class="hljs-string">&#x27;个&#x27;</span>, <span class="hljs-string">&#x27;之&#x27;</span>, <span class="hljs-string">&#x27;人&#x27;</span>, <span class="hljs-string">&#x27;在&#x27;</span>, <span class="hljs-string">&#x27;上&#x27;</span>, <span class="hljs-string">&#x27;下&#x27;</span>, <span class="hljs-string">&#x27;左&#x27;</span>, <span class="hljs-string">&#x27;右&#x27;</span>, <span class="hljs-string">&#x27;。&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>, <span class="hljs-string">&#x27;！&#x27;</span>, <span class="hljs-string">&#x27;？&#x27;</span>]<br><br>sent_cut = []<br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent_words:<br>    <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stopwords:<br>        sent_cut.append(word)<br><br><span class="hljs-built_in">print</span>(sent_cut)<br><br><br><span class="hljs-comment"># 计算其他词的TF-IDF以及频数</span><br>wsd_dict = &#123;&#125;<br><span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> os.listdir(<span class="hljs-string">&#x27;.&#x27;</span>):<br>    <span class="hljs-keyword">if</span> wsd_word <span class="hljs-keyword">in</span> file:<br>        wsd_dict[file.replace(<span class="hljs-string">&#x27;.txt&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)] = read_file(file)<br><br><span class="hljs-comment"># 统计每个词语在语料中出现的次数</span><br>tf_dict = &#123;&#125;<br><span class="hljs-keyword">for</span> meaning, sents <span class="hljs-keyword">in</span> wsd_dict.items():<br>    tf_dict[meaning] = []<br>    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent_cut:<br>        word_count = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>            example = <span class="hljs-built_in">list</span>(jieba.cut(sent, cut_all=<span class="hljs-literal">False</span>))<br>            word_count += example.count(word)<br><br>        <span class="hljs-keyword">if</span> word_count:<br>            tf_dict[meaning].append((word, word_count))<br><br>idf_dict = &#123;&#125;<br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent_cut:<br>    document_count = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> meaning, sents <span class="hljs-keyword">in</span> wsd_dict.items():<br>        <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> sent:<br>                document_count += <span class="hljs-number">1</span><br><br>    idf_dict[word] = document_count<br><br><span class="hljs-comment"># 输出值</span><br>total_document = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> meaning, sents <span class="hljs-keyword">in</span> wsd_dict.items():<br>    total_document += <span class="hljs-built_in">len</span>(sents)<br><br><span class="hljs-comment"># 计算tf_idf值</span><br>mean_tf_idf = []<br><span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> tf_dict.items():<br>    <span class="hljs-built_in">print</span>(k+<span class="hljs-string">&#x27;:&#x27;</span>)<br>    tf_idf_sum = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> v:<br>        word = item[<span class="hljs-number">0</span>]<br>        tf = item[<span class="hljs-number">1</span>]<br>        tf_idf = item[<span class="hljs-number">1</span>]*log2(total_document/(<span class="hljs-number">1</span>+idf_dict[word]))<br>        tf_idf_sum += tf_idf<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;%s, 频数为: %s, TF-IDF值为: %s&#x27;</span>% (word, tf, tf_idf))<br><br>    mean_tf_idf.append((k, tf_idf_sum))<br><br>sort_array = <span class="hljs-built_in">sorted</span>(mean_tf_idf, key=<span class="hljs-keyword">lambda</span> x:x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)<br>true_meaning = sort_array[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].split(<span class="hljs-string">&#x27;_&#x27;</span>)[<span class="hljs-number">1</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n经过词义消岐，%s在该句子中的意思为 %s .&#x27;</span> % (wsd_word, true_meaning))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">[&#x27;赛季&#x27;, &#x27;初&#x27;, &#x27;时候&#x27;, &#x27;众望所归&#x27;, &#x27;西部&#x27;, &#x27;决赛&#x27;, &#x27;球队&#x27;]</span><br><span class="hljs-attribute">火箭_燃气推进装置</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">初, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 12.49585502688717</span><br><span class="hljs-attribute">火箭_NBA球队名</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">赛季, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">63, TF-IDF值为: 204.6194333469459</span><br><span class="hljs-attribute">初, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">1, TF-IDF值为: 6.247927513443585</span><br><span class="hljs-attribute">时候, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">1, TF-IDF值为: 8.055282435501189</span><br><span class="hljs-attribute">西部, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">16, TF-IDF值为: 80.88451896801904</span><br><span class="hljs-attribute">决赛, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">7, TF-IDF值为: 33.13348038429679</span><br><span class="hljs-attribute">球队, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">40, TF-IDF值为: 158.712783770034</span><br><br>经过词义消岐，火箭在该句子中的意思为 NBA球队名 .<br></code></pre></td></tr></table></figure><h4 id="测试">测试</h4><p>接着，我们对上面的算法和程序进行更多的测试。</p><p>输入句子为:</p><blockquote><p>三十多年前，战士们在戈壁滩白手起家，建起了我国的火箭发射基地。</p></blockquote><p>输出结果为:</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs prolog">[<span class="hljs-string">&#x27;三十多年&#x27;</span>, <span class="hljs-string">&#x27;前&#x27;</span>, <span class="hljs-string">&#x27;战士&#x27;</span>, <span class="hljs-string">&#x27;们&#x27;</span>, <span class="hljs-string">&#x27;戈壁滩&#x27;</span>, <span class="hljs-string">&#x27;白手起家&#x27;</span>, <span class="hljs-string">&#x27;建起&#x27;</span>, <span class="hljs-string">&#x27;我国&#x27;</span>, <span class="hljs-string">&#x27;发射&#x27;</span>, <span class="hljs-string">&#x27;基地&#x27;</span>]<br>火箭<span class="hljs-symbol">_</span>燃气推进装置:<br>前, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">9.063440958888354</span><br>们, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">6.05528243550119</span><br>我国, 频数为: <span class="hljs-number">3</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">22.410959804340102</span><br>发射, 频数为: <span class="hljs-number">89</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">253.27878721862933</span><br>基地, 频数为: <span class="hljs-number">7</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">42.38697704850833</span><br>火箭<span class="hljs-symbol">_NBA</span>球队名:<br>前, 频数为: <span class="hljs-number">3</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">13.59516143833253</span><br>们, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">6.05528243550119</span><br><br>经过词义消岐，火箭在该句子中的意思为 燃气推进装置 .<br></code></pre></td></tr></table></figure><p>输入句子为：</p><blockquote><p>对于马刺这样级别的球队，常规赛只有屈指可数的几次交锋具有真正的意义，今天对火箭一役是其中之一。</p></blockquote><p>输出结果为：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs prolog">[<span class="hljs-string">&#x27;对于&#x27;</span>, <span class="hljs-string">&#x27;马刺&#x27;</span>, <span class="hljs-string">&#x27;这样&#x27;</span>, <span class="hljs-string">&#x27;级别&#x27;</span>, <span class="hljs-string">&#x27;球队&#x27;</span>, <span class="hljs-string">&#x27;常规赛&#x27;</span>, <span class="hljs-string">&#x27;只有&#x27;</span>, <span class="hljs-string">&#x27;屈指可数&#x27;</span>, <span class="hljs-string">&#x27;几次&#x27;</span>, <span class="hljs-string">&#x27;交锋&#x27;</span>, <span class="hljs-string">&#x27;具有&#x27;</span>, <span class="hljs-string">&#x27;真正&#x27;</span>, <span class="hljs-string">&#x27;意义&#x27;</span>, <span class="hljs-string">&#x27;今天&#x27;</span>, <span class="hljs-string">&#x27;对&#x27;</span>, <span class="hljs-string">&#x27;一役&#x27;</span>, <span class="hljs-string">&#x27;其中&#x27;</span>, <span class="hljs-string">&#x27;之一&#x27;</span>]<br>火箭<span class="hljs-symbol">_</span>燃气推进装置:<br>只有, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">7.470319934780034</span><br>具有, 频数为: <span class="hljs-number">5</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">32.35159967390017</span><br>真正, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">14.940639869560068</span><br>意义, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">8.055282435501189</span><br>对, 频数为: <span class="hljs-number">5</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">24.03677461028802</span><br>其中, 频数为: <span class="hljs-number">3</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">21.16584730650357</span><br>之一, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">14.11056487100238</span><br>火箭<span class="hljs-symbol">_NBA</span>球队名:<br>马刺, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">7.470319934780034</span><br>球队, 频数为: <span class="hljs-number">40</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">158.712783770034</span><br>常规赛, 频数为: <span class="hljs-number">14</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">73.4709851882102</span><br>只有, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">7.470319934780034</span><br>对, 频数为: <span class="hljs-number">10</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">48.07354922057604</span><br>之一, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">7.05528243550119</span><br><br>经过词义消岐，火箭在该句子中的意思为 <span class="hljs-symbol">NBA</span>球队名 .<br></code></pre></td></tr></table></figure><p>输入句子为：</p><blockquote><p>姚明是火箭队的主要得分手之一。</p></blockquote><p>输出结果为：</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">[&#x27;姚明&#x27;, &#x27;火箭队&#x27;, &#x27;主要&#x27;, &#x27;得分手&#x27;, &#x27;之一&#x27;]</span><br><span class="hljs-attribute">火箭_燃气推进装置</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">主要, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">9, TF-IDF值为: 51.60018906552445</span><br><span class="hljs-attribute">之一, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 14.11056487100238</span><br><span class="hljs-attribute">火箭_NBA球队名</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">姚明, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">18, TF-IDF值为: 90.99508383902142</span><br><span class="hljs-attribute">火箭队, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">133, TF-IDF值为: 284.1437533641371</span><br><span class="hljs-attribute">之一, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">1, TF-IDF值为: 7.05528243550119</span><br><br>经过词义消岐，火箭在该句子中的意思为 NBA球队名 .<br></code></pre></td></tr></table></figure><p>输入的句子为:</p><blockquote><p>从1992年开始研制的长征二号F型火箭，是中国航天史上技术最复杂、可靠性和安全性指标最高的运载火箭。</p></blockquote><p>输出结果为：</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">[&#x27;从&#x27;, &#x27;1992&#x27;, &#x27;年&#x27;, &#x27;开始&#x27;, &#x27;研制&#x27;, &#x27;长征二号&#x27;, &#x27;F&#x27;, &#x27;型&#x27;, &#x27;中国&#x27;, &#x27;航天史&#x27;, &#x27;技术&#x27;, &#x27;最&#x27;, &#x27;复杂&#x27;, &#x27;、&#x27;, &#x27;可靠性&#x27;, &#x27;和&#x27;, &#x27;安全性&#x27;, &#x27;指标&#x27;, &#x27;最高&#x27;, &#x27;运载火箭&#x27;]</span><br><span class="hljs-attribute">火箭_燃气推进装置</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">从, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">6, TF-IDF值为: 29.312144604353264</span><br><span class="hljs-attribute">1992, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">1, TF-IDF值为: 6.733354340613827</span><br><span class="hljs-attribute">年, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">43, TF-IDF值为: 107.52982410441274</span><br><span class="hljs-attribute">开始, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">5, TF-IDF值为: 30.27641217750595</span><br><span class="hljs-attribute">研制, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">25, TF-IDF值为: 110.28565614316162</span><br><span class="hljs-attribute">长征二号, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">37, TF-IDF值为: 159.11461253349566</span><br><span class="hljs-attribute">F, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">7, TF-IDF值为: 40.13348038429679</span><br><span class="hljs-attribute">中国, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">45, TF-IDF值为: 153.51418105769093</span><br><span class="hljs-attribute">技术, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">27, TF-IDF值为: 119.10850863461454</span><br><span class="hljs-attribute">最, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 7.614709844115208</span><br><span class="hljs-attribute">、, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">117, TF-IDF值为: 335.25857156467714</span><br><span class="hljs-attribute">可靠性, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">5, TF-IDF值为: 30.27641217750595</span><br><span class="hljs-attribute">和, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">76, TF-IDF值为: 191.22539545388003</span><br><span class="hljs-attribute">安全性, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 14.940639869560068</span><br><span class="hljs-attribute">运载火箭, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">95, TF-IDF值为: 256.28439093389505</span><br><span class="hljs-attribute">火箭_NBA球队名</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">从, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">5, TF-IDF值为: 24.42678717029439</span><br><span class="hljs-attribute">1992, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 13.466708681227654</span><br><span class="hljs-attribute">年, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">52, TF-IDF值为: 130.0360663588247</span><br><span class="hljs-attribute">开始, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 12.11056487100238</span><br><span class="hljs-attribute">中国, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">4, TF-IDF值为: 13.64570498290586</span><br><span class="hljs-attribute">最, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">3, TF-IDF值为: 11.422064766172813</span><br><span class="hljs-attribute">、, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">16, TF-IDF值为: 45.847326025938756</span><br><span class="hljs-attribute">和, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">31, TF-IDF值为: 77.99983235618791</span><br><span class="hljs-attribute">最高, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">8, TF-IDF值为: 59.76255947824027</span><br><br>经过词义消岐，火箭在该句子中的意思为 燃气推进装置 .<br></code></pre></td></tr></table></figure><p>输入句子为：</p><blockquote><p>到目前为止火箭已经在休斯顿进行了电视宣传，并在大街小巷竖起广告栏。</p></blockquote><p>输出结果为：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs prolog">[<span class="hljs-string">&#x27;到&#x27;</span>, <span class="hljs-string">&#x27;目前为止&#x27;</span>, <span class="hljs-string">&#x27;已经&#x27;</span>, <span class="hljs-string">&#x27;休斯顿&#x27;</span>, <span class="hljs-string">&#x27;进行&#x27;</span>, <span class="hljs-string">&#x27;电视&#x27;</span>, <span class="hljs-string">&#x27;宣传&#x27;</span>, <span class="hljs-string">&#x27;并&#x27;</span>, <span class="hljs-string">&#x27;大街小巷&#x27;</span>, <span class="hljs-string">&#x27;竖起&#x27;</span>, <span class="hljs-string">&#x27;广告栏&#x27;</span>]<br>火箭<span class="hljs-symbol">_</span>燃气推进装置:<br>到, 频数为: <span class="hljs-number">11</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">39.19772273088667</span><br>已经, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">13.466708681227654</span><br>进行, 频数为: <span class="hljs-number">14</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">68.39500407682429</span><br>并, 频数为: <span class="hljs-number">11</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">49.17351928258037</span><br>火箭<span class="hljs-symbol">_NBA</span>球队名:<br>到, 频数为: <span class="hljs-number">6</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">21.38057603502909</span><br>已经, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">13.466708681227654</span><br>休斯顿, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">14.940639869560068</span><br>进行, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">9.770714868117755</span><br>并, 频数为: <span class="hljs-number">5</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">22.351599673900168</span><br><br>经过词义消岐，火箭在该句子中的意思为 燃气推进装置 .<br></code></pre></td></tr></table></figure><h3 id="总结">总结</h3><p>对于笔者的这个算法，虽然有一定的效果，但是也不总是识别正确。比如，对于最后一个测试的句子，识别的结果就是错误的，其实“休斯顿”才是识别该词语义项的关键词，但很遗憾，在笔者的算法中，“休斯顿”的权重并不高。</p><p>对于词义消岐算法，如果还是笔者的这个思路，那么有以下几方面需要改进：</p><ul><li><p>语料大小及丰富程度；</p></li><li><p>停用词的扩充；</p></li><li><p>更好的算法。</p><p>笔者的这篇文章仅作为词义消岐的简介以及简单实现，希望能对读者有所启发～</p></li></ul><p><strong>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</strong></p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>词义消岐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（八）使用CRF++实现命名实体识别(NER)</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AB%EF%BC%89%E4%BD%BF%E7%94%A8CRF-%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB-NER/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AB%EF%BC%89%E4%BD%BF%E7%94%A8CRF-%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB-NER/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="crf与ner简介">CRF与NER简介</h3><p>CRF，英文全称为conditional random field,中文名为条件随机场，是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型，其特点是假设输出随机变量构成马尔可夫（Markov）随机场。</p><p>较为简单的条件随机场是定义在线性链上的条件随机场，称为线性链条件随机场（linearchain conditional random field）.线性链条件随机场可以用于序列标注等问题，而本文需要解决的命名实体识别(NER)任务正好可通过序列标注方法解决。这时，在条件概率模型P(Y|X)中，Y是输出变量，表示标记序列（或状态序列），X是输入变量，表示需要标注的观测序列。学习时，利用训练数据集通过极大似然估计或正则化的极大似然估计得到条件概率模型p(Y|X)；预测时，对于给定的输入序列x，求出条件概率p(y|x)最大的输出序列y0.</p><p><img src="/img/nlp8_1.jpeg" /></p><p>命名实体识别（Named EntityRecognition，简称NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。常见的实现NER的算法如下：</p><p><img src="/img/nlp8_2.jpeg" /></p><p>本文不准备详细介绍条件随机场的原理与实现算法，关于具体的原理与实现算法，可以参考《统计学习算法》一书。我们将借助已实现条件随机场的工具——CRF++来实现命名实体识别。关于用深度学习算法来实现命名实体识别，可以参考文章：<ahref="https://www.jianshu.com/p/ee750877ab6f">NLP入门（五）用深度学习实现命名实体识别（NER）</a>。</p><h3 id="crf">CRF++</h3><h4 id="简介">简介</h4><p>CRF++是著名的条件随机场的开源工具，也是目前综合性能最佳的CRF工具，采用C++语言编写而成。其最重要的功能我认为是采用了特征模板。这样就可以自动生成一系列的特征函数，而不用我们自己生成特征函数，我们要做的就是寻找特征，比如词性等。关于CRF++的特性，可以参考网址：<ahref="http://taku910.github.io/crfpp/">http://taku910.github.io/crfpp/</a>。</p><h4 id="安装">安装</h4><p>CRF++的安装可分为Windows环境和Linux环境下的安装。关于Linux环境下的安装，可以参考文章：<ahref="https://blog.51cto.com/wutengfei/2095715">CRFPP/CRF++编译安装与部署</a>。在Windows中CRF++不需要安装，下载解压CRF++0.58文件即可以使用，下载网址为：<ahref="https://blog.csdn.net/lilong117194/article/details/81160265">https://blog.csdn.net/lilong117194/article/details/81160265</a>。</p><h4 id="使用">使用</h4><h5 id="语料">1. 语料</h5><p>以我们本次使用的命名实体识别的语料为例，作为CRF++训练的语料（前20行，每一句话以空格隔开。）如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">played VBD O<br>on IN O<br>Monday NNP O<br>( ( O<br>home NN O<br>team NN O<br><span class="hljs-keyword">in</span> IN O<br>CAPS NNP O<br>) ) O<br>: : O<br><br>American NNP B-MISC<br>League NNP I-MISC<br><br>Cleveland NNP B-ORG<br>2 CD O<br>DETROIT NNP B-ORG<br>1 CD O<br><br>BALTIMORE VB B-ORG<br></code></pre></td></tr></table></figure><p>需要注意字与标签之间的分隔符为制表符否则会导致feature_index.cpp(86)[max_size == size] inconsistent column size错误。 ##### 2. 模板模板是使用CRF++的关键，它能帮助我们自动生成一系列的特征函数，而不用我们自己生成特征函数，而特征函数正是CRF算法的核心概念之一。一个简单的模板文件如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Unigram</span><br>U00:%x[-2,0]<br>U01:%x[0,1]<br>U02:%x[0,0]<br>U03:%x[1,0]<br>U04:%x[2,0]<br>U05:%x[-2,0]/%x[-1,0]/%x[0,0]<br>U06:%x[-1,0]/%x[0,0]/%x[1,0]<br>U07:%x[0,0]/%x[1,0]/%x[2,0]<br>U08:%x[-1,0]/%x[0,0]<br>U09:%x[0,0]/%x[1,0]<br> <br><span class="hljs-comment"># Bigram</span><br>B<br></code></pre></td></tr></table></figure><p>在这里，我们需要好好理解下模板文件的规则。T**:%x[#,#]中的T表示模板类型，两个"#"分别表示相对的行偏移与列偏移。一共有两种模板：</p><ul><li>第一种模板是Unigram template:第一个字符是U，用于描述unigramfeature的模板。每一行%x[#,#]生成一个CRF中的点(state)函数: f(s, o),其中s为t时刻的的标签(output)，o为t时刻的上下文。假设<code>home NN O</code>所在行为<code>CURRENT TOKEN</code>，</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">played VBD O<br>on IN O<br>Monday NNP O<br>( ( O<br>home NN O &lt;&lt; <span class="hljs-string">CURRENT TOKEN</span><br><span class="hljs-string">team NN O</span><br><span class="hljs-string">in IN O</span><br><span class="hljs-string">CAPS NNP O</span><br><span class="hljs-string">) ) O</span><br><span class="hljs-string">: : O</span><br></code></pre></td></tr></table></figure><p>那么%x[#,#]的对应规则如下：</p><table><thead><tr class="header"><th>template</th><th>expanded feature</th></tr></thead><tbody><tr class="odd"><td>%x[0,0]</td><td>home</td></tr><tr class="even"><td>%x[0,1]</td><td>NN</td></tr><tr class="odd"><td>%x[-1,0]</td><td>(</td></tr><tr class="even"><td>%x[-2,1]</td><td>NNP</td></tr><tr class="odd"><td>%x[0,0]/%x[0,1]</td><td>home/NN</td></tr><tr class="even"><td>ABC%x[0,1]123</td><td>ABCNN123</td></tr></tbody></table><p>以“U01:%x[0,1]”为例，它在该语料中生成的示例函数如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">func1 = <span class="hljs-keyword">if</span> (output = O and feature=<span class="hljs-string">&quot;U01:NN&quot;</span>) <span class="hljs-built_in">return</span> 1 <span class="hljs-keyword">else</span> <span class="hljs-built_in">return</span> 0<br>func2 = <span class="hljs-keyword">if</span> (output = O and feature=<span class="hljs-string">&quot;U01:N&quot;</span>) <span class="hljs-built_in">return</span> 1 <span class="hljs-keyword">else</span> <span class="hljs-built_in">return</span> 0<br>func3 = <span class="hljs-keyword">if</span> (output = O and feature=<span class="hljs-string">&quot;U01:NNP&quot;</span>) <span class="hljs-built_in">return</span> 1  <span class="hljs-keyword">else</span> <span class="hljs-built_in">return</span> 0<br>....<br></code></pre></td></tr></table></figure><ul><li>第二种模板是Bigramtemplate:第一个字符是B，每一行%x[#,#]生成一个CRFs中的边(Edge)函数:f(s',s, o),其中s'为t–1时刻的标签。也就是说,Bigram类型与Unigram大致相同,只是还要考虑到t–1时刻的标签。如果只写一个B的话,默认生成f(s',s)，这意味着前一个output token和current token将组合成bigramfeatures。</li></ul><h5 id="训练">3. 训练</h5><p>CRF++的训练命令一般格式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">crf_learn  -f 3 -c 4.0 template train.data model -t<br></code></pre></td></tr></table></figure><p>其中，template为模板文件，train.data为训练语料，-t表示可以得到一个model文件和一个model.txt文件，其他可选参数说明如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">-f, –freq=INT使用属性的出现次数不少于INT(默认为1)<br><br>-m, –maxiter=INT设置INT为LBFGS的最大迭代次数 (默认10k)<br><br>-c, –cost=FLOAT    设置FLOAT为代价参数，过大会过度拟合 (默认1.0)<br><br>-e, –eta=FLOAT设置终止标准FLOAT(默认0.0001)<br><br>-C, –convert将文本模式转为二进制模式<br><br>-t, –textmodel为调试建立文本模型文件<br><br>-a, –algorithm=(CRF|MIRA)    选择训练算法，默认为CRF-L2<br><br>-p, –thread=INT线程数(默认1)，利用多个CPU减少训练时间<br><br>-H, –shrinking-size=INT    设置INT为最适宜的跌代变量次数 (默认20)<br><br>-v, –version显示版本号并退出<br><br>-h, –<span class="hljs-built_in">help</span>显示帮助并退出<br></code></pre></td></tr></table></figure><p>在训练过程中，会输出一些信息，其意义如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">iter：迭代次数。当迭代次数达到maxiter时，迭代终止<br><br>terr：标记错误率<br><br>serr：句子错误率<br><br>obj：当前对象的值。当这个值收敛到一个确定值的时候，训练完成<br><br>diff：与上一个对象值之间的相对差。当此值低于eta时，训练完成<br></code></pre></td></tr></table></figure><h5 id="预测">4. 预测</h5><p>在训练完模型后，我们可以使用训练好的模型对新数据进行预测，预测命令格式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">crf_test -m model NER_predict.data &gt; predict.txt<br></code></pre></td></tr></table></figure><p><code>-m model</code>表示使用我们刚刚训练好的model模型，预测的数据文件为NER_predict.data,<code>&gt; predict.txt</code>表示将预测后的数据写入到predict.txt中。</p><h3 id="ner实现实例">NER实现实例</h3><p>接下来，我们将利用CRF++来实现英文命名实体识别功能。</p><p>本项目实现NER的语料库如下(文件名为train.txt，一共42000行，这里只展示前15行，可以在文章最后的Github地址下载该语料库)：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">played on Monday ( home team <span class="hljs-keyword">in</span> CAPS ) :<br>VBD IN NNP ( NN NN IN NNP ) :<br>O O O O O O O O O O<br>American League<br>NNP NNP<br>B-MISC I-MISC<br>Cleveland 2 DETROIT 1<br>NNP CD NNP CD<br>B-ORG O B-ORG O<br>BALTIMORE 12 Oakland 11 ( 10 innings )<br>VB CD NNP CD ( CD NN )<br>B-ORG O B-ORG O O O O O<br>TORONTO 5 Minnesota 3<br>TO CD NNP CD<br>B-ORG O B-ORG O<br>......<br></code></pre></td></tr></table></figure><p>简单介绍下该语料库的结构：该语料库一共42000行，每三行为一组，其中，第一行为英语句子，第二行为句子中每个单词的词性，第三行为NER系统的标注，共分4个标注类别：PER（人名），LOC（位置），ORG（组织）以及MISC，其中B表示开始，I表示中间，O表示单字词，不计入NER，sO表示特殊单字词。</p><p>首先我们将该语料分为训练集和测试集，比例为9:1，实现的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-comment"># NER预料train.txt所在的路径</span><br><span class="hljs-built_in">dir</span> = <span class="hljs-string">&quot;/Users/Shared/CRF_4_NER/CRF_TEST&quot;</span><br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/train.txt&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    sents = [line.strip() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines()]<br><br><span class="hljs-comment"># 训练集与测试集的比例为9:1</span><br>RATIO = <span class="hljs-number">0.9</span><br>train_num = <span class="hljs-built_in">int</span>((<span class="hljs-built_in">len</span>(sents)//<span class="hljs-number">3</span>)*RATIO)<br><br><span class="hljs-comment"># 将文件分为训练集与测试集</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/NER_train.data&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&quot;w&quot;</span>) <span class="hljs-keyword">as</span> g:<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(train_num):<br>        words = sents[<span class="hljs-number">3</span>*i].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        postags = sents[<span class="hljs-number">3</span>*i+<span class="hljs-number">1</span>].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        tags = sents[<span class="hljs-number">3</span>*i+<span class="hljs-number">2</span>].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        <span class="hljs-keyword">for</span> word, postag, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(words, postags, tags):<br>            g.write(word+<span class="hljs-string">&#x27; &#x27;</span>+postag+<span class="hljs-string">&#x27; &#x27;</span>+tag+<span class="hljs-string">&#x27;\n&#x27;</span>)<br>        g.write(<span class="hljs-string">&#x27;\n&#x27;</span>)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/NER_test.data&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&quot;w&quot;</span>) <span class="hljs-keyword">as</span> h:<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(train_num+<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(sents)//<span class="hljs-number">3</span>):<br>        words = sents[<span class="hljs-number">3</span>*i].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        postags = sents[<span class="hljs-number">3</span>*i+<span class="hljs-number">1</span>].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        tags = sents[<span class="hljs-number">3</span>*i+<span class="hljs-number">2</span>].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        <span class="hljs-keyword">for</span> word, postag, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(words, postags, tags):<br>            h.write(word+<span class="hljs-string">&#x27; &#x27;</span>+postag+<span class="hljs-string">&#x27; &#x27;</span>+tag+<span class="hljs-string">&#x27;\n&#x27;</span>)<br>        h.write(<span class="hljs-string">&#x27;\n&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;OK!&#x27;</span>)<br></code></pre></td></tr></table></figure><p>运行此程序，得到NER_train.data,此为训练集数据，NER_test.data，此为测试集数据。NER_train.data的前20行数据如下（以）：</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs mathematica"><span class="hljs-variable">played</span> <span class="hljs-variable">VBD</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">on</span> <span class="hljs-variable">IN</span> <span class="hljs-built_in">O</span><br><span class="hljs-built_in">Monday</span> <span class="hljs-variable">NNP</span> <span class="hljs-built_in">O</span><br><span class="hljs-punctuation">(</span> <span class="hljs-punctuation">(</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">home</span> <span class="hljs-variable">NN</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">team</span> <span class="hljs-variable">NN</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">in</span> <span class="hljs-variable">IN</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">CAPS</span> <span class="hljs-variable">NNP</span> <span class="hljs-built_in">O</span><br><span class="hljs-punctuation">)</span> <span class="hljs-punctuation">)</span> <span class="hljs-built_in">O</span><br><span class="hljs-operator">:</span> <span class="hljs-operator">:</span> <span class="hljs-built_in">O</span><br><br><span class="hljs-variable">American</span> <span class="hljs-variable">NNP</span> <span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">MISC</span><br><span class="hljs-variable">League</span> <span class="hljs-variable">NNP</span> <span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">MISC</span><br><br><span class="hljs-variable">Cleveland</span> <span class="hljs-variable">NNP</span> <span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">ORG</span><br><span class="hljs-number">2</span> <span class="hljs-variable">CD</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">DETROIT</span> <span class="hljs-variable">NNP</span> <span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">ORG</span><br><span class="hljs-number">1</span> <span class="hljs-variable">CD</span> <span class="hljs-built_in">O</span><br><br><span class="hljs-variable">BALTIMORE</span> <span class="hljs-variable">VB</span> <span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">ORG</span><br></code></pre></td></tr></table></figure><p>我们使用的模板文件template内容如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># Unigram</span><br><span class="hljs-attribute">U00</span>:%x[-<span class="hljs-number">2</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U01</span>:%x[-<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U02</span>:%x[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U03</span>:%x[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U04</span>:%x[<span class="hljs-number">2</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U05</span>:%x[-<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]/%x[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U06</span>:%x[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]/%x[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]<br><br><span class="hljs-attribute">U10</span>:%x[-<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U11</span>:%x[-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U12</span>:%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U13</span>:%x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U14</span>:%x[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U15</span>:%x[-<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]/%x[-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U16</span>:%x[-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U17</span>:%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U18</span>:%x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]<br><br><span class="hljs-attribute">U20</span>:%x[-<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]/%x[-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U21</span>:%x[-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U22</span>:%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># Bigram</span><br><span class="hljs-attribute">B</span><br></code></pre></td></tr></table></figure><p>接着训练该数据，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">crf_learn -c 3.0 template NER_train.data model -t<br></code></pre></td></tr></table></figure><p>运行时的输出信息如下：</p><p><img src="/img/nlp8_3.jpeg" /></p><p>在笔者的电脑上一共迭代了193次，运行时间为490.32秒，标记错误率为0.00004，句子错误率为0.00056。</p><p>接着，我们需要在测试集上对该模型的预测表现做评估。预测命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">crf_test -m model NER_test.data &gt; result.txt<br></code></pre></td></tr></table></figure><p>使用Python脚本统计预测的准确率，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-built_in">dir</span> = <span class="hljs-string">&quot;/Users/Shared/CRF_4_NER/CRF_TEST&quot;</span><br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/result.txt&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    sents = [line.strip() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines() <span class="hljs-keyword">if</span> line.strip()]<br><br>total = <span class="hljs-built_in">len</span>(sents)<br><span class="hljs-built_in">print</span>(total)<br><br>count = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>    words = sent.split()<br>    <span class="hljs-comment"># print(words)</span><br>    <span class="hljs-keyword">if</span> words[-<span class="hljs-number">1</span>] == words[-<span class="hljs-number">2</span>]:<br>        count += <span class="hljs-number">1</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy: %.4f&quot;</span> %(count/total))<br><span class="hljs-comment"># 0.9706</span><br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">21487</span><br><span class="hljs-attribute">Accuracy</span>: <span class="hljs-number">0</span>.<span class="hljs-number">9706</span><br></code></pre></td></tr></table></figure><p>由此可见，在测试集上的准确率高达0.9706，效果相当好。</p><p>最后，我们对新数据进行命名实体识别，看看模型在新数据上的识别效果。实现的Python代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> nltk<br><br><span class="hljs-built_in">dir</span> = <span class="hljs-string">&quot;/Users/Shared/CRF_4_NER/CRF_TEST&quot;</span><br><br>sentence = <span class="hljs-string">&quot;Venezuelan opposition leader and self-proclaimed interim president Juan Guaidó said Thursday he will return to his country by Monday, and that a dialogue with President Nicolas Maduro won&#x27;t be possible without discussing elections.&quot;</span><br><span class="hljs-comment">#sentence = &quot;Real Madrid&#x27;s season on the brink after 3-0 Barcelona defeat&quot;</span><br><span class="hljs-comment"># sentence = &quot;British artist David Hockney is known as a voracious smoker, but the habit got him into a scrape in Amsterdam on Wednesday.&quot;</span><br><span class="hljs-comment"># sentence = &quot;India is waiting for the release of an pilot who has been in Pakistani custody since he was shot down over Kashmir on Wednesday, a goodwill gesture which could defuse the gravest crisis in the disputed border region in years.&quot;</span><br><span class="hljs-comment"># sentence = &quot;Instead, President Donald Trump&#x27;s second meeting with North Korean despot Kim Jong Un ended in a most uncharacteristic fashion for a showman commander in chief: fizzle.&quot;</span><br><span class="hljs-comment"># sentence = &quot;And in a press conference at the Civic Leadership Academy in Queens, de Blasio said the program is already working.&quot;</span><br><span class="hljs-comment">#sentence = &quot;The United States is a founding member of the United Nations, World Bank, International Monetary Fund.&quot;</span><br><br>default_wt = nltk.word_tokenize <span class="hljs-comment"># 分词</span><br>words = default_wt(sentence)<br><span class="hljs-built_in">print</span>(words)<br>postags = nltk.pos_tag(words)<br><span class="hljs-built_in">print</span>(postags)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/NER_predict.data&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> postags:<br>        f.write(item[<span class="hljs-number">0</span>]+<span class="hljs-string">&#x27; &#x27;</span>+item[<span class="hljs-number">1</span>]+<span class="hljs-string">&#x27; O\n&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;write successfully!&quot;</span>)<br><br>os.chdir(<span class="hljs-built_in">dir</span>)<br>os.system(<span class="hljs-string">&quot;crf_test -m model NER_predict.data &gt; predict.txt&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;get predict file!&quot;</span>)<br><br><span class="hljs-comment"># 读取预测文件redict.txt</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/predict.txt&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    sents = [line.strip() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines() <span class="hljs-keyword">if</span> line.strip()]<br><br>word = []<br>predict = []<br><br><span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>    words = sent.split()<br>    word.append(words[<span class="hljs-number">0</span>])<br>    predict.append(words[-<span class="hljs-number">1</span>])<br><br><span class="hljs-comment"># print(word)</span><br><span class="hljs-comment"># print(predict)</span><br><br><span class="hljs-comment"># 去掉NER标注为O的元素</span><br>ner_reg_list = []<br><span class="hljs-keyword">for</span> word, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(word, predict):<br>    <span class="hljs-keyword">if</span> tag != <span class="hljs-string">&#x27;O&#x27;</span>:<br>        ner_reg_list.append((word, tag))<br><br><span class="hljs-comment"># 输出模型的NER识别结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;NER识别结果：&quot;</span>)<br><span class="hljs-keyword">if</span> ner_reg_list:<br>    <span class="hljs-keyword">for</span> i, item <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(ner_reg_list):<br>        <span class="hljs-keyword">if</span> item[<span class="hljs-number">1</span>].startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>            end = i+<span class="hljs-number">1</span><br>            <span class="hljs-keyword">while</span> end &lt;= <span class="hljs-built_in">len</span>(ner_reg_list)-<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> ner_reg_list[end][<span class="hljs-number">1</span>].startswith(<span class="hljs-string">&#x27;I&#x27;</span>):<br>                end += <span class="hljs-number">1</span><br><br>            ner_type = item[<span class="hljs-number">1</span>].split(<span class="hljs-string">&#x27;-&#x27;</span>)[<span class="hljs-number">1</span>]<br>            ner_type_dict = &#123;<span class="hljs-string">&#x27;PER&#x27;</span>: <span class="hljs-string">&#x27;PERSON: &#x27;</span>,<br>                             <span class="hljs-string">&#x27;LOC&#x27;</span>: <span class="hljs-string">&#x27;LOCATION: &#x27;</span>,<br>                             <span class="hljs-string">&#x27;ORG&#x27;</span>: <span class="hljs-string">&#x27;ORGANIZATION: &#x27;</span>,<br>                             <span class="hljs-string">&#x27;MISC&#x27;</span>: <span class="hljs-string">&#x27;MISC: &#x27;</span><br>                            &#125;<br>            <span class="hljs-built_in">print</span>(ner_type_dict[ner_type], <span class="hljs-string">&#x27; &#x27;</span>.join([item[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> ner_reg_list[i:end]]))<br></code></pre></td></tr></table></figure><p>识别的结果如下：</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">MISC:</span>  Venezuelan<br><span class="hljs-symbol">PERSON:</span>  Juan Guaidó<br><span class="hljs-symbol">PERSON:</span>  Nicolas Maduro<br></code></pre></td></tr></table></figure><p>识别有个地方不准确， Venezuelan应该是LOC，而不是MISC.我们再接着测试其它的新数据：</p><p>输入语句1：</p><blockquote><p>Real Madrid's season on the brink after 3-0 Barcelona defeat</p></blockquote><p>识别效果1：</p><blockquote><p>ORGANIZATION: Real Madrid LOCATION: Barcelona</p></blockquote><p>输入语句2：</p><blockquote><p>British artist David Hockney is known as a voracious smoker, but thehabit got him into a scrape in Amsterdam on Wednesday.</p></blockquote><p>识别效果2：</p><blockquote><p>MISC: British PERSON: David Hockney LOCATION: Amsterdam</p></blockquote><p>输入语句3：</p><blockquote><p>India is waiting for the release of an pilot who has been inPakistani custody since he was shot down over Kashmir on Wednesday, agoodwill gesture which could defuse the gravest crisis in the disputedborder region in years.</p></blockquote><p>识别效果3：</p><blockquote><p>LOCATION: India LOCATION: Pakistani LOCATION: Kashmir</p></blockquote><p>输入语句4：</p><blockquote><p>Instead, President Donald Trump's second meeting with North Koreandespot Kim Jong Un ended in a most uncharacteristic fashion for ashowman commander in chief: fizzle.</p></blockquote><p>识别效果4：</p><blockquote><p>PERSON: Donald Trump PERSON: Kim Jong Un</p></blockquote><p>输入语句5：</p><blockquote><p>And in a press conference at the Civic Leadership Academy in Queens,de Blasio said the program is already working.</p></blockquote><p>识别效果5：</p><blockquote><p>ORGANIZATION: Civic Leadership Academy LOCATION: Queens PERSON: deBlasio</p></blockquote><p>输入语句6：</p><blockquote><p>The United States is a founding member of the United Nations, WorldBank, International Monetary Fund.</p></blockquote><p>识别效果6：</p><blockquote><p>LOCATION: United States ORGANIZATION: United Nations PERSON: WorldBank ORGANIZATION: International Monetary Fund</p></blockquote><p>在这些例子中，有让我们惊喜之处：识别出了人物Donald Trump, Kim JongUn. 但也有些不足指出，如将WorldBank识别为人物，而不是组织机构。总的来说，识别效果还是让人满意的。</p><h3 id="总结">总结</h3><p>最近由于工作繁忙，无暇顾及博客。但转念一想，技术输出也是比较重要的，需要长期坚持下去～</p><p>本项目的Github地址为：<ahref="https://github.com/percent4/CRF_4_NER">https://github.com/percent4/CRF_4_NER</a>。</p><p>五一将至，祝大家假期愉快～</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>NER</tag>
      
      <tag>CRF++</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（七）中文预处理之繁简体转换及获取拼音</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%83%EF%BC%89%E4%B8%AD%E6%96%87%E9%A2%84%E5%A4%84%E7%90%86%E4%B9%8B%E7%B9%81%E7%AE%80%E4%BD%93%E8%BD%AC%E6%8D%A2%E5%8F%8A%E8%8E%B7%E5%8F%96%E6%8B%BC%E9%9F%B3/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%83%EF%BC%89%E4%B8%AD%E6%96%87%E9%A2%84%E5%A4%84%E7%90%86%E4%B9%8B%E7%B9%81%E7%AE%80%E4%BD%93%E8%BD%AC%E6%8D%A2%E5%8F%8A%E8%8E%B7%E5%8F%96%E6%8B%BC%E9%9F%B3/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在日常的中文NLP中，经常会涉及到中文的繁简体转换以及拼音的标注等问题，本文将介绍这两个方面的实现。</p><p>首先是中文的繁简体转换，不需要使用额外的Python模块，至需要以下两个Python代码文件即可：</p><ul><li><p>langconv.py 地址： <ahref="https://raw.githubusercontent.com/skydark/nstools/master/zhtools/langconv.py">https://raw.githubusercontent.com/skydark/nstools/master/zhtools/langconv.py</a></p></li><li><p>zh_wiki.py 地址：<ahref="https://raw.githubusercontent.com/skydark/nstools/master/zhtools/zh_wiki.py">https://raw.githubusercontent.com/skydark/nstools/master/zhtools/zh_wiki.py</a></p><p>示例代码如下（将代码文件与langconv.py与zh_wiki.py放在同一目录下）：</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langconv <span class="hljs-keyword">import</span> *<br><br><span class="hljs-comment"># 转换繁体到简体</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cht_2_chs</span>(<span class="hljs-params">line</span>):<br>    line = Converter(<span class="hljs-string">&#x27;zh-hans&#x27;</span>).convert(line)<br>    line.encode(<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>    <span class="hljs-keyword">return</span> line<br><br>line_cht= <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">台北市長柯文哲今在臉書開直播，先向網友報告自己3月16日至24日要出訪美國東部4城市，接著他無預警宣布，</span><br><span class="hljs-string">2月23日要先出訪以色列，預計停留4至5天。雖他強調台北市、以色列已在資安方面有所交流，也可到當地城市交流、</span><br><span class="hljs-string">參觀產業創新等內容，但柯也說「也是去看看一個小國在這麼惡劣環境，howtosurvive，他的祕訣是什麼？」這番話，</span><br><span class="hljs-string">也被解讀，頗有更上層樓、直指總統大位的思維。</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br>line_cht = line_cht.replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>ret_chs = cht_2_chs(line_cht)<br><span class="hljs-built_in">print</span>(ret_chs)<br><br><span class="hljs-comment"># 转换简体到繁体</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">chs_2_cht</span>(<span class="hljs-params">sentence</span>):<br>    sentence = Converter(<span class="hljs-string">&#x27;zh-hant&#x27;</span>).convert(sentence)<br>    <span class="hljs-keyword">return</span> sentence<br><br>line_chs = <span class="hljs-string">&#x27;忧郁的台湾乌龟&#x27;</span><br>line_cht = chs_2_cht(line_chs)<br><span class="hljs-built_in">print</span>(line_cht)<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><blockquote><p>台北市长柯文哲今在脸书开直播，先向网友报告自己3月16日至24日要出访美国东部4城市，接着他无预警宣布，2月23日要先出访以色列，预计停留4至5天。虽他强调台北市、以色列已在资安方面有所交流，也可到当地城市交流、参观产业创新等内容，但柯也说「也是去看看一个小国在这么恶劣环境，howtosurvive，他的祕诀是什么？」这番话，也被解读，颇有更上层楼、直指总统大位的思维。憂郁的臺灣烏龜</p></blockquote><p>接着是获取中文汉字的拼音，这方面的Python模块有xpinyin,pypinyin等。本文以xpinyin为例，展示如何获取汉字的拼音。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> xpinyin <span class="hljs-keyword">import</span> Pinyin<br><br>p = Pinyin()<br><br><span class="hljs-comment"># 默认分隔符为-</span><br><span class="hljs-built_in">print</span>(p.get_pinyin(<span class="hljs-string">&quot;上海&quot;</span>))<br><br><span class="hljs-comment"># 显示声调</span><br><span class="hljs-built_in">print</span>(p.get_pinyin(<span class="hljs-string">&quot;上海&quot;</span>, tone_marks=<span class="hljs-string">&#x27;marks&#x27;</span>))<br><span class="hljs-built_in">print</span>(p.get_pinyin(<span class="hljs-string">&quot;上海&quot;</span>, tone_marks=<span class="hljs-string">&#x27;numbers&#x27;</span>))<br><br><span class="hljs-comment"># 去掉分隔符</span><br><span class="hljs-built_in">print</span>(p.get_pinyin(<span class="hljs-string">&quot;上海&quot;</span>, <span class="hljs-string">&#x27;&#x27;</span>))<br><span class="hljs-comment"># 设为分隔符为空格</span><br><span class="hljs-built_in">print</span>(p.get_pinyin(<span class="hljs-string">&quot;上海&quot;</span>, <span class="hljs-string">&#x27; &#x27;</span>))<br><br><span class="hljs-comment"># 获取拼音首字母</span><br><span class="hljs-built_in">print</span>(p.get_initial(<span class="hljs-string">&quot;上&quot;</span>))<br><span class="hljs-built_in">print</span>(p.get_initials(<span class="hljs-string">&quot;上海&quot;</span>))<br><span class="hljs-built_in">print</span>(p.get_initials(<span class="hljs-string">&quot;上海&quot;</span>, <span class="hljs-string">&#x27;&#x27;</span>))<br><span class="hljs-built_in">print</span>(p.get_initials(<span class="hljs-string">&quot;上海&quot;</span>, <span class="hljs-string">&#x27; &#x27;</span>))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">shang-hai</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">shàng-hǎi</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">shang4-hai3</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">shanghai</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">shang </span>hai<br>S<br>S-H<br><span class="hljs-keyword">SH</span><br><span class="hljs-keyword"></span>S H<br></code></pre></td></tr></table></figure><p>本次分享到此结束，感谢大家阅读~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>繁简体转换</tag>
      
      <tag>拼音</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（六）pyltp的介绍与使用</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AD%EF%BC%89pyltp%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AD%EF%BC%89pyltp%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="pyltp的简介">pyltp的简介</h3><p>语言技术平台(LTP)经过哈工大社会计算与信息检索研究中心 11年的持续研发和推广，是国内外最具影响力的中文处理基础平台。它提供的功能包括中文分词、词性标注、命名实体识别、依存句法分析、语义角色标注等。</p><figure><img src="/img/nlp6_1.png" alt="语言技术平台架构" /><figcaption aria-hidden="true">语言技术平台架构</figcaption></figure><p>pyltp 是 LTP 的 Python封装，同时支持Python2和Python3版本。Python3的安装方法为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip3 install pyltp<br></code></pre></td></tr></table></figure><ul><li><p>官网下载网址：https://pypi.org/project/pyltp/0.1.7/</p></li><li><p>官方使用说明文档：https://pyltp.readthedocs.io/zh_CN/develop/api.html</p><p>在使用该模块前，需要下载完整的模型文件，文件下载地址为：<ahref="https://pan.baidu.com/share/link?shareid=1988562907&amp;uk=2738088569#list/path=%2F">https://pan.baidu.com/share/link?shareid=1988562907&amp;uk=2738088569#list/path=%2F</a>。pyltp 的所有输入的分析文本和输出的结果的编码均为UTF-8。模型的数据文件如下：</p></li></ul><figure><img src="/img/nlp6_2.png" alt="模型数据" /><figcaption aria-hidden="true">模型数据</figcaption></figure><p>其中，cws.model用于分词模型，lexicon.txt为分词时添加的用户字典，ner.model为命名实体识别模型，parser.model为依存句法分析模型，pisrl.model为语义角色标注模型，pos为词性标注模型。</p><h3 id="pyltp的使用">pyltp的使用</h3><p>pyltp的使用示例项目结构如下：</p><figure><img src="/img/nlp6_3.png" alt="示例项目" /><figcaption aria-hidden="true">示例项目</figcaption></figure><h5 id="分句">分句</h5><p>分句指的是将一段话或一片文章中的文字按句子分开，按句子形成独立的单元。示例的Python代码sentenct_split.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> SentenceSplitter<br><br><span class="hljs-comment"># 分句</span><br>doc = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span> \<br>      <span class="hljs-string">&#x27;盖茨原计划从明年1月9日至14日陆续访问中国和日本，目前，他决定在行程中增加对韩国的访问。莫莱尔表示，&#x27;</span> \<br>      <span class="hljs-string">&#x27;盖茨在访韩期间将会晤韩国国防部长官金宽镇，就朝鲜近日的行动交换意见，同时商讨加强韩美两军同盟关系等问题，&#x27;</span> \<br>      <span class="hljs-string">&#x27;拟定共同应对朝鲜挑衅和核计划的方案。&#x27;</span><br>sents = SentenceSplitter.split(doc)  <span class="hljs-comment"># 分句</span><br><br><br><span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>    <span class="hljs-built_in">print</span>(sent)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs dns">据韩联社<span class="hljs-number">12</span>月<span class="hljs-number">28</span>日反映，美国防部发言人杰夫·莫莱尔<span class="hljs-number">27</span>日表示，美国防部长盖茨将于<span class="hljs-number">2011年1月14</span>日访问韩国。<br>盖茨原计划从明年<span class="hljs-number">1</span>月<span class="hljs-number">9</span>日至<span class="hljs-number">14</span>日陆续访问中国和日本，目前，他决定在行程中增加对韩国的访问。<br>莫莱尔表示，盖茨在访韩期间将会晤韩国国防部长官金宽镇，就朝鲜近日的行动交换意见，同时商讨加强韩美两军同盟关系等问题，拟定共同应对朝鲜挑衅和核计划的方案。<br></code></pre></td></tr></table></figure><h5 id="分词">分词</h5><p>分词指的是将一句话按词语分开，按词语形成独立的单元。示例的Python代码words_split.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor<br><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;/&#x27;</span>.join(words))<br><br>segmentor.release()<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">据<span class="hljs-regexp">/韩联社/</span><span class="hljs-number">12</span>月<span class="hljs-regexp">/28日/</span>反映<span class="hljs-regexp">/，/</span>美<span class="hljs-regexp">/国防部/</span>发言人<span class="hljs-regexp">/杰夫·莫莱尔/</span><span class="hljs-number">27</span>日<span class="hljs-regexp">/表示/</span>，<span class="hljs-regexp">/美/</span>国防部长<span class="hljs-regexp">/盖茨/</span>将<span class="hljs-regexp">/于/</span><span class="hljs-number">2011</span>年<span class="hljs-regexp">/1月/</span><span class="hljs-number">14</span>日<span class="hljs-regexp">/访问/</span>韩国/。<br></code></pre></td></tr></table></figure><h5 id="词性标注">词性标注</h5><p>词性标注指的是一句话分完词后，制定每个词语的词性。示例的Python代码postagger.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor, Postagger<br><br><span class="hljs-comment"># 分词</span><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br><br>postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br>postags = postagger.postag(words)  <span class="hljs-comment"># 词性标注</span><br><br><span class="hljs-keyword">for</span> word, postag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(words, postags):<br>    <span class="hljs-built_in">print</span>(word, postag)<br><br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">词性标注结果说明</span><br><span class="hljs-string">https://ltp.readthedocs.io/zh_CN/latest/appendix.html#id3</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs excel">据 p<br>韩联社 ni<br><span class="hljs-number">12</span>月 nt<br><span class="hljs-number">28</span>日 nt<br>反映 v<br>， wp<br>美 j<br>国防部 <span class="hljs-built_in">n</span><br>发言人 <span class="hljs-built_in">n</span><br>杰夫·莫莱尔 nh<br><span class="hljs-number">27</span>日 nt<br>表示 v<br>， wp<br>美 j<br>国防部长 <span class="hljs-built_in">n</span><br>盖茨 nh<br>将 d<br>于 p<br><span class="hljs-number">2011</span>年 nt<br><span class="hljs-number">1</span>月 nt<br><span class="hljs-number">14</span>日 nt<br>访问 v<br>韩国 ns<br>。 wp<br></code></pre></td></tr></table></figure><p>词性标注结果可参考网址：<ahref="https://ltp.readthedocs.io/zh_CN/latest/appendix.html">https://ltp.readthedocs.io/zh_CN/latest/appendix.html</a>。</p><h5 id="命名实体识别">命名实体识别</h5><p>命名实体识别（NER）指的是识别出一句话或一段话或一片文章中的命名实体，比如人名，地名，组织机构名。示例的Python代码ner.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor, Postagger<br><br><span class="hljs-comment"># 分词</span><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br><br>postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br>postags = postagger.postag(words)  <span class="hljs-comment"># 词性标注</span><br><br><br>ner_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/ner.model&#x27;</span>)   <span class="hljs-comment"># 命名实体识别模型路径，模型名称为`pos.model`</span><br><br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> NamedEntityRecognizer<br>recognizer = NamedEntityRecognizer() <span class="hljs-comment"># 初始化实例</span><br>recognizer.load(ner_model_path)  <span class="hljs-comment"># 加载模型</span><br><span class="hljs-comment"># netags = recognizer.recognize(words, postags)  # 命名实体识别</span><br><br><br><span class="hljs-comment"># 提取识别结果中的人名，地名，组织机构名</span><br><br>persons, places, orgs = <span class="hljs-built_in">set</span>(), <span class="hljs-built_in">set</span>(), <span class="hljs-built_in">set</span>()<br><br><br>netags = <span class="hljs-built_in">list</span>(recognizer.recognize(words, postags))  <span class="hljs-comment"># 命名实体识别</span><br><span class="hljs-built_in">print</span>(netags)<br><span class="hljs-comment"># print(netags)</span><br>i = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> tag, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(netags, words):<br>    j = i<br>    <span class="hljs-comment"># 人名</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;Nh&#x27;</span> <span class="hljs-keyword">in</span> tag:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;S&#x27;</span>):<br>            persons.add(word)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>            union_person = word<br>            <span class="hljs-keyword">while</span> netags[j] != <span class="hljs-string">&#x27;E-Nh&#x27;</span>:<br>                j += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> j &lt; <span class="hljs-built_in">len</span>(words):<br>                    union_person += words[j]<br>            persons.add(union_person)<br>    <span class="hljs-comment"># 地名</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;Ns&#x27;</span> <span class="hljs-keyword">in</span> tag:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;S&#x27;</span>):<br>            places.add(word)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>            union_place = word<br>            <span class="hljs-keyword">while</span> netags[j] != <span class="hljs-string">&#x27;E-Ns&#x27;</span>:<br>                j += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> j &lt; <span class="hljs-built_in">len</span>(words):<br>                    union_place += words[j]<br>            places.add(union_place)<br>    <span class="hljs-comment"># 机构名</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;Ni&#x27;</span> <span class="hljs-keyword">in</span> tag:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;S&#x27;</span>):<br>            orgs.add(word)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>            union_org = word<br>            <span class="hljs-keyword">while</span> netags[j] != <span class="hljs-string">&#x27;E-Ni&#x27;</span>:<br>                j += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> j &lt; <span class="hljs-built_in">len</span>(words):<br>                    union_org += words[j]<br>            orgs.add(union_org)<br><br>    i += <span class="hljs-number">1</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;人名：&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>.join(persons))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;地名：&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>.join(places))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;组织机构：&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>.join(orgs))<br><br><br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br>recognizer.release()<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Ni&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-Ni&#x27;</span>, <span class="hljs-string">&#x27;E-Ni&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Nh&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Ns&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Nh&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Ns&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>人名： 杰夫·莫莱尔，盖茨<br>地名： 美，韩国<br>组织机构： 韩联社，美国防部<br></code></pre></td></tr></table></figure><p>命名实体识别结果可参考网址：<ahref="https://ltp.readthedocs.io/zh_CN/latest/appendix.html">https://ltp.readthedocs.io/zh_CN/latest/appendix.html</a>。</p><h4 id="依存句法分析">依存句法分析</h4><p>依存语法 (Dependency Parsing, DP)通过分析语言单位内成分之间的依存关系揭示其句法结构。直观来讲，依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关系。示例的Python代码parser.py代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor, Postagger, Parser<br><br><span class="hljs-comment"># 分词</span><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br><br>postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br>postags = postagger.postag(words)  <span class="hljs-comment"># 词性标注</span><br><br><br><span class="hljs-comment"># 依存句法分析</span><br>par_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/parser.model&#x27;</span>)  <span class="hljs-comment"># 模型路径，模型名称为`parser.model`</span><br><br>parser = Parser() <span class="hljs-comment"># 初始化实例</span><br>parser.load(par_model_path)  <span class="hljs-comment"># 加载模型</span><br>arcs = parser.parse(words, postags)  <span class="hljs-comment"># 句法分析</span><br><br>rely_id = [arc.head <span class="hljs-keyword">for</span> arc <span class="hljs-keyword">in</span> arcs]  <span class="hljs-comment"># 提取依存父节点id</span><br>relation = [arc.relation <span class="hljs-keyword">for</span> arc <span class="hljs-keyword">in</span> arcs]  <span class="hljs-comment"># 提取依存关系</span><br>heads = [<span class="hljs-string">&#x27;Root&#x27;</span> <span class="hljs-keyword">if</span> <span class="hljs-built_in">id</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> words[<span class="hljs-built_in">id</span>-<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> rely_id]  <span class="hljs-comment"># 匹配依存父节点词语</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(words)):<br>    <span class="hljs-built_in">print</span>(relation[i] + <span class="hljs-string">&#x27;(&#x27;</span> + words[i] + <span class="hljs-string">&#x27;, &#x27;</span> + heads[i] + <span class="hljs-string">&#x27;)&#x27;</span>)<br><br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br>parser.release()<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(据, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">SBV</span><span class="hljs-params">(韩联社, 反映)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">12</span>月, <span class="hljs-number">28</span>日)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(<span class="hljs-number">28</span>日, 反映)</span></span><br><span class="hljs-function"><span class="hljs-title">POB</span><span class="hljs-params">(反映, 据)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(，, 据)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(美, 国防部)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(国防部, 发言人)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(发言人, 杰夫·莫莱尔)</span></span><br><span class="hljs-function"><span class="hljs-title">SBV</span><span class="hljs-params">(杰夫·莫莱尔, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(<span class="hljs-number">27</span>日, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">HED</span><span class="hljs-params">(表示, Root)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(，, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(美, 国防部长)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(国防部长, 盖茨)</span></span><br><span class="hljs-function"><span class="hljs-title">SBV</span><span class="hljs-params">(盖茨, 访问)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(将, 访问)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(于, 访问)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">2011</span>年, <span class="hljs-number">14</span>日)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">1</span>月, <span class="hljs-number">14</span>日)</span></span><br><span class="hljs-function"><span class="hljs-title">POB</span><span class="hljs-params">(<span class="hljs-number">14</span>日, 于)</span></span><br><span class="hljs-function"><span class="hljs-title">VOB</span><span class="hljs-params">(访问, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">VOB</span><span class="hljs-params">(韩国, 访问)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(。, 表示)</span></span><br></code></pre></td></tr></table></figure><p>依存句法分析结果可参考网址：<ahref="https://ltp.readthedocs.io/zh_CN/latest/appendix.html">https://ltp.readthedocs.io/zh_CN/latest/appendix.html</a>。</p><h5 id="语义角色标注">语义角色标注</h5><p>语义角色标注是实现浅层语义分析的一种方式。在一个句子中，谓词是对主语的陈述或说明，指出“做什么”、“是什么”或“怎么样，代表了一个事件的核心，跟谓词搭配的名词称为论元。语义角色是指论元在动词所指事件中担任的角色。主要有：施事者（Agent）、受事者（Patient）、客体（Theme）、经验者（Experiencer）、受益者（Beneficiary）、工具（Instrument）、处所（Location）、目标（Goal）和来源（Source）等。示例的Python代码rolelabel.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor, Postagger, Parser, SementicRoleLabeller<br><br><span class="hljs-comment"># 分词</span><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br><br>postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br>postags = postagger.postag(words)  <span class="hljs-comment"># 词性标注</span><br><br><span class="hljs-comment"># 依存句法分析</span><br>par_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/parser.model&#x27;</span>)  <span class="hljs-comment"># 模型路径，模型名称为`parser.model`</span><br><br>parser = Parser() <span class="hljs-comment"># 初始化实例</span><br>parser.load(par_model_path)  <span class="hljs-comment"># 加载模型</span><br>arcs = parser.parse(words, postags)  <span class="hljs-comment"># 句法分析</span><br><br><span class="hljs-comment"># 语义角色标注</span><br>srl_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pisrl.model&#x27;</span>)  <span class="hljs-comment"># 语义角色标注模型目录路径</span><br>labeller = SementicRoleLabeller() <span class="hljs-comment"># 初始化实例</span><br>labeller.load(srl_model_path)  <span class="hljs-comment"># 加载模型</span><br>roles = labeller.label(words, postags, arcs)  <span class="hljs-comment"># 语义角色标注</span><br><br><span class="hljs-comment"># 打印结果</span><br><span class="hljs-keyword">for</span> role <span class="hljs-keyword">in</span> roles:<br>    <span class="hljs-built_in">print</span>(words[role.index], end=<span class="hljs-string">&#x27; &#x27;</span>)<br>    <span class="hljs-built_in">print</span>(role.index, <span class="hljs-string">&quot;&quot;</span>.join([<span class="hljs-string">&quot;%s:(%d,%d)&quot;</span> % (arg.name, arg.<span class="hljs-built_in">range</span>.start, arg.<span class="hljs-built_in">range</span>.end) <span class="hljs-keyword">for</span> arg <span class="hljs-keyword">in</span> role.arguments]))<br><br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br>parser.release()<br>labeller.release()<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">反映 <span class="hljs-number">4</span> <span class="hljs-built_in">A0</span>:(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<span class="hljs-built_in">A0</span>:(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<br>表示 <span class="hljs-number">11</span> MNR:(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>)<span class="hljs-built_in">A0</span>:(<span class="hljs-number">6</span>,<span class="hljs-number">9</span>)TMP:(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>)<span class="hljs-built_in">A1</span>:(<span class="hljs-number">13</span>,<span class="hljs-number">22</span>)<br>访问 <span class="hljs-number">21</span> <span class="hljs-built_in">A0</span>:(<span class="hljs-number">13</span>,<span class="hljs-number">15</span>)ADV:(<span class="hljs-number">16</span>,<span class="hljs-number">16</span>)TMP:(<span class="hljs-number">17</span>,<span class="hljs-number">20</span>)<span class="hljs-built_in">A1</span>:(<span class="hljs-number">22</span>,<span class="hljs-number">22</span>)<br></code></pre></td></tr></table></figure><h3 id="总结">总结</h3><p>本文介绍了中文NLP的一个杰出工具pyltp，并给出了该模块的各个功能的一个示例，希望能给读者一些思考与启示。本文到此结束，感谢大家阅读~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>pyltp</tag>
      
      <tag>NLP工具</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（五）用深度学习实现命名实体识别（NER）</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%94%EF%BC%89%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%94%EF%BC%89%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="前言">前言</h3><p>在文章：<ahref="https://percent4.github.io/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/">NLP入门（四）命名实体识别（NER）</a>中，笔者介绍了两个实现命名实体识别的工具——NLTK和StanfordNLP。在本文中，我们将会学习到如何使用深度学习工具来自己一步步地实现NER，只要你坚持看完，就一定会很有收获的。</p><p>OK，话不多说，让我们进入正题。</p><p>几乎所有的NLP都依赖一个强大的语料库，本项目实现NER的语料库如下(文件名为train.txt，一共42000行，这里只展示前15行，可以在文章最后的Github地址下载该语料库)：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">playedonMonday(hometeam<span class="hljs-keyword">in</span>CAPS):<br>VBDINNNP(NNNNINNNP):<br>OOOOOOOOOO<br>AmericanLeague<br>NNPNNP<br>B-MISCI-MISC<br>Cleveland2DETROIT1<br>NNPCDNNPCD<br>B-ORGOB-ORGO<br>BALTIMORE12Oakland11(10innings)<br>VBCDNNPCD(CDNN)<br>B-ORGOB-ORGOOOOO<br>TORONTO5Minnesota3<br>TOCDNNPCD<br>B-ORGOB-ORGO<br>......<br></code></pre></td></tr></table></figure><p>简单介绍下该语料库的结构：该语料库一共42000行，每三行为一组，其中，第一行为英语句子，第二行为每个句子的词性（关于英语单词的词性，可参考文章：<ahref="https://percent4.github.io/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F%EF%BC%88Lemmatization%EF%BC%89/">NLP入门（三）词形还原（Lemmatization）</a>），第三行为NER系统的标注，具体的含义会在之后介绍。</p><p>我们的NER项目的名称为DL_4_NER，结构如下：</p><figure><img src="/img/nlp5_1.png" alt="NER项目名称" /><figcaption aria-hidden="true">NER项目名称</figcaption></figure><p>项目中每个文件的功能如下：</p><ul><li><p>utils.py: 项目配置及数据导入</p></li><li><p>data_processing.py: 数据探索</p></li><li><p>Bi_LSTM_Model_training.py: 模型创建及训练</p></li><li><p>Bi_LSTM_Model_predict.py: 对新句子进行NER预测</p><p>接下来，笔者将结合代码文件，分部介绍该项目的步骤，当所有步骤介绍完毕后，我们的项目就结束了，而你，也就知道了如何用深度学习实现命名实体识别（NER）。</p><p>Let's begin!</p></li></ul><h3 id="项目配置">项目配置</h3><p>第一步，是项目的配置及数据导入，在utils.py文件中实现，完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><span class="hljs-comment"># basic settings for DL_4_NER Project</span><br>BASE_DIR = <span class="hljs-string">&quot;F://NERSystem&quot;</span><br>CORPUS_PATH = <span class="hljs-string">&quot;%s/train.txt&quot;</span> % BASE_DIR<br><br>KERAS_MODEL_SAVE_PATH = <span class="hljs-string">&#x27;%s/Bi-LSTM-4-NER.h5&#x27;</span> % BASE_DIR<br>WORD_DICTIONARY_PATH = <span class="hljs-string">&#x27;%s/word_dictionary.pk&#x27;</span> % BASE_DIR<br>InVERSE_WORD_DICTIONARY_PATH = <span class="hljs-string">&#x27;%s/inverse_word_dictionary.pk&#x27;</span> % BASE_DIR<br>LABEL_DICTIONARY_PATH = <span class="hljs-string">&#x27;%s/label_dictionary.pk&#x27;</span> % BASE_DIR<br>OUTPUT_DICTIONARY_PATH = <span class="hljs-string">&#x27;%s/output_dictionary.pk&#x27;</span> % BASE_DIR<br><br>CONSTANTS = [<br>             KERAS_MODEL_SAVE_PATH,<br>             InVERSE_WORD_DICTIONARY_PATH,<br>             WORD_DICTIONARY_PATH,<br>             LABEL_DICTIONARY_PATH,<br>             OUTPUT_DICTIONARY_PATH<br>             ]<br><br><span class="hljs-comment"># load data from corpus to from pandas DataFrame</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>():<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CORPUS_PATH, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        text_data = [text.strip() <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> f.readlines()]<br>    text_data = [text_data[k].split(<span class="hljs-string">&#x27;\t&#x27;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(text_data))]<br>    index = <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(text_data), <span class="hljs-number">3</span>)<br><br>    <span class="hljs-comment"># Transforming data to matrix format for neural network</span><br>    input_data = <span class="hljs-built_in">list</span>()<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(index) - <span class="hljs-number">1</span>):<br>        rows = text_data[index[i-<span class="hljs-number">1</span>]:index[i]]<br>        sentence_no = np.array([i]*<span class="hljs-built_in">len</span>(rows[<span class="hljs-number">0</span>]), dtype=<span class="hljs-built_in">str</span>)<br>        rows.append(sentence_no)<br>        rows = np.array(rows).T<br>        input_data.append(rows)<br><br>    input_data = pd.DataFrame(np.concatenate([item <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> input_data]),\<br>                               columns=[<span class="hljs-string">&#x27;word&#x27;</span>, <span class="hljs-string">&#x27;pos&#x27;</span>, <span class="hljs-string">&#x27;tag&#x27;</span>, <span class="hljs-string">&#x27;sent_no&#x27;</span>])<br><br>    <span class="hljs-keyword">return</span> input_data<br></code></pre></td></tr></table></figure><p>在该代码中，先是设置了语料库文件的路径CORPUS_PATH，KERAS模型保存路径KERAS_MODEL_SAVE_PATH，以及在项目过程中会用到的三个字典的保存路径（以pickle文件形式保存）WORD_DICTIONARY_PATH，LABEL_DICTIONARY_PATH，OUTPUT_DICTIONARY_PATH。然后是load_data()函数，它将语料库中的文本以Pandas中的DataFrame结构展示出来，该数据框的前30行如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs apache">         <span class="hljs-attribute">word</span>  pos     tag sent_no<br><span class="hljs-attribute">0</span>      played  VBD       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">1</span>          <span class="hljs-literal">on</span>   IN       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">2</span>      Monday  NNP       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">3</span>           (    (       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">4</span>        home   NN       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">5</span>        team   NN       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">6</span>          in   IN       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">7</span>        CAPS  NNP       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">8</span>           )    )       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">9</span>           :    :       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">10</span>   American  NNP  B-MISC       <span class="hljs-number">2</span><br><span class="hljs-attribute">11</span>     League  NNP  I-MISC       <span class="hljs-number">2</span><br><span class="hljs-attribute">12</span>  Cleveland  NNP   B-ORG       <span class="hljs-number">3</span><br><span class="hljs-attribute">13</span>          <span class="hljs-number">2</span>   CD       O       <span class="hljs-number">3</span><br><span class="hljs-attribute">14</span>    DETROIT  NNP   B-ORG       <span class="hljs-number">3</span><br><span class="hljs-attribute">15</span>          <span class="hljs-number">1</span>   CD       O       <span class="hljs-number">3</span><br><span class="hljs-attribute">16</span>  BALTIMORE   VB   B-ORG       <span class="hljs-number">4</span><br><span class="hljs-attribute">17</span>         <span class="hljs-number">12</span>   CD       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">18</span>    Oakland  NNP   B-ORG       <span class="hljs-number">4</span><br><span class="hljs-attribute">19</span>         <span class="hljs-number">11</span>   CD       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">20</span>          (    (       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">21</span>         <span class="hljs-number">10</span>   CD       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">22</span>    innings   NN       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">23</span>          )    )       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">24</span>    TORONTO   TO   B-ORG       <span class="hljs-number">5</span><br><span class="hljs-attribute">25</span>          <span class="hljs-number">5</span>   CD       O       <span class="hljs-number">5</span><br><span class="hljs-attribute">26</span>  Minnesota  NNP   B-ORG       <span class="hljs-number">5</span><br><span class="hljs-attribute">27</span>          <span class="hljs-number">3</span>   CD       O       <span class="hljs-number">5</span><br><span class="hljs-attribute">28</span>  Milwaukee  NNP   B-ORG       <span class="hljs-number">6</span><br><span class="hljs-attribute">29</span>          <span class="hljs-number">3</span>   CD       O       <span class="hljs-number">6</span><br></code></pre></td></tr></table></figure><p>在该数据框中，word这一列表示文本语料库中的单词，pos这一列表示该单词的词性，tag这一列表示NER的标注，sent_no这一列表示该单词在第几个句子中。</p><h3 id="数据探索">数据探索</h3><p>接着，第二步是数据探索，即对输入的数据（input_data）进行一些数据review，完整的代码（data_processing.py）如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> accumulate<br><span class="hljs-keyword">from</span> operator <span class="hljs-keyword">import</span> itemgetter<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib <span class="hljs-keyword">as</span> mpl<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> BASE_DIR, CONSTANTS, load_data<br><br><span class="hljs-comment"># 设置matplotlib绘图时的字体</span><br>mpl.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>]=[<span class="hljs-string">&#x27;SimHei&#x27;</span>]<br><br><span class="hljs-comment"># 数据查看</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_review</span>():<br><br>    <span class="hljs-comment"># 数据导入</span><br>    input_data = load_data()<br><br>    <span class="hljs-comment"># 基本的数据review</span><br>    sent_num = input_data[<span class="hljs-string">&#x27;sent_no&#x27;</span>].astype(np.<span class="hljs-built_in">int</span>).<span class="hljs-built_in">max</span>()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;一共有%s个句子。\n&quot;</span>%sent_num)<br><br>    vocabulary = input_data[<span class="hljs-string">&#x27;word&#x27;</span>].unique()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;一共有%d个单词。&quot;</span>%<span class="hljs-built_in">len</span>(vocabulary))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;前10个单词为：%s.\n&quot;</span>%vocabulary[:<span class="hljs-number">11</span>])<br><br>    pos_arr = input_data[<span class="hljs-string">&#x27;pos&#x27;</span>].unique()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;单词的词性列表：%s.\n&quot;</span>%pos_arr)<br><br>    ner_tag_arr = input_data[<span class="hljs-string">&#x27;tag&#x27;</span>].unique()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;NER的标注列表：%s.\n&quot;</span> % ner_tag_arr)<br><br>    df = input_data[[<span class="hljs-string">&#x27;word&#x27;</span>, <span class="hljs-string">&#x27;sent_no&#x27;</span>]].groupby(<span class="hljs-string">&#x27;sent_no&#x27;</span>).count()<br>    sent_len_list = df[<span class="hljs-string">&#x27;word&#x27;</span>].tolist()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;句子长度及出现频数字典：\n%s.&quot;</span> % <span class="hljs-built_in">dict</span>(Counter(sent_len_list)))<br><br>    <span class="hljs-comment"># 绘制句子长度及出现频数统计图</span><br>    sort_sent_len_dist = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">dict</span>(Counter(sent_len_list)).items(), key=itemgetter(<span class="hljs-number">0</span>))<br>    sent_no_data = [item[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sort_sent_len_dist]<br>    sent_count_data = [item[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sort_sent_len_dist]<br>    plt.bar(sent_no_data, sent_count_data)<br>    plt.title(<span class="hljs-string">&quot;句子长度及出现频数统计图&quot;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;句子长度&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;句子长度出现的频数&quot;</span>)<br>    plt.savefig(<span class="hljs-string">&quot;%s/句子长度及出现频数统计图.png&quot;</span> % BASE_DIR)<br>    plt.close()<br><br>    <span class="hljs-comment"># 绘制句子长度累积分布函数(CDF)</span><br>    sent_pentage_list = [(count/sent_num) <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> accumulate(sent_count_data)]<br><br>    <span class="hljs-comment"># 寻找分位点为quantile的句子长度</span><br>    quantile = <span class="hljs-number">0.9992</span><br>    <span class="hljs-comment">#print(list(sent_pentage_list))</span><br>    <span class="hljs-keyword">for</span> length, per <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(sent_no_data, sent_pentage_list):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">round</span>(per, <span class="hljs-number">4</span>) == quantile:<br>            index = length<br>            <span class="hljs-keyword">break</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n分位点为%s的句子长度:%d.&quot;</span> % (quantile, index))<br><br>    <span class="hljs-comment"># 绘制CDF</span><br>    plt.plot(sent_no_data, sent_pentage_list)<br>    plt.hlines(quantile, <span class="hljs-number">0</span>, index, colors=<span class="hljs-string">&quot;c&quot;</span>, linestyles=<span class="hljs-string">&quot;dashed&quot;</span>)<br>    plt.vlines(index, <span class="hljs-number">0</span>, quantile, colors=<span class="hljs-string">&quot;c&quot;</span>, linestyles=<span class="hljs-string">&quot;dashed&quot;</span>)<br>    plt.text(<span class="hljs-number">0</span>, quantile, <span class="hljs-built_in">str</span>(quantile))<br>    plt.text(index, <span class="hljs-number">0</span>, <span class="hljs-built_in">str</span>(index))<br>    plt.title(<span class="hljs-string">&quot;句子长度累积分布函数图&quot;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;句子长度&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;句子长度累积频率&quot;</span>)<br>    plt.savefig(<span class="hljs-string">&quot;%s/句子长度累积分布函数图.png&quot;</span> % BASE_DIR)<br>    plt.close()<br><br><span class="hljs-comment"># 数据处理</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_processing</span>():<br>    <span class="hljs-comment"># 数据导入</span><br>    input_data = load_data()<br><br>    <span class="hljs-comment"># 标签及词汇表</span><br>    labels, vocabulary = <span class="hljs-built_in">list</span>(input_data[<span class="hljs-string">&#x27;tag&#x27;</span>].unique()), <span class="hljs-built_in">list</span>(input_data[<span class="hljs-string">&#x27;word&#x27;</span>].unique())<br><br>    <span class="hljs-comment"># 字典列表</span><br>    word_dictionary = &#123;word: i+<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocabulary)&#125;<br>    inverse_word_dictionary = &#123;i+<span class="hljs-number">1</span>: word <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocabulary)&#125;<br>    label_dictionary = &#123;label: i+<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels)&#125;<br>    output_dictionary = &#123;i+<span class="hljs-number">1</span>: labels <span class="hljs-keyword">for</span> i, labels <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels)&#125;<br><br>    dict_list = [word_dictionary, inverse_word_dictionary,label_dictionary, output_dictionary]<br><br>    <span class="hljs-comment"># 保存为pickle形式</span><br>    <span class="hljs-keyword">for</span> dict_item, path <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(dict_list, CONSTANTS[<span class="hljs-number">1</span>:]):<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(path, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            pickle.dump(dict_item, f)<br><br><span class="hljs-comment">#data_review()</span><br></code></pre></td></tr></table></figure><p>调用data_review()函数，输出的结果如下：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs prolog">一共有<span class="hljs-number">13998</span>个句子。<br><br>一共有<span class="hljs-number">24339</span>个单词。<br>前<span class="hljs-number">10</span>个单词为：[<span class="hljs-string">&#x27;played&#x27;</span> <span class="hljs-string">&#x27;on&#x27;</span> <span class="hljs-string">&#x27;Monday&#x27;</span> <span class="hljs-string">&#x27;(&#x27;</span> <span class="hljs-string">&#x27;home&#x27;</span> <span class="hljs-string">&#x27;team&#x27;</span> <span class="hljs-string">&#x27;in&#x27;</span> <span class="hljs-string">&#x27;CAPS&#x27;</span> <span class="hljs-string">&#x27;)&#x27;</span> <span class="hljs-string">&#x27;:&#x27;</span> <span class="hljs-string">&#x27;American&#x27;</span>].<br><br>单词的词性列表：[<span class="hljs-string">&#x27;VBD&#x27;</span> <span class="hljs-string">&#x27;IN&#x27;</span> <span class="hljs-string">&#x27;NNP&#x27;</span> <span class="hljs-string">&#x27;(&#x27;</span> <span class="hljs-string">&#x27;NN&#x27;</span> <span class="hljs-string">&#x27;)&#x27;</span> <span class="hljs-string">&#x27;:&#x27;</span> <span class="hljs-string">&#x27;CD&#x27;</span> <span class="hljs-string">&#x27;VB&#x27;</span> <span class="hljs-string">&#x27;TO&#x27;</span> <span class="hljs-string">&#x27;NNS&#x27;</span> <span class="hljs-string">&#x27;,&#x27;</span> <span class="hljs-string">&#x27;VBP&#x27;</span> <span class="hljs-string">&#x27;VBZ&#x27;</span><br> <span class="hljs-string">&#x27;.&#x27;</span> <span class="hljs-string">&#x27;VBG&#x27;</span> <span class="hljs-string">&#x27;PRP$&#x27;</span> <span class="hljs-string">&#x27;JJ&#x27;</span> <span class="hljs-string">&#x27;CC&#x27;</span> <span class="hljs-string">&#x27;JJS&#x27;</span> <span class="hljs-string">&#x27;RB&#x27;</span> <span class="hljs-string">&#x27;DT&#x27;</span> <span class="hljs-string">&#x27;VBN&#x27;</span> <span class="hljs-string">&#x27;&quot;&#x27;</span> <span class="hljs-string">&#x27;PRP&#x27;</span> <span class="hljs-string">&#x27;WDT&#x27;</span> <span class="hljs-string">&#x27;WRB&#x27;</span><br> <span class="hljs-string">&#x27;MD&#x27;</span> <span class="hljs-string">&#x27;WP&#x27;</span> <span class="hljs-string">&#x27;POS&#x27;</span> <span class="hljs-string">&#x27;JJR&#x27;</span> <span class="hljs-string">&#x27;WP$&#x27;</span> <span class="hljs-string">&#x27;RP&#x27;</span> <span class="hljs-string">&#x27;NNPS&#x27;</span> <span class="hljs-string">&#x27;RBS&#x27;</span> <span class="hljs-string">&#x27;FW&#x27;</span> <span class="hljs-string">&#x27;$&#x27;</span> <span class="hljs-string">&#x27;RBR&#x27;</span> <span class="hljs-string">&#x27;EX&#x27;</span> <span class="hljs-string">&quot;&#x27;&#x27;&quot;</span><br> <span class="hljs-string">&#x27;PDT&#x27;</span> <span class="hljs-string">&#x27;UH&#x27;</span> <span class="hljs-string">&#x27;SYM&#x27;</span> <span class="hljs-string">&#x27;LS&#x27;</span> <span class="hljs-string">&#x27;NN|SYM&#x27;</span>].<br><br><span class="hljs-symbol">NER</span>的标注列表：[<span class="hljs-string">&#x27;O&#x27;</span> <span class="hljs-string">&#x27;B-MISC&#x27;</span> <span class="hljs-string">&#x27;I-MISC&#x27;</span> <span class="hljs-string">&#x27;B-ORG&#x27;</span> <span class="hljs-string">&#x27;I-ORG&#x27;</span> <span class="hljs-string">&#x27;B-PER&#x27;</span> <span class="hljs-string">&#x27;B-LOC&#x27;</span> <span class="hljs-string">&#x27;I-PER&#x27;</span> <span class="hljs-string">&#x27;I-LOC&#x27;</span><br> <span class="hljs-string">&#x27;sO&#x27;</span>].<br><br>句子长度及出现频数字典：<br>&#123;<span class="hljs-number">1</span>: <span class="hljs-number">177</span>, <span class="hljs-number">2</span>: <span class="hljs-number">1141</span>, <span class="hljs-number">3</span>: <span class="hljs-number">620</span>, <span class="hljs-number">4</span>: <span class="hljs-number">794</span>, <span class="hljs-number">5</span>: <span class="hljs-number">769</span>, <span class="hljs-number">6</span>: <span class="hljs-number">639</span>, <span class="hljs-number">7</span>: <span class="hljs-number">999</span>, <span class="hljs-number">8</span>: <span class="hljs-number">977</span>, <span class="hljs-number">9</span>: <span class="hljs-number">841</span>, <span class="hljs-number">10</span>: <span class="hljs-number">501</span>, <span class="hljs-number">11</span>: <span class="hljs-number">395</span>, <span class="hljs-number">12</span>: <span class="hljs-number">316</span>, <span class="hljs-number">13</span>: <span class="hljs-number">339</span>, <span class="hljs-number">14</span>: <span class="hljs-number">291</span>, <span class="hljs-number">15</span>: <span class="hljs-number">275</span>, <span class="hljs-number">16</span>: <span class="hljs-number">225</span>, <span class="hljs-number">17</span>: <span class="hljs-number">229</span>, <span class="hljs-number">18</span>: <span class="hljs-number">212</span>, <span class="hljs-number">19</span>: <span class="hljs-number">197</span>, <span class="hljs-number">20</span>: <span class="hljs-number">221</span>, <span class="hljs-number">21</span>: <span class="hljs-number">228</span>, <span class="hljs-number">22</span>: <span class="hljs-number">221</span>, <span class="hljs-number">23</span>: <span class="hljs-number">230</span>, <span class="hljs-number">24</span>: <span class="hljs-number">210</span>, <span class="hljs-number">25</span>: <span class="hljs-number">207</span>, <span class="hljs-number">26</span>: <span class="hljs-number">224</span>, <span class="hljs-number">27</span>: <span class="hljs-number">188</span>, <span class="hljs-number">28</span>: <span class="hljs-number">199</span>, <span class="hljs-number">29</span>: <span class="hljs-number">214</span>, <span class="hljs-number">30</span>: <span class="hljs-number">183</span>, <span class="hljs-number">31</span>: <span class="hljs-number">202</span>, <span class="hljs-number">32</span>: <span class="hljs-number">167</span>, <span class="hljs-number">33</span>: <span class="hljs-number">167</span>, <span class="hljs-number">34</span>: <span class="hljs-number">141</span>, <span class="hljs-number">35</span>: <span class="hljs-number">130</span>, <span class="hljs-number">36</span>: <span class="hljs-number">119</span>, <span class="hljs-number">37</span>: <span class="hljs-number">105</span>, <span class="hljs-number">38</span>: <span class="hljs-number">112</span>, <span class="hljs-number">39</span>: <span class="hljs-number">98</span>, <span class="hljs-number">40</span>: <span class="hljs-number">78</span>, <span class="hljs-number">41</span>: <span class="hljs-number">74</span>, <span class="hljs-number">42</span>: <span class="hljs-number">63</span>, <span class="hljs-number">43</span>: <span class="hljs-number">51</span>, <span class="hljs-number">44</span>: <span class="hljs-number">42</span>, <span class="hljs-number">45</span>: <span class="hljs-number">39</span>, <span class="hljs-number">46</span>: <span class="hljs-number">19</span>, <span class="hljs-number">47</span>: <span class="hljs-number">22</span>, <span class="hljs-number">48</span>: <span class="hljs-number">19</span>, <span class="hljs-number">49</span>: <span class="hljs-number">15</span>, <span class="hljs-number">50</span>: <span class="hljs-number">16</span>, <span class="hljs-number">51</span>: <span class="hljs-number">8</span>, <span class="hljs-number">52</span>: <span class="hljs-number">9</span>, <span class="hljs-number">53</span>: <span class="hljs-number">5</span>, <span class="hljs-number">54</span>: <span class="hljs-number">4</span>, <span class="hljs-number">55</span>: <span class="hljs-number">9</span>, <span class="hljs-number">56</span>: <span class="hljs-number">2</span>, <span class="hljs-number">57</span>: <span class="hljs-number">2</span>, <span class="hljs-number">58</span>: <span class="hljs-number">2</span>, <span class="hljs-number">59</span>: <span class="hljs-number">2</span>, <span class="hljs-number">60</span>: <span class="hljs-number">3</span>, <span class="hljs-number">62</span>: <span class="hljs-number">2</span>, <span class="hljs-number">66</span>: <span class="hljs-number">1</span>, <span class="hljs-number">67</span>: <span class="hljs-number">1</span>, <span class="hljs-number">69</span>: <span class="hljs-number">1</span>, <span class="hljs-number">71</span>: <span class="hljs-number">1</span>, <span class="hljs-number">72</span>: <span class="hljs-number">1</span>, <span class="hljs-number">78</span>: <span class="hljs-number">1</span>, <span class="hljs-number">80</span>: <span class="hljs-number">1</span>, <span class="hljs-number">113</span>: <span class="hljs-number">1</span>, <span class="hljs-number">124</span>: <span class="hljs-number">1</span>&#125;.<br><br>分位点为<span class="hljs-number">0.9992</span>的句子长度:<span class="hljs-number">60.</span><br></code></pre></td></tr></table></figure><p>在该语料库中，一共有13998个句子，比预期的42000/3=14000个句子少两个。一个有24339个单词，单词量还是蛮大的，当然，这里对单词没有做任何处理，直接保留了语料库中的形式（后期可以继续优化）。单词的词性可以参考文章：<ahref="https://www.jianshu.com/p/79255fe0c5b5">NLP入门（三）词形还原（Lemmatization）</a>。我们需要注意的是，NER的标注列表为['O','B-MISC', 'I-MISC', 'B-ORG' ,'I-ORG', 'B-PER' ,'B-LOC' ,'I-PER','I-LOC','sO']，因此，本项目的NER一共分为四类：PER（人名），LOC（位置），ORG（组织）以及MISC，其中B表示开始，I表示中间，O表示单字词，不计入NER，sO表示特殊单字词。</p><p>接下来，让我们考虑下句子的长度，这对后面的建模时填充的句子长度有有参考作用。句子长度及出现频数的统计图如下：</p><figure><img src="/img/nlp5_2.png" alt="句子长度及出现频数统计图" /><figcaption aria-hidden="true">句子长度及出现频数统计图</figcaption></figure><p>可以看到，句子长度基本在60以下，当然，这也可以在输出的句子长度及出现频数字典中看到。那么，我们是否可以选在一个标准作为后面模型的句子填充的长度呢？答案是，利用出现频数的累计分布函数的分位点，在这里，我们选择分位点为0.9992,对应的句子长度为60，如下图：</p><figure><img src="/img/nlp5_3.png" alt="句子长度累积分布函数图" /><figcaption aria-hidden="true">句子长度累积分布函数图</figcaption></figure><p>接着是数据处理函数data_processing()，它的功能主要是实现单词、标签字典，并保存为pickle文件形式，便于后续直接调用。</p><h3 id="建模">建模</h3><p>在第三步中，我们建立Bi-LSTM模型来训练训练，完整的Python代码（Bi_LSTM_Model_training.py）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> BASE_DIR, CONSTANTS, load_data<br><span class="hljs-keyword">from</span> data_processing <span class="hljs-keyword">import</span> data_processing<br><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> np_utils, plot_model<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Bidirectional, LSTM, Dense, Embedding, TimeDistributed<br><br><br><span class="hljs-comment"># 模型输入数据</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">input_data_for_model</span>(<span class="hljs-params">input_shape</span>):<br><br>    <span class="hljs-comment"># 数据导入</span><br>    input_data = load_data()<br>    <span class="hljs-comment"># 数据处理</span><br>    data_processing()<br>    <span class="hljs-comment"># 导入字典</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">1</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        word_dictionary = pickle.load(f)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">2</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        inverse_word_dictionary = pickle.load(f)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">3</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        label_dictionary = pickle.load(f)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">4</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        output_dictionary = pickle.load(f)<br>    vocab_size = <span class="hljs-built_in">len</span>(word_dictionary.keys())<br>    label_size = <span class="hljs-built_in">len</span>(label_dictionary.keys())<br><br>    <span class="hljs-comment"># 处理输入数据</span><br>    aggregate_function = <span class="hljs-keyword">lambda</span> <span class="hljs-built_in">input</span>: [(word, pos, label) <span class="hljs-keyword">for</span> word, pos, label <span class="hljs-keyword">in</span><br>                                            <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;word&#x27;</span>].values.tolist(),<br>                                                <span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;pos&#x27;</span>].values.tolist(),<br>                                                <span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;tag&#x27;</span>].values.tolist())]<br><br>    grouped_input_data = input_data.groupby(<span class="hljs-string">&#x27;sent_no&#x27;</span>).apply(aggregate_function)<br>    sentences = [sentence <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> grouped_input_data]<br><br>    x = [[word_dictionary[word[<span class="hljs-number">0</span>]] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sentences]<br>    x = pad_sequences(maxlen=input_shape, sequences=x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br>    y = [[label_dictionary[word[<span class="hljs-number">2</span>]] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sentences]<br>    y = pad_sequences(maxlen=input_shape, sequences=y, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br>    y = [np_utils.to_categorical(label, num_classes=label_size + <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> y]<br><br>    <span class="hljs-keyword">return</span> x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary<br><br><br><span class="hljs-comment"># 定义深度学习模型：Bi-LSTM</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_Bi_LSTM</span>(<span class="hljs-params">vocab_size, label_size, input_shape, output_dim, n_units, out_act, activation</span>):<br>    model = Sequential()<br>    model.add(Embedding(input_dim=vocab_size + <span class="hljs-number">1</span>, output_dim=output_dim,<br>                        input_length=input_shape, mask_zero=<span class="hljs-literal">True</span>))<br>    model.add(Bidirectional(LSTM(units=n_units, activation=activation,<br>                                 return_sequences=<span class="hljs-literal">True</span>)))<br>    model.add(TimeDistributed(Dense(label_size + <span class="hljs-number">1</span>, activation=out_act)))<br>    model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>, loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-comment"># 模型训练</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_train</span>():<br><br>    <span class="hljs-comment"># 将数据集分为训练集和测试集，占比为9:1</span><br>    input_shape = <span class="hljs-number">60</span><br>    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = input_data_for_model(input_shape)<br>    train_end = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(x)*<span class="hljs-number">0.9</span>)<br>    train_x, train_y = x[<span class="hljs-number">0</span>:train_end], np.array(y[<span class="hljs-number">0</span>:train_end])<br>    test_x, test_y = x[train_end:], np.array(y[train_end:])<br><br>    <span class="hljs-comment"># 模型输入参数</span><br>    activation = <span class="hljs-string">&#x27;selu&#x27;</span><br>    out_act = <span class="hljs-string">&#x27;softmax&#x27;</span><br>    n_units = <span class="hljs-number">100</span><br>    batch_size = <span class="hljs-number">32</span><br>    epochs = <span class="hljs-number">10</span><br>    output_dim = <span class="hljs-number">20</span><br><br>    <span class="hljs-comment"># 模型训练</span><br>    lstm_model = create_Bi_LSTM(vocab_size, label_size, input_shape, output_dim, n_units, out_act, activation)<br>    lstm_model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 模型保存</span><br>    model_save_path = CONSTANTS[<span class="hljs-number">0</span>]<br>    lstm_model.save(model_save_path)<br>    plot_model(lstm_model, to_file=<span class="hljs-string">&#x27;%s/LSTM_model.png&#x27;</span> % BASE_DIR)<br><br>    <span class="hljs-comment"># 在测试集上的效果</span><br>    N = test_x.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 测试的条数</span><br>    avg_accuracy = <span class="hljs-number">0</span>  <span class="hljs-comment"># 预测的平均准确率</span><br>    <span class="hljs-keyword">for</span> start, end <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, N, <span class="hljs-number">1</span>), <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, N+<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)):<br>        sentence = [inverse_word_dictionary[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> test_x[start] <span class="hljs-keyword">if</span> i != <span class="hljs-number">0</span>]<br>        y_predict = lstm_model.predict(test_x[start:end])<br>        input_sequences, output_sequences = [], []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(y_predict[<span class="hljs-number">0</span>])):<br>            output_sequences.append(np.argmax(y_predict[<span class="hljs-number">0</span>][i]))<br>            input_sequences.append(np.argmax(test_y[start][i]))<br><br>        <span class="hljs-built_in">eval</span> = lstm_model.evaluate(test_x[start:end], test_y[start:end])<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test Accuracy: loss = %0.6f accuracy = %0.2f%%&#x27;</span> % (<span class="hljs-built_in">eval</span>[<span class="hljs-number">0</span>], <span class="hljs-built_in">eval</span>[<span class="hljs-number">1</span>] * <span class="hljs-number">100</span>))<br>        avg_accuracy += <span class="hljs-built_in">eval</span>[<span class="hljs-number">1</span>]<br>        output_sequences = <span class="hljs-string">&#x27; &#x27;</span>.join([output_dictionary[key] <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> output_sequences <span class="hljs-keyword">if</span> key != <span class="hljs-number">0</span>]).split()<br>        input_sequences = <span class="hljs-string">&#x27; &#x27;</span>.join([output_dictionary[key] <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> input_sequences <span class="hljs-keyword">if</span> key != <span class="hljs-number">0</span>]).split()<br>        output_input_comparison = pd.DataFrame([sentence, output_sequences, input_sequences]).T<br>        <span class="hljs-built_in">print</span>(output_input_comparison.dropna())<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;#&#x27;</span> * <span class="hljs-number">80</span>)<br><br>    avg_accuracy /= N<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;测试样本的平均预测准确率：%.2f%%.&quot;</span> % (avg_accuracy * <span class="hljs-number">100</span>))<br><br>model_train()<br></code></pre></td></tr></table></figure><p>在上面的代码中，先是通过input_data_for_model()函数来处理好进入模型的数据，其参数为input_shape，即填充句子时的长度。然后是创建Bi-LSTM模型create_Bi_LSTM()，模型的示意图如下：</p><figure><img src="/img/nlp5_4.png" alt="模型示意图" /><figcaption aria-hidden="true">模型示意图</figcaption></figure><p>最后，是在输入的数据上进行模型训练，将原始的数据分为训练集和测试集，占比为9:1，训练的周期为10次。</p><h3 id="模型训练">模型训练</h3><p>运行上述模型训练代码，一共训练10个周期，训练时间大概为500s，在训练集上的准确率达99%以上，在测试集上的平均准确率为95%以上。以下是最后几个测试集上的预测结果：</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs clean">......(前面的输出已忽略)<br>Test Accuracy: loss = <span class="hljs-number">0.000986</span> accuracy = <span class="hljs-number">100.00</span>%<br>          <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>   Cardiff  B-ORG  B-ORG<br><span class="hljs-number">1</span>         <span class="hljs-number">1</span>      O      O<br><span class="hljs-number">2</span>  Brighton  B-ORG  B-ORG<br><span class="hljs-number">3</span>         <span class="hljs-number">0</span>      O      O<br>################################################################################<br><br><span class="hljs-number">1</span>/<span class="hljs-number">1</span> [==============================] - <span class="hljs-number">0</span>s <span class="hljs-number">10</span>ms/step<br>Test Accuracy: loss = <span class="hljs-number">0.000274</span> accuracy = <span class="hljs-number">100.00</span>%<br>          <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>  Carlisle  B-ORG  B-ORG<br><span class="hljs-number">1</span>         <span class="hljs-number">0</span>      O      O<br><span class="hljs-number">2</span>      Hull  B-ORG  B-ORG<br><span class="hljs-number">3</span>         <span class="hljs-number">0</span>      O      O<br>################################################################################<br><br><span class="hljs-number">1</span>/<span class="hljs-number">1</span> [==============================] - <span class="hljs-number">0</span>s <span class="hljs-number">9</span>ms/step<br>Test Accuracy: loss = <span class="hljs-number">0.000479</span> accuracy = <span class="hljs-number">100.00</span>%<br>           <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>    Chester  B-ORG  B-ORG<br><span class="hljs-number">1</span>          <span class="hljs-number">1</span>      O      O<br><span class="hljs-number">2</span>  Cambridge  B-ORG  B-ORG<br><span class="hljs-number">3</span>          <span class="hljs-number">1</span>      O      O<br>################################################################################<br><br><span class="hljs-number">1</span>/<span class="hljs-number">1</span> [==============================] - <span class="hljs-number">0</span>s <span class="hljs-number">9</span>ms/step<br>Test Accuracy: loss = <span class="hljs-number">0.003092</span> accuracy = <span class="hljs-number">100.00</span>%<br>            <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>  Darlington  B-ORG  B-ORG<br><span class="hljs-number">1</span>           <span class="hljs-number">4</span>      O      O<br><span class="hljs-number">2</span>     Swansea  B-ORG  B-ORG<br><span class="hljs-number">3</span>           <span class="hljs-number">1</span>      O      O<br>################################################################################<br><br><span class="hljs-number">1</span>/<span class="hljs-number">1</span> [==============================] - <span class="hljs-number">0</span>s <span class="hljs-number">8</span>ms/step<br>Test Accuracy: loss = <span class="hljs-number">0.000705</span> accuracy = <span class="hljs-number">100.00</span>%<br>             <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>       Exeter  B-ORG  B-ORG<br><span class="hljs-number">1</span>            <span class="hljs-number">2</span>      O      O<br><span class="hljs-number">2</span>  Scarborough  B-ORG  B-ORG<br><span class="hljs-number">3</span>            <span class="hljs-number">2</span>      O      O<br>################################################################################<br>测试样本的平均预测准确率：<span class="hljs-number">95.55</span>%.<br></code></pre></td></tr></table></figure><p>该模型在原始数据上的识别效果还是可以的。</p><p>训练完模型后，BASE_DIR中的所有文件如下：</p><figure><img src="/img/nlp5_5.png" alt="模型训练完后的所有文件截图" /><figcaption aria-hidden="true">模型训练完后的所有文件截图</figcaption></figure><h3 id="模型预测">模型预测</h3><p>最后，也许是整个项目最为激动人心的时刻，因为，我们要在新数据集上测试模型的识别效果。预测新数据的识别结果的完整Python代码（Bi_LSTM_Model_predict.py）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># Name entity recognition for new data</span><br><br><span class="hljs-comment"># Import the necessary modules</span><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> CONSTANTS<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<br><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize<br><br><span class="hljs-comment"># 导入字典</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">1</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    word_dictionary = pickle.load(f)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">4</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    output_dictionary = pickle.load(f)<br><br><span class="hljs-keyword">try</span>:<br>    <span class="hljs-comment"># 数据预处理</span><br>    input_shape = <span class="hljs-number">60</span><br>    sent = <span class="hljs-string">&#x27;New York is the biggest city in America.&#x27;</span><br>    new_sent = word_tokenize(sent)<br>    new_x = [[word_dictionary[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> new_sent]]<br>    x = pad_sequences(maxlen=input_shape, sequences=new_x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># 载入模型</span><br>    model_save_path = CONSTANTS[<span class="hljs-number">0</span>]<br>    lstm_model = load_model(model_save_path)<br><br>    <span class="hljs-comment"># 模型预测</span><br>    y_predict = lstm_model.predict(x)<br><br>    ner_tag = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(new_sent)):<br>        ner_tag.append(np.argmax(y_predict[<span class="hljs-number">0</span>][i]))<br><br>    ner = [output_dictionary[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ner_tag]<br>    <span class="hljs-built_in">print</span>(new_sent)<br>    <span class="hljs-built_in">print</span>(ner)<br><br>    <span class="hljs-comment"># 去掉NER标注为O的元素</span><br>    ner_reg_list = []<br>    <span class="hljs-keyword">for</span> word, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(new_sent, ner):<br>        <span class="hljs-keyword">if</span> tag != <span class="hljs-string">&#x27;O&#x27;</span>:<br>            ner_reg_list.append((word, tag))<br><br>    <span class="hljs-comment"># 输出模型的NER识别结果</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;NER识别结果：&quot;</span>)<br>    <span class="hljs-keyword">if</span> ner_reg_list:<br>        <span class="hljs-keyword">for</span> i, item <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(ner_reg_list):<br>            <span class="hljs-keyword">if</span> item[<span class="hljs-number">1</span>].startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>                end = i+<span class="hljs-number">1</span><br>                <span class="hljs-keyword">while</span> end &lt;= <span class="hljs-built_in">len</span>(ner_reg_list)-<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> ner_reg_list[end][<span class="hljs-number">1</span>].startswith(<span class="hljs-string">&#x27;I&#x27;</span>):<br>                    end += <span class="hljs-number">1</span><br><br>                ner_type = item[<span class="hljs-number">1</span>].split(<span class="hljs-string">&#x27;-&#x27;</span>)[<span class="hljs-number">1</span>]<br>                ner_type_dict = &#123;<span class="hljs-string">&#x27;PER&#x27;</span>: <span class="hljs-string">&#x27;PERSON: &#x27;</span>,<br>                                <span class="hljs-string">&#x27;LOC&#x27;</span>: <span class="hljs-string">&#x27;LOCATION: &#x27;</span>,<br>                                <span class="hljs-string">&#x27;ORG&#x27;</span>: <span class="hljs-string">&#x27;ORGANIZATION: &#x27;</span>,<br>                                <span class="hljs-string">&#x27;MISC&#x27;</span>: <span class="hljs-string">&#x27;MISC: &#x27;</span><br>                                &#125;<br>                <span class="hljs-built_in">print</span>(ner_type_dict[ner_type],\<br>                    <span class="hljs-string">&#x27; &#x27;</span>.join([item[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> ner_reg_list[i:end]]))<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型并未识别任何有效命名实体。&quot;</span>)<br><br><span class="hljs-keyword">except</span> KeyError <span class="hljs-keyword">as</span> err:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;您输入的句子有单词不在词汇表中，请重新输入！&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;不在词汇表中的单词为：%s.&quot;</span> % err)<br></code></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">[<span class="hljs-string">&#x27;New&#x27;</span>, <span class="hljs-string">&#x27;York&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;biggest&#x27;</span>, <span class="hljs-string">&#x27;city&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;America&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]<br>[<span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]<br>NER识别结果：<br><span class="hljs-keyword">LOCATION</span>:  <span class="hljs-built_in">New</span> York<br><span class="hljs-keyword">LOCATION</span>:  America<br></code></pre></td></tr></table></figure><p>接下来，再测试三个笔者自己想的句子：</p><p>输入为： <figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">sent = <span class="hljs-symbol">&#x27;James</span> <span class="hljs-keyword">is</span> a world famous actor, whose home <span class="hljs-keyword">is</span> <span class="hljs-keyword">in</span> London.&#x27;<br></code></pre></td></tr></table></figure> 输出结果为：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;James&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;world&#x27;</span>, <span class="hljs-string">&#x27;famous&#x27;</span>, <span class="hljs-string">&#x27;actor&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;whose&#x27;</span>, <span class="hljs-string">&#x27;home&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;London&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>PERSON:  James<br>LOCATION:  London<br></code></pre></td></tr></table></figure><p>输入为： <figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">sent = <span class="hljs-symbol">&#x27;Oxford</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">in</span> England, Jack <span class="hljs-keyword">is</span> from here.&#x27;<br></code></pre></td></tr></table></figure> 输出为： <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;Oxford&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;England&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;Jack&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;from&#x27;</span>, <span class="hljs-string">&#x27;here&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>PERSON:  Oxford<br>LOCATION:  England<br>PERSON:  Jack<br></code></pre></td></tr></table></figure></p><p>输入为： <figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">sent</span> = <span class="hljs-string">&#x27;I love Shanghai.&#x27;</span><br></code></pre></td></tr></table></figure> 输出为： <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;Shanghai&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>LOCATION:  Shanghai<br></code></pre></td></tr></table></figure></p><p>在上面的例子中，只有Oxford的识别效果不理想，模型将它识别为PERSON，其实应该是ORGANIZATION。</p><p>接下来是三个来自CNN和wikipedia的句子：</p><p>输入为： <figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">sent</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;the US runs the risk of a military defeat by China or Russia&quot;</span><br></code></pre></td></tr></table></figure> 输出为： <figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">[<span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;US&#x27;</span>, <span class="hljs-string">&#x27;runs&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;risk&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;military&#x27;</span>, <span class="hljs-string">&#x27;defeat&#x27;</span>, <span class="hljs-string">&#x27;by&#x27;</span>, <span class="hljs-string">&#x27;China&#x27;</span>, <span class="hljs-string">&#x27;or&#x27;</span>, <span class="hljs-string">&#x27;Russia&#x27;</span>]<br>[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>]<br>NER识别结果：<br><span class="hljs-keyword">LOCATION</span>:  US<br><span class="hljs-keyword">LOCATION</span>:  China<br><span class="hljs-keyword">LOCATION</span>:  Russia<br></code></pre></td></tr></table></figure> 输入为：<figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk">sent = <span class="hljs-comment">&quot;Home to the headquarters of the United Nations, New York is an important center for international diplomacy.&quot;</span><br></code></pre></td></tr></table></figure> 输出为： <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;Home&#x27;</span>, <span class="hljs-string">&#x27;to&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;headquarters&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;United&#x27;</span>, <span class="hljs-string">&#x27;Nations&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;New&#x27;</span>, <span class="hljs-string">&#x27;York&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;an&#x27;</span>, <span class="hljs-string">&#x27;important&#x27;</span>, <span class="hljs-string">&#x27;center&#x27;</span>, <span class="hljs-string">&#x27;for&#x27;</span>, <span class="hljs-string">&#x27;international&#x27;</span>, <span class="hljs-string">&#x27;diplomacy&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>ORGANIZATION:  United Nations<br>LOCATION:  New York<br></code></pre></td></tr></table></figure></p><p>输入为： <figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">sent</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;The United States is a founding member of the United Nations, World Bank, International Monetary Fund.&quot;</span><br></code></pre></td></tr></table></figure> 输出为: <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;The&#x27;</span>, <span class="hljs-string">&#x27;United&#x27;</span>, <span class="hljs-string">&#x27;States&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;founding&#x27;</span>, <span class="hljs-string">&#x27;member&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;United&#x27;</span>, <span class="hljs-string">&#x27;Nations&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;World&#x27;</span>, <span class="hljs-string">&#x27;Bank&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;International&#x27;</span>, <span class="hljs-string">&#x27;Monetary&#x27;</span>, <span class="hljs-string">&#x27;Fund&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>LOCATION:  United States<br>ORGANIZATION:  United Nations<br>ORGANIZATION:  World Bank<br>ORGANIZATION:  International Monetary Fund<br></code></pre></td></tr></table></figure></p><p>这三个例子识别全部正确。</p><h3 id="总结">总结</h3><p>到这儿，笔者的这个项目就差不多了。我们有必要对这个项目做个总结。</p><p>首先是这个项目的优点。它的优点在于能够让你一步步地实现NER，而且除了语料库，你基本熟悉了如何创建一个识别NER系统的步骤，同时，对深度学习模型及其应用也有了深刻理解。因此，好处是显而易见的。当然，在实际工作中，语料库的整理才是最耗费时间的，能够占到90%或者更多的时间，因此，有一个好的语料库你才能展开工作。</p><p>接着讲讲这个项目的缺点。第一个，是语料库不够大，当然，约14000条句子也够了，但本项目没有对句子进行文本预处理，所以，有些单词的变形可能无法进入词汇表。第二个，缺少对新词的处理，一旦句子中出现一个新的单词，这个模型便无法处理，这是后期需要完善的地方。第三个，句子的填充长度为60，如果输入的句子长度大于60，则后面的部分将无法有效识别。</p><p>因此，后续还有更多的工作需要去做，当然，做一个中文NER也是可以考虑的。</p><p>本项目已上传Github,地址为 <ahref="https://github.com/percent4/DL_4_NER">https://github.com/percent4/DL_4_NER</a>。：欢迎大家参考~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p><h3 id="参考文献">参考文献</h3><ol type="1"><li>BOOK： Applied Natural Language Processing with Python， TawehBeysolow II</li><li>WEBSITE：https://github.com/Apress/applied-natural-language-processing-w-python</li><li>WEBSITE: NLP入门（四）命名实体识别（NER）:https://www.jianshu.com/p/16e1f6a7aaef</li></ol>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>NER</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（四）命名实体识别（NER）</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文将会简单介绍自然语言处理（NLP）中的命名实体识别（NER）。</p><p>命名实体识别（Named EntityRecognition，简称NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。</p><p>举个简单的例子，在句子“小明早上8点去学校上课。”中，对其进行命名实体识别，应该能提取信息</p><blockquote><p>人名：小明，时间：早上8点，地点：学校。</p></blockquote><p>本文将会介绍几个工具用来进行命名实体识别，后续有机会的话，我们将会尝试着用HMM、CRF或深度学习来实现命名实体识别。</p><p>首先我们来看一下NLTK和StanfordNLP中对命名实体识别的分类，如下图：</p><figure><img src="/img/nlp4_1.png"alt="NLTK和Stanford NLP中对命名实体识别的分类" /><figcaption aria-hidden="true">NLTK和StanfordNLP中对命名实体识别的分类</figcaption></figure><p>在上图中，LOCATION和GPE有重合。GPE通常表示地理—政治条目，比如城市，州，国家，洲等。LOCATION除了上述内容外，还能表示名山大川等。FACILITY通常表示知名的纪念碑或人工制品等。</p><p>下面介绍两个工具来进行NER的任务：NLTK和Stanford NLP。</p><p>首先是NLTK，我们的示例文档（介绍FIFA，来源于维基百科）如下：</p><blockquote><p>FIFA was founded in 1904 to oversee international competition amongthe national associations of Belgium, Denmark, France, Germany, theNetherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich,its membership now comprises 211 national associations. Member countriesmust each also be members of one of the six regional confederations intowhich the world is divided: Africa, Asia, Europe, North &amp; CentralAmerica and the Caribbean, Oceania, and South America.</p></blockquote><p>实现NER的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> nltk<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_document</span>(<span class="hljs-params">document</span>):<br>   document = re.sub(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, document)<br>   <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(document, <span class="hljs-built_in">str</span>):<br>       document = document<br>   <span class="hljs-keyword">else</span>:<br>       <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Document is not string!&#x27;</span>)<br>   document = document.strip()<br>   sentences = nltk.sent_tokenize(document)<br>   sentences = [sentence.strip() <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences]<br>   <span class="hljs-keyword">return</span> sentences<br><br><span class="hljs-comment"># sample document</span><br>text = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">FIFA was founded in 1904 to oversee international competition among the national associations of Belgium, </span><br><span class="hljs-string">Denmark, France, Germany, the Netherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich, its </span><br><span class="hljs-string">membership now comprises 211 national associations. Member countries must each also be members of one of </span><br><span class="hljs-string">the six regional confederations into which the world is divided: Africa, Asia, Europe, North &amp; Central America </span><br><span class="hljs-string">and the Caribbean, Oceania, and South America.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># tokenize sentences</span><br>sentences = parse_document(text)<br>tokenized_sentences = [nltk.word_tokenize(sentence) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences]<br><span class="hljs-comment"># tag sentences and use nltk&#x27;s Named Entity Chunker</span><br>tagged_sentences = [nltk.pos_tag(sentence) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> tokenized_sentences]<br>ne_chunked_sents = [nltk.ne_chunk(tagged) <span class="hljs-keyword">for</span> tagged <span class="hljs-keyword">in</span> tagged_sentences]<br><span class="hljs-comment"># extract all named entities</span><br>named_entities = []<br><span class="hljs-keyword">for</span> ne_tagged_sentence <span class="hljs-keyword">in</span> ne_chunked_sents:<br>   <span class="hljs-keyword">for</span> tagged_tree <span class="hljs-keyword">in</span> ne_tagged_sentence:<br>       <span class="hljs-comment"># extract only chunks having NE labels</span><br>       <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(tagged_tree, <span class="hljs-string">&#x27;label&#x27;</span>):<br>           entity_name = <span class="hljs-string">&#x27; &#x27;</span>.join(c[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> tagged_tree.leaves()) <span class="hljs-comment">#get NE name</span><br>           entity_type = tagged_tree.label() <span class="hljs-comment"># get NE category</span><br>           named_entities.append((entity_name, entity_type))<br>           <span class="hljs-comment"># get unique named entities</span><br>           named_entities = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(named_entities))<br><br><span class="hljs-comment"># store named entities in a data frame</span><br>entity_frame = pd.DataFrame(named_entities, columns=[<span class="hljs-string">&#x27;Entity Name&#x27;</span>, <span class="hljs-string">&#x27;Entity Type&#x27;</span>])<br><span class="hljs-comment"># display results</span><br><span class="hljs-built_in">print</span>(entity_frame)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">        Entity Name   Entity Type<br><span class="hljs-number">0</span>              FIFA  <span class="hljs-keyword">ORGANIZATION</span><br><span class="hljs-keyword"></span><span class="hljs-number">1</span>   Central America  <span class="hljs-keyword">ORGANIZATION</span><br><span class="hljs-keyword"></span><span class="hljs-number">2</span>           <span class="hljs-keyword">Belgium </span>          GPE<br><span class="hljs-number">3</span>         Caribbean      LOCATION<br><span class="hljs-number">4</span>              Asia           GPE<br><span class="hljs-number">5</span>            France           GPE<br><span class="hljs-number">6</span>           Oceania           GPE<br><span class="hljs-number">7</span>           Germany           GPE<br><span class="hljs-number">8</span>     South America           GPE<br><span class="hljs-number">9</span>           Denmark           GPE<br><span class="hljs-number">10</span>           Zürich           GPE<br><span class="hljs-number">11</span>           Africa        PERSON<br><span class="hljs-number">12</span>           <span class="hljs-keyword">Sweden </span>          GPE<br><span class="hljs-number">13</span>      Netherlands           GPE<br><span class="hljs-number">14</span>            Spain           GPE<br><span class="hljs-number">15</span>      <span class="hljs-keyword">Switzerland </span>          GPE<br><span class="hljs-number">16</span>            <span class="hljs-keyword">North </span>          GPE<br><span class="hljs-number">17</span>           Europe           GPE<br></code></pre></td></tr></table></figure><p>可以看到，NLTK中的NER任务大体上完成得还是不错的，能够识别FIFA为组织（ORGANIZATION），Belgium,Asia为GPE,但是也有一些不太如人意的地方，比如，它将CentralAmerica识别为ORGANIZATION，而实际上它应该为GPE；将Africa识别为PERSON，实际上应该为GPE。</p><p>接下来，我们尝试着用StanfordNLP工具。关于该工具，我们主要使用Stanford NER标注工具。在使用这个工具之前，你需要在自己的电脑上安装Java（一般是JDK），并将Java添加到系统路径中，同时下载英语NER的文件包：stanford-ner-2018-10-16.zip（大小为172MB），下载地址为：https://nlp.stanford.edu/software/CRF-NER.shtml。以笔者的电脑为例，Java所在的路径为：C:Files.0_161.exe， 下载StanfordNER的zip文件解压后的文件夹的路径为：E://stanford-ner-2018-10-16，如下图所示：</p><p><img src="/img/nlp4_2.png" /></p><p>在classifer文件夹中有如下文件：</p><p><img src="/img/nlp4_3.png" /></p><p>它们代表的含义如下：</p><blockquote><p>3 class: Location, Person, Organization 4 class: Location, Person,Organization, Misc 7 class: Location, Person, Organization, Money,Percent, Date, Time</p></blockquote><p>可以使用Python实现Stanford NER，完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">from</span> nltk.tag <span class="hljs-keyword">import</span> StanfordNERTagger<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> nltk<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_document</span>(<span class="hljs-params">document</span>):<br>   document = re.sub(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, document)<br>   <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(document, <span class="hljs-built_in">str</span>):<br>       document = document<br>   <span class="hljs-keyword">else</span>:<br>       <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Document is not string!&#x27;</span>)<br>   document = document.strip()<br>   sentences = nltk.sent_tokenize(document)<br>   sentences = [sentence.strip() <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences]<br>   <span class="hljs-keyword">return</span> sentences<br><br><span class="hljs-comment"># sample document</span><br>text = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">FIFA was founded in 1904 to oversee international competition among the national associations of Belgium, </span><br><span class="hljs-string">Denmark, France, Germany, the Netherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich, its </span><br><span class="hljs-string">membership now comprises 211 national associations. Member countries must each also be members of one of </span><br><span class="hljs-string">the six regional confederations into which the world is divided: Africa, Asia, Europe, North &amp; Central America </span><br><span class="hljs-string">and the Caribbean, Oceania, and South America.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>sentences = parse_document(text)<br>tokenized_sentences = [nltk.word_tokenize(sentence) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences]<br><br><span class="hljs-comment"># set java path in environment variables</span><br>java_path = <span class="hljs-string">r&#x27;C:\Program Files\Java\jdk1.8.0_161\bin\java.exe&#x27;</span><br>os.environ[<span class="hljs-string">&#x27;JAVAHOME&#x27;</span>] = java_path<br><span class="hljs-comment"># load stanford NER</span><br>sn = StanfordNERTagger(<span class="hljs-string">&#x27;E://stanford-ner-2018-10-16/classifiers/english.muc.7class.distsim.crf.ser.gz&#x27;</span>,<br>                       path_to_jar=<span class="hljs-string">&#x27;E://stanford-ner-2018-10-16/stanford-ner.jar&#x27;</span>)<br><br><span class="hljs-comment"># tag sentences</span><br>ne_annotated_sentences = [sn.tag(sent) <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> tokenized_sentences]<br><span class="hljs-comment"># extract named entities</span><br>named_entities = []<br><span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> ne_annotated_sentences:<br>   temp_entity_name = <span class="hljs-string">&#x27;&#x27;</span><br>   temp_named_entity = <span class="hljs-literal">None</span><br>   <span class="hljs-keyword">for</span> term, tag <span class="hljs-keyword">in</span> sentence:<br>       <span class="hljs-comment"># get terms with NE tags</span><br>       <span class="hljs-keyword">if</span> tag != <span class="hljs-string">&#x27;O&#x27;</span>:<br>           temp_entity_name = <span class="hljs-string">&#x27; &#x27;</span>.join([temp_entity_name, term]).strip() <span class="hljs-comment">#get NE name</span><br>           temp_named_entity = (temp_entity_name, tag) <span class="hljs-comment"># get NE and its category</span><br>       <span class="hljs-keyword">else</span>:<br>           <span class="hljs-keyword">if</span> temp_named_entity:<br>               named_entities.append(temp_named_entity)<br>               temp_entity_name = <span class="hljs-string">&#x27;&#x27;</span><br>               temp_named_entity = <span class="hljs-literal">None</span><br><br><span class="hljs-comment"># get unique named entities</span><br>named_entities = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(named_entities))<br><span class="hljs-comment"># store named entities in a data frame</span><br>entity_frame = pd.DataFrame(named_entities, columns=[<span class="hljs-string">&#x27;Entity Name&#x27;</span>, <span class="hljs-string">&#x27;Entity Type&#x27;</span>])<br><span class="hljs-comment"># display results</span><br><span class="hljs-built_in">print</span>(entity_frame)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">                Entity Name   Entity <span class="hljs-keyword">Type</span><br><span class="hljs-number">0</span>                      <span class="hljs-number">1904</span>          <span class="hljs-keyword">DATE</span><br><span class="hljs-number">1</span>                   Denmark      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">2</span>                     Spain      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">3</span>   North &amp; Central America  ORGANIZATION<br><span class="hljs-number">4</span>             South America      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">5</span>                   Belgium      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">6</span>                    Zürich      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">7</span>           the Netherlands      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">8</span>                    France      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">9</span>                 Caribbean      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">10</span>                   Sweden      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">11</span>                  Oceania      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">12</span>                     Asia      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">13</span>                     FIFA  ORGANIZATION<br><span class="hljs-number">14</span>                   Europe      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">15</span>                   Africa      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">16</span>              Switzerland      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">17</span>                  Germany      LOCATION<br></code></pre></td></tr></table></figure><p>可以看到，在StanfordNER的帮助下，NER的实现效果较好，将Africa识别为LOCATION，将1904识别为时间（这在NLTK中没有识别出来），但还是对North&amp; Central America识别有误，将其识别为ORGANIZATION。</p><p>值得注意的是，并不是说Stanford NER一定会比NLTKNER的效果好，两者针对的对象，预料，算法可能有差异，因此，需要根据自己的需求决定使用什么工具。</p><p>本次分享到此结束，以后有机会的话，将会尝试着用HMM、CRF或深度学习来实现命名实体识别。</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>NER</tag>
      
      <tag>NLP工具</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（三）词形还原（Lemmatization）</title>
    <link href="/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F%EF%BC%88Lemmatization%EF%BC%89/"/>
    <url>/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F%EF%BC%88Lemmatization%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>词形还原（Lemmatization）是文本预处理中的重要部分，与词干提取（stemming）很相似。</p><p>简单说来，词形还原就是去掉单词的词缀，提取单词的主干部分，通常提取后的单词会是字典中的单词，不同于词干提取（stemming），提取后的单词不一定会出现在单词中。比如，单词“cars”词形还原后的单词为“car”，单词“ate”词形还原后的单词为“eat”。</p><p>在Python的nltk模块中，使用WordNet为我们提供了稳健的词形还原的函数。如以下示例Python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.stem <span class="hljs-keyword">import</span> WordNetLemmatizer<br><br>wnl = WordNetLemmatizer()<br><span class="hljs-comment"># lemmatize nouns</span><br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;cars&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>))<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;men&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>))<br><br><span class="hljs-comment"># lemmatize verbs</span><br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;running&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>))<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;ate&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>))<br><br><span class="hljs-comment"># lemmatize adjectives</span><br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;saddest&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>))<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;fancier&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>car men run eat sad fancy</p></blockquote><p>在以上代码中，wnl.lemmatize()函数可以进行词形还原，第一个参数为单词，第二个参数为该单词的词性，如名词，动词，形容词等，返回的结果为输入单词的词形还原后的结果。</p><p>词形还原一般是简单的，但具体我们在使用时，指定单词的词性很重要，不然词形还原可能效果不好，如以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.stem <span class="hljs-keyword">import</span> WordNetLemmatizer<br><br>wnl = WordNetLemmatizer()<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;ate&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>))<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;fancier&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>ate fancier</p></blockquote><p>那么，如何获取单词的词性呢？在NLP中，使用Parts ofspeech（POS）技术实现。在nltk中，可以使用nltk.pos_tag()获取单词在句子中的词性，如以下Python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">sentence = <span class="hljs-string">&#x27;The brown fox is quick and he is jumping over the lazy dog&#x27;</span><br><span class="hljs-keyword">import</span> nltk<br>tokens = nltk.word_tokenize(sentence)<br>tagged_sent = nltk.pos_tag(tokens)<br><span class="hljs-built_in">print</span>(tagged_sent)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'),('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'),('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'),('dog', 'NN')]</p></blockquote><p>关于上述词性的说明，可以参考下表：</p><figure><img src="/img/nlp3_1.webp" alt="词性说明表1" /><figcaption aria-hidden="true">词性说明表1</figcaption></figure><figure><img src="/img/nlp3_2.webp" alt="词性说明表2" /><figcaption aria-hidden="true">词性说明表2</figcaption></figure><p>OK，知道了获取单词在句子中的词性，再结合词形还原，就能很好地完成词形还原功能。示例的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize, pos_tag<br><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> wordnet<br><span class="hljs-keyword">from</span> nltk.stem <span class="hljs-keyword">import</span> WordNetLemmatizer<br><br><span class="hljs-comment"># 获取单词的词性</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_wordnet_pos</span>(<span class="hljs-params">tag</span>):<br>    <span class="hljs-keyword">if</span> tag.startswith(<span class="hljs-string">&#x27;J&#x27;</span>):<br>        <span class="hljs-keyword">return</span> wordnet.ADJ<br>    <span class="hljs-keyword">elif</span> tag.startswith(<span class="hljs-string">&#x27;V&#x27;</span>):<br>        <span class="hljs-keyword">return</span> wordnet.VERB<br>    <span class="hljs-keyword">elif</span> tag.startswith(<span class="hljs-string">&#x27;N&#x27;</span>):<br>        <span class="hljs-keyword">return</span> wordnet.NOUN<br>    <span class="hljs-keyword">elif</span> tag.startswith(<span class="hljs-string">&#x27;R&#x27;</span>):<br>        <span class="hljs-keyword">return</span> wordnet.ADV<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>sentence = <span class="hljs-string">&#x27;football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal.&#x27;</span><br>tokens = word_tokenize(sentence)  <span class="hljs-comment"># 分词</span><br>tagged_sent = pos_tag(tokens)     <span class="hljs-comment"># 获取单词词性</span><br><br>wnl = WordNetLemmatizer()<br>lemmas_sent = []<br><span class="hljs-keyword">for</span> tag <span class="hljs-keyword">in</span> tagged_sent:<br>    wordnet_pos = get_wordnet_pos(tag[<span class="hljs-number">1</span>]) <span class="hljs-keyword">or</span> wordnet.NOUN<br>    lemmas_sent.append(wnl.lemmatize(tag[<span class="hljs-number">0</span>], pos=wordnet_pos)) <span class="hljs-comment"># 词形还原</span><br><br><span class="hljs-built_in">print</span>(lemmas_sent)<br><br></code></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>['football', 'be', 'a', 'family', 'of', 'team', 'sport', 'that','involve', ',', 'to', 'vary', 'degree', ',', 'kick', 'a', 'ball', 'to','score', 'a', 'goal', '.']</p></blockquote><p>输出的结果就是对句子中的单词进行词形还原后的结果。本次分享到此结束，欢迎大家交流~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>词形还原</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（二）探究TF-IDF的原理</title>
    <link href="/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%8C%EF%BC%89%E6%8E%A2%E7%A9%B6TF-IDF%E7%9A%84%E5%8E%9F%E7%90%86/"/>
    <url>/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%8C%EF%BC%89%E6%8E%A2%E7%A9%B6TF-IDF%E7%9A%84%E5%8E%9F%E7%90%86/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="tf-idf介绍">TF-IDF介绍</h3><p>TF-IDF是NLP中一种常用的统计方法，用以评估一个字词对于一个文件集或一个语料库中的其中一份文件的重要程度，通常用于提取文本的特征，即关键词。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。</p><p>在NLP中，TF-IDF的计算公式如下：</p><p><span class="math display">\[tfidf = tf*idf.\]</span></p><p>其中，tf是词频(Term Frequency)，idf为逆向文件频率(Inverse DocumentFrequency)。</p><p>tf为词频，即一个词语在文档中的出现频率，假设一个词语在整个文档中出现了i次，而整个文档有N个词语，则tf的值为i/N.</p><p>idf为逆向文件频率，假设整个文档有n篇文章，而一个词语在k篇文章中出现，则idf值为</p><p><span class="math display">\[idf=\log_{2}(\frac{n}{k}).\]</span></p><p>当然，不同地方的idf值计算公式会有稍微的不同。比如有些地方会在分母的k上加1，防止分母为0，还有些地方会让分子，分母都加上1，这是smoothing技巧。在本文中，还是采用最原始的idf值计算公式，因为这与gensim里面的计算公式一致。</p><p>假设整个文档有D篇文章，则单词i在第j篇文章中的tfidf值为</p><figure><img src="/img/nlp2_1.webp" alt="gensim中tfidf的计算公式" /><figcaption aria-hidden="true">gensim中tfidf的计算公式</figcaption></figure><p>以上就是TF-IDF的计算方法。</p><h3 id="文本介绍及预处理">文本介绍及预处理</h3><p>我们将采用以下三个示例文本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">text1 =<span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal. </span><br><span class="hljs-string">Unqualified, the word football is understood to refer to whichever form of football is the most popular </span><br><span class="hljs-string">in the regional context in which the word appears. Sports commonly called football in certain places </span><br><span class="hljs-string">include association football (known as soccer in some countries); gridiron football (specifically American </span><br><span class="hljs-string">football or Canadian football); Australian rules football; rugby football (either rugby league or rugby union); </span><br><span class="hljs-string">and Gaelic football. These different variations of football are known as football codes.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>text2 = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Basketball is a team sport in which two teams of five players, opposing one another on a rectangular court, </span><br><span class="hljs-string">compete with the primary objective of shooting a basketball (approximately 9.4 inches (24 cm) in diameter) </span><br><span class="hljs-string">through the defender&#x27;s hoop (a basket 18 inches (46 cm) in diameter mounted 10 feet (3.048 m) high to a backboard </span><br><span class="hljs-string">at each end of the court) while preventing the opposing team from shooting through their own hoop. A field goal is </span><br><span class="hljs-string">worth two points, unless made from behind the three-point line, when it is worth three. After a foul, timed play stops </span><br><span class="hljs-string">and the player fouled or designated to shoot a technical foul is given one or more one-point free throws. The team with </span><br><span class="hljs-string">the most points at the end of the game wins, but if regulation play expires with the score tied, an additional period </span><br><span class="hljs-string">of play (overtime) is mandated.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>text3 = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Volleyball, game played by two teams, usually of six players on a side, in which the players use their hands to bat a </span><br><span class="hljs-string">ball back and forth over a high net, trying to make the ball touch the court within the opponents’ playing area before </span><br><span class="hljs-string">it can be returned. To prevent this a player on the opposing team bats the ball up and toward a teammate before it touches </span><br><span class="hljs-string">the court surface—that teammate may then volley it back across the net or bat it to a third teammate who volleys it across </span><br><span class="hljs-string">the net. A team is allowed only three touches of the ball before it must be returned over the net.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure><p>这三篇文章分别是关于足球，篮球，排球的介绍，它们组成一篇文档。</p><p>接下来是文本的预处理部分。</p><p>首先是对文本去掉换行符，然后是分句，分词，再去掉其中的标点，完整的Python代码如下，输入的参数为文章text:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> nltk<br><span class="hljs-keyword">import</span> string<br><br><span class="hljs-comment"># 文本预处理</span><br><span class="hljs-comment"># 函数：text文件分句，分词，并去掉标点</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_tokens</span>(<span class="hljs-params">text</span>):<br>    text = text.replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>    sents = nltk.sent_tokenize(text)  <span class="hljs-comment"># 分句</span><br>    tokens = []<br>    <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> nltk.word_tokenize(sent):  <span class="hljs-comment"># 分词</span><br>            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> string.punctuation: <span class="hljs-comment"># 去掉标点</span><br>                tokens.append(word)<br>    <span class="hljs-keyword">return</span> tokens<br></code></pre></td></tr></table></figure><p>接着，去掉文章中的通用词（stopwords），然后统计每个单词的出现次数，完整的Python代码如下，输入的参数为文章text:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords     <span class="hljs-comment">#停用词</span><br><br><span class="hljs-comment"># 对原始的text文件去掉停用词</span><br><span class="hljs-comment"># 生成count字典，即每个单词的出现次数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_count</span>(<span class="hljs-params">text</span>):<br>    tokens = get_tokens(text)<br>    filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> w <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)]    <span class="hljs-comment">#去掉停用词</span><br>    count = Counter(filtered)<br>    <span class="hljs-keyword">return</span> count<br></code></pre></td></tr></table></figure><p>以text3为例，生成的count字典如下：</p><blockquote><p>Counter({'ball': 4, 'net': 4, 'teammate': 3, 'returned': 2, 'bat': 2,'court': 2, 'team': 2, 'across': 2, 'touches': 2, 'back': 2, 'players':2, 'touch': 1, 'must': 1, 'usually': 1, 'side': 1, 'player': 1, 'area':1, 'Volleyball': 1, 'hands': 1, 'may': 1, 'toward': 1, 'A': 1, 'third':1, 'two': 1, 'six': 1, 'opposing': 1, 'within': 1, 'prevent': 1,'allowed': 1, '’': 1, 'playing': 1, 'played': 1, 'volley': 1,'surface—that': 1, 'volleys': 1, 'opponents': 1, 'use': 1, 'high': 1,'teams': 1, 'bats': 1, 'To': 1, 'game': 1, 'make': 1, 'forth': 1,'three': 1, 'trying': 1})</p></blockquote><h3 id="gensim中的tf-idf">Gensim中的TF-IDF</h3><p>对文本进行预处理后，对于以上三个示例文本，我们都会得到一个count字典，里面是每个文本中单词的出现次数。下面，我们将用gensim中的已实现的TF-IDF模型，来输出每篇文章中TF-IDF排名前三的单词及它们的tfidf值，完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords     <span class="hljs-comment">#停用词</span><br><span class="hljs-keyword">from</span> gensim <span class="hljs-keyword">import</span> corpora, models, matutils<br><br><span class="hljs-comment">#training by gensim&#x27;s Ifidf Model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_words</span>(<span class="hljs-params">text</span>):<br>    tokens = get_tokens(text)<br>    filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> w <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)]<br>    <span class="hljs-keyword">return</span> filtered<br><br><span class="hljs-comment"># get text</span><br>count1, count2, count3 = get_words(text1), get_words(text2), get_words(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-comment"># training by TfidfModel in gensim</span><br>dictionary = corpora.Dictionary(countlist)<br>new_dict = &#123;v:k <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> dictionary.token2id.items()&#125;<br>corpus2 = [dictionary.doc2bow(count) <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> countlist]<br>tfidf2 = models.TfidfModel(corpus2)<br>corpus_tfidf = tfidf2[corpus2]<br><br><span class="hljs-comment"># output</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nTraining by gensim Tfidf Model.......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, doc <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(corpus_tfidf):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    sorted_words = <span class="hljs-built_in">sorted</span>(doc, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    <span class="hljs-keyword">for</span> num, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(new_dict[num], <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Training</span> by gensim Tfidf Model.......<br><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">1</span><br>    <span class="hljs-attribute">Word</span>: football, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">84766</span><br>    <span class="hljs-attribute">Word</span>: rugby, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">21192</span><br>    <span class="hljs-attribute">Word</span>: known, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">14128</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">2</span><br>    <span class="hljs-attribute">Word</span>: play, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">29872</span><br>    <span class="hljs-attribute">Word</span>: cm, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">19915</span><br>    <span class="hljs-attribute">Word</span>: diameter, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">19915</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">3</span><br>    <span class="hljs-attribute">Word</span>: net, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">45775</span><br>    <span class="hljs-attribute">Word</span>: teammate, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">34331</span><br>    <span class="hljs-attribute">Word</span>: across, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">22888</span><br></code></pre></td></tr></table></figure><p>输出的结果还是比较符合我们的预期的，比如关于足球的文章中提取了football,rugby关键词，关于篮球的文章中提取了plat,cm关键词，关于排球的文章中提取了net, teammate关键词。</p><h3 id="自己动手实践tf-idf模型">自己动手实践TF-IDF模型</h3><p>有了以上我们对TF-IDF模型的理解，其实我们自己也可以动手实践一把，这是学习算法的最佳方式！</p><p>以下是笔者实践TF-IDF的代码（接文本预处理代码）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><br><span class="hljs-comment"># 计算tf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tf</span>(<span class="hljs-params">word, count</span>):<br>    <span class="hljs-keyword">return</span> count[word] / <span class="hljs-built_in">sum</span>(count.values())<br><span class="hljs-comment"># 计算count_list有多少个文件包含word</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">n_containing</span>(<span class="hljs-params">word, count_list</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> count_list <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> count)<br><br><span class="hljs-comment"># 计算idf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">idf</span>(<span class="hljs-params">word, count_list</span>):<br>    <span class="hljs-keyword">return</span> math.log2(<span class="hljs-built_in">len</span>(count_list) / (n_containing(word, count_list)))    <span class="hljs-comment">#对数以2为底</span><br><span class="hljs-comment"># 计算tf-idf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tfidf</span>(<span class="hljs-params">word, count, count_list</span>):<br>    <span class="hljs-keyword">return</span> tf(word, count) * idf(word, count_list)<br><br><span class="hljs-comment"># TF-IDF测试</span><br>count1, count2, count3 = make_count(text1), make_count(text2), make_count(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training by original algorithm......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, count <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(countlist):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    scores = &#123;word: tfidf(word, count, countlist) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> count&#125;<br>    sorted_words = <span class="hljs-built_in">sorted</span>(scores.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    <span class="hljs-comment"># sorted_words = matutils.unitvec(sorted_words)</span><br>    <span class="hljs-keyword">for</span> word, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(word, <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Training</span> by original algorithm......<br><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">1</span><br>    <span class="hljs-attribute">Word</span>: football, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">30677</span><br>    <span class="hljs-attribute">Word</span>: rugby, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">07669</span><br>    <span class="hljs-attribute">Word</span>: known, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">05113</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">2</span><br>    <span class="hljs-attribute">Word</span>: play, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">05283</span><br>    <span class="hljs-attribute">Word</span>: inches, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">03522</span><br>    <span class="hljs-attribute">Word</span>: worth, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">03522</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">3</span><br>    <span class="hljs-attribute">Word</span>: net, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">10226</span><br>    <span class="hljs-attribute">Word</span>: teammate, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">07669</span><br>    <span class="hljs-attribute">Word</span>: across, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">05113</span><br></code></pre></td></tr></table></figure><p>可以看到，笔者自己动手实践的TF-IDF模型提取的关键词与gensim一致，至于篮球中为什么后两个单词不一致，是因为这些单词的tfidf一样，随机选择的结果不同而已。但是有一个问题，那就是计算得到的tfidf值不一样，这是什么原因呢？</p><p>查阅gensim中计算tf-idf值的源代码（https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/tfidfmodel.py）：</p><figure><img src="/img/nlp2_2.webp" alt="TfidfModel类的参数" /><figcaption aria-hidden="true">TfidfModel类的参数</figcaption></figure><figure><img src="/img/nlp2_3.webp" alt="normalize参数的说明" /><figcaption aria-hidden="true">normalize参数的说明</figcaption></figure><p>也就是说，gensim对得到的tf-idf向量做了规范化（normalize），将其转化为单位向量。因此，我们需要在刚才的代码中加入规范化这一步，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 对向量做规范化, normalize</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">unitvec</span>(<span class="hljs-params">sorted_words</span>):<br>    lst = [item[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sorted_words]<br>    L2Norm = math.sqrt(<span class="hljs-built_in">sum</span>(np.array(lst)*np.array(lst)))<br>    unit_vector = [(item[<span class="hljs-number">0</span>], item[<span class="hljs-number">1</span>]/L2Norm) <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sorted_words]<br>    <span class="hljs-keyword">return</span> unit_vector<br><br><span class="hljs-comment"># TF-IDF测试</span><br>count1, count2, count3 = make_count(text1), make_count(text2), make_count(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training by original algorithm......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, count <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(countlist):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    scores = &#123;word: tfidf(word, count, countlist) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> count&#125;<br>    sorted_words = <span class="hljs-built_in">sorted</span>(scores.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    sorted_words = unitvec(sorted_words)   <span class="hljs-comment"># normalize</span><br>    <span class="hljs-keyword">for</span> word, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(word, <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Training</span> by original algorithm......<br><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">1</span><br>    <span class="hljs-attribute">Word</span>: football, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">84766</span><br>    <span class="hljs-attribute">Word</span>: rugby, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">21192</span><br>    <span class="hljs-attribute">Word</span>: known, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">14128</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">2</span><br>    <span class="hljs-attribute">Word</span>: play, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">29872</span><br>    <span class="hljs-attribute">Word</span>: shooting, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">19915</span><br>    <span class="hljs-attribute">Word</span>: diameter, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">19915</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">3</span><br>    <span class="hljs-attribute">Word</span>: net, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">45775</span><br>    <span class="hljs-attribute">Word</span>: teammate, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">34331</span><br>    <span class="hljs-attribute">Word</span>: back, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">22888</span><br></code></pre></td></tr></table></figure><p>现在的输出结果与gensim得到的结果一致！</p><h3 id="总结">总结</h3><p>Gensim是Python做NLP时鼎鼎大名的模块，有空还是多读读源码吧！以后，我们还会继续介绍TF-IDF在其它方面的应用，欢迎大家交流~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p><p>本文的完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> nltk<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> string<br><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords     <span class="hljs-comment">#停用词</span><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter       <span class="hljs-comment">#计数</span><br><span class="hljs-keyword">from</span> gensim <span class="hljs-keyword">import</span> corpora, models, matutils<br><br>text1 =<span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal. </span><br><span class="hljs-string">Unqualified, the word football is understood to refer to whichever form of football is the most popular </span><br><span class="hljs-string">in the regional context in which the word appears. Sports commonly called football in certain places </span><br><span class="hljs-string">include association football (known as soccer in some countries); gridiron football (specifically American </span><br><span class="hljs-string">football or Canadian football); Australian rules football; rugby football (either rugby league or rugby union); </span><br><span class="hljs-string">and Gaelic football. These different variations of football are known as football codes.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>text2 = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Basketball is a team sport in which two teams of five players, opposing one another on a rectangular court, </span><br><span class="hljs-string">compete with the primary objective of shooting a basketball (approximately 9.4 inches (24 cm) in diameter) </span><br><span class="hljs-string">through the defender&#x27;s hoop (a basket 18 inches (46 cm) in diameter mounted 10 feet (3.048 m) high to a backboard </span><br><span class="hljs-string">at each end of the court) while preventing the opposing team from shooting through their own hoop. A field goal is </span><br><span class="hljs-string">worth two points, unless made from behind the three-point line, when it is worth three. After a foul, timed play stops </span><br><span class="hljs-string">and the player fouled or designated to shoot a technical foul is given one or more one-point free throws. The team with </span><br><span class="hljs-string">the most points at the end of the game wins, but if regulation play expires with the score tied, an additional period </span><br><span class="hljs-string">of play (overtime) is mandated.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>text3 = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Volleyball, game played by two teams, usually of six players on a side, in which the players use their hands to bat a </span><br><span class="hljs-string">ball back and forth over a high net, trying to make the ball touch the court within the opponents’ playing area before </span><br><span class="hljs-string">it can be returned. To prevent this a player on the opposing team bats the ball up and toward a teammate before it touches </span><br><span class="hljs-string">the court surface—that teammate may then volley it back across the net or bat it to a third teammate who volleys it across </span><br><span class="hljs-string">the net. A team is allowed only three touches of the ball before it must be returned over the net.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># 文本预处理</span><br><span class="hljs-comment"># 函数：text文件分句，分词，并去掉标点</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_tokens</span>(<span class="hljs-params">text</span>):<br>    text = text.replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>    sents = nltk.sent_tokenize(text)  <span class="hljs-comment"># 分句</span><br>    tokens = []<br>    <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> nltk.word_tokenize(sent):  <span class="hljs-comment"># 分词</span><br>            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> string.punctuation: <span class="hljs-comment"># 去掉标点</span><br>                tokens.append(word)<br>    <span class="hljs-keyword">return</span> tokens<br><br><span class="hljs-comment"># 对原始的text文件去掉停用词</span><br><span class="hljs-comment"># 生成count字典，即每个单词的出现次数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_count</span>(<span class="hljs-params">text</span>):<br>    tokens = get_tokens(text)<br>    filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> w <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)]    <span class="hljs-comment">#去掉停用词</span><br>    count = Counter(filtered)<br>    <span class="hljs-keyword">return</span> count<br><br><span class="hljs-comment"># 计算tf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tf</span>(<span class="hljs-params">word, count</span>):<br>    <span class="hljs-keyword">return</span> count[word] / <span class="hljs-built_in">sum</span>(count.values())<br><span class="hljs-comment"># 计算count_list有多少个文件包含word</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">n_containing</span>(<span class="hljs-params">word, count_list</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> count_list <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> count)<br><br><span class="hljs-comment"># 计算idf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">idf</span>(<span class="hljs-params">word, count_list</span>):<br>    <span class="hljs-keyword">return</span> math.log2(<span class="hljs-built_in">len</span>(count_list) / (n_containing(word, count_list)))    <span class="hljs-comment">#对数以2为底</span><br><span class="hljs-comment"># 计算tf-idf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tfidf</span>(<span class="hljs-params">word, count, count_list</span>):<br>    <span class="hljs-keyword">return</span> tf(word, count) * idf(word, count_list)<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 对向量做规范化, normalize</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">unitvec</span>(<span class="hljs-params">sorted_words</span>):<br>    lst = [item[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sorted_words]<br>    L2Norm = math.sqrt(<span class="hljs-built_in">sum</span>(np.array(lst)*np.array(lst)))<br>    unit_vector = [(item[<span class="hljs-number">0</span>], item[<span class="hljs-number">1</span>]/L2Norm) <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sorted_words]<br>    <span class="hljs-keyword">return</span> unit_vector<br><br><span class="hljs-comment"># TF-IDF测试</span><br>count1, count2, count3 = make_count(text1), make_count(text2), make_count(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training by original algorithm......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, count <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(countlist):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    scores = &#123;word: tfidf(word, count, countlist) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> count&#125;<br>    sorted_words = <span class="hljs-built_in">sorted</span>(scores.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    sorted_words = unitvec(sorted_words)   <span class="hljs-comment"># normalize</span><br>    <span class="hljs-keyword">for</span> word, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(word, <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br><br><span class="hljs-comment">#training by gensim&#x27;s Ifidf Model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_words</span>(<span class="hljs-params">text</span>):<br>    tokens = get_tokens(text)<br>    filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> w <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)]<br>    <span class="hljs-keyword">return</span> filtered<br><br><span class="hljs-comment"># get text</span><br>count1, count2, count3 = get_words(text1), get_words(text2), get_words(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-comment"># training by TfidfModel in gensim</span><br>dictionary = corpora.Dictionary(countlist)<br>new_dict = &#123;v:k <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> dictionary.token2id.items()&#125;<br>corpus2 = [dictionary.doc2bow(count) <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> countlist]<br>tfidf2 = models.TfidfModel(corpus2)<br>corpus_tfidf = tfidf2[corpus2]<br><br><span class="hljs-comment"># output</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nTraining by gensim Tfidf Model.......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, doc <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(corpus_tfidf):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    sorted_words = <span class="hljs-built_in">sorted</span>(doc, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    <span class="hljs-keyword">for</span> num, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(new_dict[num], <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br>        <br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">输出结果：</span><br><span class="hljs-string"></span><br><span class="hljs-string">Training by original algorithm......</span><br><span class="hljs-string"></span><br><span class="hljs-string">Top words in document 1</span><br><span class="hljs-string">    Word: football, TF-IDF: 0.84766</span><br><span class="hljs-string">    Word: rugby, TF-IDF: 0.21192</span><br><span class="hljs-string">    Word: word, TF-IDF: 0.14128</span><br><span class="hljs-string">Top words in document 2</span><br><span class="hljs-string">    Word: play, TF-IDF: 0.29872</span><br><span class="hljs-string">    Word: inches, TF-IDF: 0.19915</span><br><span class="hljs-string">    Word: points, TF-IDF: 0.19915</span><br><span class="hljs-string">Top words in document 3</span><br><span class="hljs-string">    Word: net, TF-IDF: 0.45775</span><br><span class="hljs-string">    Word: teammate, TF-IDF: 0.34331</span><br><span class="hljs-string">    Word: bat, TF-IDF: 0.22888</span><br><span class="hljs-string"></span><br><span class="hljs-string">Training by gensim Tfidf Model.......</span><br><span class="hljs-string"></span><br><span class="hljs-string">Top words in document 1</span><br><span class="hljs-string">    Word: football, TF-IDF: 0.84766</span><br><span class="hljs-string">    Word: rugby, TF-IDF: 0.21192</span><br><span class="hljs-string">    Word: known, TF-IDF: 0.14128</span><br><span class="hljs-string">Top words in document 2</span><br><span class="hljs-string">    Word: play, TF-IDF: 0.29872</span><br><span class="hljs-string">    Word: cm, TF-IDF: 0.19915</span><br><span class="hljs-string">    Word: diameter, TF-IDF: 0.19915</span><br><span class="hljs-string">Top words in document 3</span><br><span class="hljs-string">    Word: net, TF-IDF: 0.45775</span><br><span class="hljs-string">    Word: teammate, TF-IDF: 0.34331</span><br><span class="hljs-string">    Word: across, TF-IDF: 0.22888</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>TF-IDF</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（一）词袋模型及句子相似度</title>
    <link href="/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    <url>/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文作为笔者NLP入门系列文章第一篇，以后我们就要步入NLP时代。</p><p>本文将会介绍NLP中常见的词袋模型（Bag ofWords）以及如何利用词袋模型来计算句子间的相似度（余弦相似度，cosinesimilarity）。</p><p>首先，让我们来看一下，什么是词袋模型。我们以下面两个简单句子为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">sent1 = <span class="hljs-string">&quot;I love sky, I love sea.&quot;</span><br>sent2 = <span class="hljs-string">&quot;I like running, I love reading.&quot;</span><br></code></pre></td></tr></table></figure><p>通常，NLP无法一下子处理完整的段落或句子，因此，第一步往往是分句和分词。这里只有句子，因此我们只需要分词即可。对于英语句子，可以使用NLTK中的word_tokenize函数，对于中文句子，则可使用jieba模块。故第一步为分词，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize<br>sents = [sent1, sent2]<br>texts = [[word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_tokenize(sent)] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents]<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scheme">[[<span class="hljs-symbol">&#x27;I</span>&#x27;, <span class="hljs-symbol">&#x27;love</span>&#x27;, <span class="hljs-symbol">&#x27;sky</span>&#x27;, &#x27;,&#x27;, <span class="hljs-symbol">&#x27;I</span>&#x27;, <span class="hljs-symbol">&#x27;love</span>&#x27;, <span class="hljs-symbol">&#x27;sea</span>&#x27;, <span class="hljs-symbol">&#x27;.</span>&#x27;], [<span class="hljs-symbol">&#x27;I</span>&#x27;, <span class="hljs-symbol">&#x27;like</span>&#x27;, <span class="hljs-symbol">&#x27;running</span>&#x27;, &#x27;,&#x27;, <span class="hljs-symbol">&#x27;I</span>&#x27;, <span class="hljs-symbol">&#x27;love</span>&#x27;, <span class="hljs-symbol">&#x27;reading</span>&#x27;, <span class="hljs-symbol">&#x27;.</span>&#x27;]]<br></code></pre></td></tr></table></figure><p>分词完毕。下一步是构建语料库，即所有句子中出现的单词及标点。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">all_list = []<br><span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>    all_list += text<br>corpus = <span class="hljs-built_in">set</span>(all_list)<br><span class="hljs-built_in">print</span>(corpus)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">&#123;&#x27;love&#x27;, &#x27;running&#x27;, &#x27;reading&#x27;, &#x27;sky&#x27;, &#x27;.&#x27;, &#x27;I&#x27;, &#x27;like&#x27;, &#x27;sea&#x27;, &#x27;,&#x27;&#125;<br></code></pre></td></tr></table></figure><p>可以看到，语料库中一共是8个单词及标点。接下来，对语料库中的单词及标点建立数字映射，便于后续的句子的向量表示。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">corpus_dict = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(corpus, <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(corpus))))<br><span class="hljs-built_in">print</span>(corpus_dict)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">&#123;&#x27;running&#x27;: <span class="hljs-number">1</span>, &#x27;reading&#x27;: <span class="hljs-number">2</span>, &#x27;love&#x27;: <span class="hljs-number">0</span>, &#x27;sky&#x27;: <span class="hljs-number">3</span>, &#x27;.&#x27;: <span class="hljs-number">4</span>, &#x27;I&#x27;: <span class="hljs-number">5</span>, &#x27;like&#x27;: <span class="hljs-number">6</span>, &#x27;sea&#x27;: <span class="hljs-number">7</span>, &#x27;,&#x27;: <span class="hljs-number">8</span>&#125;<br></code></pre></td></tr></table></figure><p>虽然单词及标点并没有按照它们出现的顺序来建立数字映射，不过这并不会影响句子的向量表示及后续的句子间的相似度。</p><p>下一步，也就是词袋模型的关键一步，就是建立句子的向量表示。这个表示向量并不是简单地以单词或标点出现与否来选择0，1数字，而是把单词或标点的出现频数作为其对应的数字表示，结合刚才的语料库字典，句子的向量表示的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 建立句子的向量表示</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">vector_rep</span>(<span class="hljs-params">text, corpus_dict</span>):<br>    vec = []<br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> corpus_dict.keys():<br>        <span class="hljs-keyword">if</span> key <span class="hljs-keyword">in</span> text:<br>            vec.append((corpus_dict[key], text.count(key)))<br>        <span class="hljs-keyword">else</span>:<br>            vec.append((corpus_dict[key], <span class="hljs-number">0</span>))<br><br>    vec = <span class="hljs-built_in">sorted</span>(vec, key= <span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">0</span>])<br><br>    <span class="hljs-keyword">return</span> vec<br><br>vec1 = vector_rep(texts[<span class="hljs-number">0</span>], corpus_dict)<br>vec2 = vector_rep(texts[<span class="hljs-number">1</span>], corpus_dict)<br><span class="hljs-built_in">print</span>(vec1)<br><span class="hljs-built_in">print</span>(vec2)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scheme">[(<span class="hljs-name">0</span>, <span class="hljs-number">2</span>), (<span class="hljs-name">1</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">2</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">3</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">4</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">5</span>, <span class="hljs-number">2</span>), (<span class="hljs-name">6</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">7</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">8</span>, <span class="hljs-number">1</span>)]<br>[(<span class="hljs-name">0</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">1</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">2</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">3</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">4</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">5</span>, <span class="hljs-number">2</span>), (<span class="hljs-name">6</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">7</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">8</span>, <span class="hljs-number">1</span>)]<br></code></pre></td></tr></table></figure><p>让我们稍微逗留一会儿，来看看这个向量。在第一句中I出现了两次，在预料库字典中，I对应的数字为5，因此在第一句中5出现2次，在列表中的元组即为(5,2)，代表单词I在第一句中出现了2次。以上的输出可能并不那么直观，真实的两个句子的代表向量应为：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-string">[2, 0, 0, 1, 1, 2, 0, 1, 1]</span><br><span class="hljs-string">[1, 1, 1, 0, 1, 2, 1, 0, 1]</span><br></code></pre></td></tr></table></figure><p>OK，词袋模型到此结束。接下来，我们会利用刚才得到的词袋模型，即两个句子的向量表示，来计算相似度。</p><p>在NLP中，如果得到了两个句子的向量表示，那么，一般会选择用余弦相似度作为它们的相似度，而向量的余弦相似度即为两个向量的夹角的余弦值。其计算的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> sqrt<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">similarity_with_2_sents</span>(<span class="hljs-params">vec1, vec2</span>):<br>    inner_product = <span class="hljs-number">0</span><br>    square_length_vec1 = <span class="hljs-number">0</span><br>    square_length_vec2 = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> tup1, tup2 <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(vec1, vec2):<br>        inner_product += tup1[<span class="hljs-number">1</span>]*tup2[<span class="hljs-number">1</span>]<br>        square_length_vec1 += tup1[<span class="hljs-number">1</span>]**<span class="hljs-number">2</span><br>        square_length_vec2 += tup2[<span class="hljs-number">1</span>]**<span class="hljs-number">2</span><br><br>    <span class="hljs-keyword">return</span> (inner_product/sqrt(square_length_vec1*square_length_vec2))<br><br><br>cosine_sim = similarity_with_2_sents(vec1, vec2)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;两个句子的余弦相似度为： %.4f。&#x27;</span>%cosine_sim)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">两个句子的余弦相似度为： 0.7303。<br></code></pre></td></tr></table></figure><p>这样，我们就通过句子的词袋模型，得到了它们间的句子相似度。</p><p>当然，在实际的NLP项目中，如果需要计算两个句子的相似度，我们只需调用gensim模块即可，它是NLP的利器，能够帮助我们处理很多NLP任务。下面为用gensim计算两个句子的相似度的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">sent1 = <span class="hljs-string">&quot;I love sky, I love sea.&quot;</span><br>sent2 = <span class="hljs-string">&quot;I like running, I love reading.&quot;</span><br><br><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize<br>sents = [sent1, sent2]<br>texts = [[word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_tokenize(sent)] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents]<br><span class="hljs-built_in">print</span>(texts)<br><br><span class="hljs-keyword">from</span> gensim <span class="hljs-keyword">import</span> corpora<br><span class="hljs-keyword">from</span> gensim.similarities <span class="hljs-keyword">import</span> Similarity<br><br><span class="hljs-comment">#  语料库</span><br>dictionary = corpora.Dictionary(texts)<br><br><span class="hljs-comment"># 利用doc2bow作为词袋模型</span><br>corpus = [dictionary.doc2bow(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts]<br>similarity = Similarity(<span class="hljs-string">&#x27;-Similarity-index&#x27;</span>, corpus, num_features=<span class="hljs-built_in">len</span>(dictionary))<br><span class="hljs-built_in">print</span>(similarity)<br><span class="hljs-comment"># 获取句子的相似度</span><br>new_sensence = sent1<br>test_corpus_1 = dictionary.doc2bow(word_tokenize(new_sensence))<br><br>cosine_sim = similarity[test_corpus_1][<span class="hljs-number">1</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;利用gensim计算得到两个句子的相似度： %.4f。&quot;</span>%cosine_sim)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs delphi">[[<span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;sky&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;sea&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>], [<span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;like&#x27;</span>, <span class="hljs-string">&#x27;running&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;reading&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]]<br>Similarity <span class="hljs-keyword">index</span> <span class="hljs-keyword">with</span> <span class="hljs-number">2</span> documents <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> shards (<span class="hljs-keyword">stored</span> under -Similarity-<span class="hljs-keyword">index</span>)<br>利用gensim计算得到两个句子的相似度： <span class="hljs-number">0.7303</span>。<br></code></pre></td></tr></table></figure><p>注意，如果在运行代码时出现以下warning:</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">gensim\utils.py:<span class="hljs-number">1209</span>: UserWarning: detected Windows; aliasing chunkize <span class="hljs-keyword">to</span> chunkize_serial<br>  warnings.warn(&quot;detected Windows; aliasing chunkize to chunkize_serial&quot;)<br><br>gensim\matutils.py:<span class="hljs-number">737</span>: FutureWarning: <span class="hljs-keyword">Conversion</span> <span class="hljs-keyword">of</span> the second argument <span class="hljs-keyword">of</span> issubdtype <span class="hljs-keyword">from</span> `<span class="hljs-type">int</span>` <span class="hljs-keyword">to</span> `np.signedinteger` <span class="hljs-keyword">is</span> deprecated. <span class="hljs-keyword">In</span> future, it will be treated <span class="hljs-keyword">as</span> `np.int32 == np.dtype(<span class="hljs-type">int</span>).<span class="hljs-keyword">type</span>`.<br>  <span class="hljs-keyword">if</span> np.issubdtype(vec.dtype, np.int):<br></code></pre></td></tr></table></figure><p>如果想要去掉这些warning，则在导入gensim模块的代码前添加以下代码即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> warnings<br>warnings.filterwarnings(action=<span class="hljs-string">&#x27;ignore&#x27;</span>,category=UserWarning,module=<span class="hljs-string">&#x27;gensim&#x27;</span>)<br>warnings.filterwarnings(action=<span class="hljs-string">&#x27;ignore&#x27;</span>,category=FutureWarning,module=<span class="hljs-string">&#x27;gensim&#x27;</span>)<br></code></pre></td></tr></table></figure><p>本文到此结束，感谢阅读！如果不当之处，请速联系笔者，欢迎大家交流！祝您好运~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape），欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>词袋模型</tag>
      
      <tag>句子相似度</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>技术文章写作计划</title>
    <link href="/2023/07/06/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0%E5%86%99%E4%BD%9C%E8%AE%A1%E5%88%92/"/>
    <url>/2023/07/06/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0%E5%86%99%E4%BD%9C%E8%AE%A1%E5%88%92/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><ul class="task-list"><li><label><input type="checkbox"checked="" />滑动验证码的识别</label></li><li><label><input type="checkbox"checked="" />滑动验证码的获取</label></li><li><label><input type="checkbox" />点选验证码的识别</label></li><li><label><input type="checkbox" />ELK简单搭建的demo</label></li><li><label><input type="checkbox" />文本聚类</label></li><li><label><input type="checkbox" />智能问答</label></li><li><label><input type="checkbox" />车牌的识别</label></li><li><label><input type="checkbox"checked="" />个人足迹地图（WEB服务）</label></li><li><label><input type="checkbox" checked="" />别名发现系统</label></li><li><label><input type="checkbox" />读取doc和docx文档</label></li><li><label><input type="checkbox"checked="" />利用celery实现定时任务</label></li><li><label><input type="checkbox"checked="" />文本标注工具Doccano</label></li><li><label><input type="checkbox"checked="" />利用Conda创建Python虚拟环境</label></li><li><label><input type="checkbox"checked="" />利用SFTP连接Linux服务器并上传、下载文件</label></li><li><label><input type="checkbox" checked="" />Flask学习之RESTfulAPI</label></li><li><label><input type="checkbox" />Flask学习之JWT认证</label></li><li><label><input type="checkbox" checked="" />BSON文件读取</label></li><li><label><inputtype="checkbox" />Flask学习之Flask-SQLALCHEMY</label></li><li><label><input type="checkbox"checked="" />设计模式（完成三篇：单例模式、工厂模式、监听模式）</label></li><li><label><input type="checkbox" />Redis</label></li><li><label><input type="checkbox" />supervisor使用</label></li><li><label><input type="checkbox"checked="" />tornado之文件下载（包含中文文件下载）</label></li><li><label><input type="checkbox"checked="" />利用CRF实现中文分词</label></li><li><label><input type="checkbox"checked="" />利用CRF实现模型预测</label></li><li><label><input type="checkbox"checked="" />protobuf的初次使用</label></li><li><label><inputtype="checkbox" />更新tensorflow/serving中的models.config文件中的model_version_policy</label></li><li><label><input type="checkbox"checked="" />tensorflow同时使用多个session</label></li><li><label><input type="checkbox"checked="" />如何离线安装tensorflow模块</label></li><li><label><input type="checkbox"checked="" />tensorboard查看ckpt和pb文件模型</label></li><li><label><input type="checkbox"checked="" />将ckpt转化为pb文件</label></li><li><label><input type="checkbox"checked="" />tensorflow/serving之BERT模型部署和预测</label></li><li><label><input type="checkbox"checked="" />tensorflow/serving实现模型部署</label></li><li><label><input type="checkbox" /><span class="citation"data-cites="property">@property</span></label></li><li><label><input type="checkbox" />tf_record</label></li><li><label><input type="checkbox" />指代关系抽取</label></li><li><label><inputtype="checkbox" />实体链接（百度实体链接比赛、武器装备知识图谱）</label></li><li><label><input type="checkbox"checked="" />文本多分类BERT微调</label></li><li><label><input type="checkbox"checked="" />文本多标签分类BERT微调</label></li><li><label><input type="checkbox"checked="" />文本序列标注BERT微调</label></li><li><label><input type="checkbox" checked="" />keras-bertEnglish系列（3个模型稍微调整即可）</label></li><li><label><input type="checkbox"checked="" />keras-bert调用ALBERT</label></li><li><label><input type="checkbox"checked="" />keras-bert模型部署</label></li><li><label><input type="checkbox"checked="" />h5文件转化为pb文件进行部署</label></li><li><label><input type="checkbox"checked="" />tensorflow/serving高效调用</label></li><li><label><inputtype="checkbox" />tensorflow_hub实现英文文本二分类</label></li><li><label><inputtype="checkbox" />tensorflow2.0和transformers实现文本多分类</label></li><li><label><input type="checkbox" />抽取式问答</label></li><li><label><input type="checkbox"checked="" />完形填空与文本纠错</label></li><li><label><inputtype="checkbox" />transformers实现中文序列标注</label></li><li><label><inputtype="checkbox" />tokenizers中的token使用方法</label></li><li><label><input type="checkbox" />BPE token 算法</label></li><li><label><input type="checkbox" checked="" />Keras:K折交叉验证</label></li><li><label><input type="checkbox"checked="" />使用Prothemus对tensorflow/serving进行服务监控</label></li><li><label><input type="checkbox"checked="" />seqeval获取序列标注实体识别结果</label></li><li><label><input type="checkbox" />ES进阶</label></li><li><label><input type="checkbox"checked="" />从荷兰国旗问题到快速排序</label></li><li><label><input type="checkbox" />中英文大模型调研</label></li><li><label><input type="checkbox" />LLaMA模型的介绍及其使用</label></li><li><label><input type="checkbox" />Fine-tune LLaMA模型</label></li><li><label><input type="checkbox"checked="" />OpenAI的tokenizer调研</label></li><li><label><input type="checkbox" checked="" />Gitlab CI/CD入门</label></li><li><label><input type="checkbox"checked="" />LangChain使用</label></li><li><label><input type="checkbox" checked="" />Flask部署</label></li><li><label><input type="checkbox" />LangChain构建阅读助手</label></li><li><label><inputtype="checkbox" />使用LoRA训练Flan-T5-XXL模型</label></li><li><label><inputtype="checkbox" />VSCode连接远程服务器进行开发</label></li></ul>]]></content>
    
    
    <categories>
      
      <category>写作计划</category>
      
    </categories>
    
    
    <tags>
      
      <tag>写作计划</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用Hexo+Github搭建个人博客网站</title>
    <link href="/2023/07/06/%E4%BD%BF%E7%94%A8Hexo-Github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/"/>
    <url>/2023/07/06/%E4%BD%BF%E7%94%A8Hexo-Github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>曾几何时，笔者也幻想过写个项目来搭建属于自己的个人博客。但是，写程序以及维护的成本，不禁让我犹豫再三，最后还是选择了CSDN等博客网站。将近六年的博客生涯，我尝试了不同的博客网站，各有各的利和弊，不变的是广告，这让人很不爽。直到今天，我看到了别人写的利用Hexo+Github来搭建个人博客网站，如获至宝。折腾了一阵以后，轻松完成了个人博客的搭建，这种清爽的界面风格，让人耳目一新，同时它又是免费的，功能繁多的，便于维护的。下面，我将会介绍如何来使用Hexo+Github搭建个人博客网站。</p><h3 id="准备工作">准备工作</h3><p>为了顺利地完成个人博客网站的搭建，需要做以下准备工作：</p><ul><li>安装Git和NodeJs（版本为18.16.1）；</li><li>安装Hexo（命令为<code>npm i -g hexo</code>）;</li><li>Github账号</li></ul><h3 id="搭建博客">搭建博客</h3><p>下面将分步来介绍如何使用Hexo和Github来搭建个人博客网站。</p><h4 id="创建github仓库">创建Github仓库</h4><p>在Github中新建一个名为username.github.io的空仓库，其中username是你在GitHub上的用户名，比如笔者的仓库名为percent.github.io。</p><h4 id="配置ssh">配置SSH</h4><p>如果想要使用远程从你的电脑上传文件至你的github仓库，那么，你就需要配置SSH。点击你个人Github上的Settings选项，在<code>SSH and GPG keys</code>中配置SSH的公钥，一般公钥位于<code>.ssh/id_rsa.pub</code>中，如下图：<img src="/img/hexo1.png" alt="配置SSH" /></p><h4 id="博客初始化">博客初始化</h4><p>新建一个空的文件夹，比如笔者新建了文件夹<code>github_blog</code>，使用<code>hexo init</code>命令初始化博客。初始化后的文件夹结构如下：<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs 1c">.<br>├── _config.yml<br>├── package.json<br>├── scaffolds<br>├── source<br><span class="hljs-string">|   ├── _drafts</span><br><span class="hljs-string">|   └── _posts</span><br>└── themes<br></code></pre></td></tr></table></figure> 上述文件说明如下：</p><ul><li>_config.yml 网站的 配置 信息，您可以在此配置大部分的参数。</li><li>package.json：应用程序的信息。EJS, Stylus 和 Markdown renderer已默认安装，您可以自由移除。</li><li>scaffolds：模版文件夹。当您新建文章时，Hexo会根据 scaffold来建立文件。</li><li>source：资源文件夹是存放用户资源的地方。</li><li>themes：主题文件夹。Hexo 会根据主题来生成静态页面。</li></ul><h4 id="生成个人博客网站">生成个人博客网站</h4><p>配置_config.yml文件，配置信息如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># Deployment</span><br><span class="hljs-comment">## Docs: https://hexo.io/docs/deployment.html</span><br><span class="hljs-attr">deploy:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">git</span><br>  <span class="hljs-attr">repo:</span> <span class="hljs-string">https://github.com/percent4/percent4.github.io.git(第一步创建的Github仓库)</span><br>  <span class="hljs-attr">branch:</span> <span class="hljs-string">master</span><br></code></pre></td></tr></table></figure><p>安装插件<code>npm install hexo-deployer-git --save</code>后，运行如下命令：<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">hexo</span> clean<span class="hljs-comment"># 清除数据</span><br>hexo d -g<span class="hljs-comment"># 生成博客</span><br></code></pre></td></tr></table></figure>这时候，你会看到博客数据会提交至Github的信息，而第一步创建的空仓库也有了提交内容，当然，你的个人博客也搭建搭建完毕，访问网址为：https://username.github.io/，其中username是你在GitHub上的用户名。界面如下： <imgsrc="/img/hexo2.png" alt="Hexo界面" /></p><h3 id="博客维护">博客维护</h3><p>Hexo提供了一套维护博客的优雅的办法。笔者在此仅介绍如何新建一篇博客。新建博客格式为markdown格式，比如我想创建一篇名为<code>利用Tornado搭建文档预览系统</code>的博客，可以使用以下命令：</p><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs haxe">hexo <span class="hljs-keyword">new</span> <span class="hljs-type"></span>利用Tornado搭建文档预览系统<br></code></pre></td></tr></table></figure><p>这时候会在你当前目录下的source/_posts文件夹下生成<code>利用Tornado搭建文档预览系统.md</code>,其中内容如下：<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">利用Tornado搭建文档预览系统</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2020-06-09 18:32:29</span><br><span class="hljs-attr">tags:</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure>其中title为博客标题，date为博客时间，tags为博客标签。在<code>---</code>后面可以写博客正文的内容。写完博客后，使用命令 <figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">hexo</span> clean<span class="hljs-comment"># 清除数据</span><br>hexo d -g<span class="hljs-comment"># 生成博客</span><br></code></pre></td></tr></table></figure> 就会更新个人博客。</p><h3 id="更换主题">更换主题</h3><p>Hexo提供的默认主题为landscape,我们想替换主题为Fluid.Hexo替换主题为Fluid的步骤如下：</p><ol type="1"><li>通过<code>npm</code>直接安装，进入博客目录执行命令：<code>npm install --save hexo-theme-fluid</code></li><li>将node_modules文件夹下的hexo-theme-fluid复制到themes文件夹，并重名为fluid</li><li>在博客目录下创建_config.fluid.yml，将主题的_config.yml内容复制进去，并将<code>theme:</code>后面的主题修改为fluid</li><li>使用<code>hexo s</code>进行本地部署，如无问题，则使用命令<code>hexo d -g</code>进行远程部署</li></ol><h3 id="总结">总结</h3><p>当然，Hexo还提供了许多丰富的功能，比如theme（主题）的个性化定制等，这会使得你的博客内容更加丰富，功能更加完善。</p><p>笔者大家的个人博客网站为：<ahref="https://percent4.github.io/">https://percent4.github.io/</a>，欢迎大家访问。以后，笔者将会逐渐往个人博客网站倾斜，而减少使用公开的博客社区。</p>]]></content>
    
    
    <categories>
      
      <category>Hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
      <tag>Github</tag>
      
      <tag>个人博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何使用Hexo？</title>
    <link href="/2023/07/06/hello-world/"/>
    <url>/2023/07/06/hello-world/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>欢迎来到 <a href="https://hexo.io/">Hexo</a>!这是我的第一篇博客。查阅 <ahref="https://hexo.io/docs/">documentation</a> 获取更多信息。如果你在使用Hexo遇到问题，你可以在这里找到答案 <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a>，或者你可以在这上面提问：<ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="快速开始">快速开始</h2><h3 id="创建一篇新的博客">创建一篇新的博客</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>更新信息: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="运行服务">运行服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>更新信息: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="产生静态文件">产生静态文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>更新信息: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="远程部署">远程部署</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>更新信息: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
