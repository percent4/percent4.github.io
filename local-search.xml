<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>利用关系抽取构建知识图谱的一次尝试</title>
    <link href="/2023/07/08/%E5%88%A9%E7%94%A8%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E6%9E%84%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/"/>
    <url>/2023/07/08/%E5%88%A9%E7%94%A8%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%E6%9E%84%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%9A%84%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<h3 id="关系抽取">关系抽取</h3><p>信息抽取（Information Extraction, IE）旨在从大规模非结构或半结构的自然语言文本中抽取结构化信息。关系抽取（Relation Extraction, RE）是其中的重要子任务之一，主要目的是从文本中识别实体并抽取实体之间的语义关系，是自然语言处理（NLP）中的一项基本任务。比如，我们可以从下面的一段话中，</p><blockquote><p>鸿海集团董事长郭台铭25日表示，阿里巴巴集团董事局主席马云提的新零售、新制造中的「新制造」，是他给加上的。网易科技报导，郭台铭在2018深圳IT领袖峰会谈到工业互联网时表示，眼睛看的、脑筋想的、嘴巴吃的、耳朵听的，都在随着互联网的发展而蓬勃发展，当然互联网不是万能的，比如说刚才李小加要水喝，在手机上一按就能出一瓶水吗？当然做不到，还是得有实体经济。</p></blockquote><p>可以抽取出如下三元组，用来表示实体之间的关系：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scheme">[<span class="hljs-symbol">&#x27;鸿海集团</span>&#x27;, <span class="hljs-symbol">&#x27;董事长</span>&#x27;, <span class="hljs-symbol">&#x27;郭台铭</span>&#x27;]<br>[<span class="hljs-symbol">&#x27;阿里巴巴集团</span>&#x27;, <span class="hljs-symbol">&#x27;主席</span>&#x27;, <span class="hljs-symbol">&#x27;马云</span>&#x27;]<br></code></pre></td></tr></table></figure><p>并且能够形成如下的简单的知识图谱（Knowledge Graph）。</p><p><img src="/img/kg1_1.png" alt=""></p><p>关于知识图谱，笔者已经在文章<a href="https://www.jianshu.com/p/286eeef0e0c3">SPARQL入门（一）SPARQL简介与简单使用</a>中给出了一些介绍，而利用关系抽取，我们可以从一些非结构化数据中，提取出实体之间的关系，形成知识图谱，这在很大程度上可以帮助我们减轻构建知识图谱的成本。非结构化数据越多，关系抽取效果越好，我们构建的知识图谱就会越庞大，实体之间的关系也会越丰富。</p><h3 id="如何做好关系抽取？">如何做好关系抽取？</h3><p>目前，网络上有许多与关系抽取相关的公开比赛，比如：</p><ul><li><p>CCKS 2019 人物关系抽取，网址为：<a href="https://biendata.com/competition/ccks_2019_ipre/">https://biendata.com/competition/ccks_2019_ipre/</a>  ；</p></li><li><p>2019语言与智能技术竞赛信息抽取：<a href="http://lic2019.ccf.org.cn/kg">http://lic2019.ccf.org.cn/kg</a> 。</p><p>常用的关系抽取语料如下：</p></li><li><p>MUC关系抽取任务数据集；</p></li><li><p>ACE关系抽取任务数据集；</p></li><li><p>TAC-KBP数据集。</p><p>现阶段，关系抽取的办法主要如下：</p></li><li><p>基于规则的模式匹配；</p></li><li><p>基于监督学习的方法；</p></li><li><p>半监督和无监督学习方法；</p></li><li><p>远程监督的方法；</p></li><li><p>深度学习模型。</p><p>接着，笔者想说下，为什么最近会研究关系抽取。在一个偶然的机会，笔者看到了这个网站：<a href="https://www.wisers.ai/zh-cn/browse/relation-extraction/demo/">https://www.wisers.ai/zh-cn/browse/relation-extraction/demo/</a> ，截图如下：</p></li></ul><p><img src="/img/kg1_2.png" alt=""></p><p>这个图给人以一种非常炫酷的感觉，因此，笔者就被它所吸引了。但笔者在这个demo网站上尝试了几篇新的语料，有些效果好，有些效果不尽如人意，因此，笔者决定自己动手实现一个关系抽取的模型！</p><p>虽然网上已经有许多现成的很好的关系抽取的模型，但笔者还是希望能够按照自己的意愿和想法来实现一下，当然，仅仅是作为一次尝试。笔者的思路如下：</p><ul><li>以句子级别进行标注，标注出句子中的主语，谓语，宾语，形成标注序列；</li><li>利用标注好的语料，采用bert+dl的方法进行训练；</li><li>对新的语料，预测主语，谓语，宾语，然后利用一定的策略，形成实体关系；</li><li>对新语料的实体关系进行可视化展示。</li></ul><p>如果你对笔者的尝试感兴趣，请尝试这阅读下去。</p><h3 id="如何标注？">如何标注？</h3><p>按照笔者的惯例，还是自己进行标注。那么，对于关系抽取，该如何进行标注呢？比如，下面这句话：</p><blockquote><p>应日本国首相安倍晋三邀请，出席二十国集团领导人第十四次峰会。</p></blockquote><p>我们需要的实体关系应该是： 日本国–&gt;首相–&gt;安倍晋三，那么我们可以选择主语为日本，谓语为首相，宾语为安倍晋三，形成的标注序列如下：</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs mathematica">应<span class="hljs-built_in">O</span><br>日<span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>本<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>国<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>首<span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">PRED</span><br>相<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">PRED</span><br>安<span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">OBJ</span><br>倍<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">OBJ</span><br>晋<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">OBJ</span><br>三<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">OBJ</span><br>邀<span class="hljs-built_in">O</span><br>请<span class="hljs-built_in">O</span><br>，<span class="hljs-built_in">O</span><br>出<span class="hljs-built_in">O</span><br>席<span class="hljs-built_in">O</span><br>二<span class="hljs-built_in">O</span><br>十<span class="hljs-built_in">O</span><br>国<span class="hljs-built_in">O</span><br>集<span class="hljs-built_in">O</span><br>团<span class="hljs-built_in">O</span><br>领<span class="hljs-built_in">O</span><br>导<span class="hljs-built_in">O</span><br>人<span class="hljs-built_in">O</span><br>第<span class="hljs-built_in">O</span><br>十<span class="hljs-built_in">O</span><br>四<span class="hljs-built_in">O</span><br>次<span class="hljs-built_in">O</span><br>峰<span class="hljs-built_in">O</span><br>会<span class="hljs-built_in">O</span><br>。<span class="hljs-built_in">O</span><br></code></pre></td></tr></table></figure><p>对于句子中出现多主语，多谓语，多宾语的情况，也可以照此进行标注，比如下面这句：</p><blockquote><p>齐鹏飞同志任中共中国人民大学委员会常委、副书记。</p></blockquote><p>形成的标注序列如下：</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs mathematica">齐<span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">OBJ</span><br>鹏<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">OBJ</span><br>飞<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">OBJ</span><br>同<span class="hljs-built_in">O</span><br>志<span class="hljs-built_in">O</span><br>任<span class="hljs-built_in">O</span><br>中<span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>共<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>中<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>国<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>人<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>民<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>大<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>学<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>委<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>员<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>会<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">SUBJ</span><br>常<span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">PRED</span><br>委<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">PRED</span><br>、<span class="hljs-built_in">O</span><br>副<span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">PRED</span><br>书<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">PRED</span><br>记<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">PRED</span><br>。<span class="hljs-built_in">O</span><br></code></pre></td></tr></table></figure><p>对此，我们希望形成两个三元组，分别为：中共中国人民大学委员会–&gt;常委–&gt;齐鹏飞, 中共中国人民大学委员会–&gt;副书记–&gt;齐鹏飞。</p><p>笔者利用自己的标注平台（后续会在Github开源），一共标注了950分语料，其中80%作为训练集，10%作为验证集，另外10%作为测试集。当然，标注的过程是很痛苦的，这些标注量也还远远不够，后续会持续不断地更新。</p><h3 id="模型训练">模型训练</h3><p>由于是小样本量的标注数量，因此，在模型的选择上，需要预训练模型，笔者的预训练模型选择BERT。在预训练的基础上，选择BiLSTM+CRF深度学习模型，对上述语料进行训练，共训练100次，在验证集和测试集上的效果如下：</p><p>验证集：</p><table><thead><tr><th>项目</th><th>precision</th><th>recall</th><th>f1</th></tr></thead><tbody><tr><td>全部</td><td>71.08%</td><td>78.27%</td><td>74.50%</td></tr><tr><td>宾语</td><td>78.95%</td><td>88.24%</td><td>83.33%</td></tr><tr><td>谓语</td><td>68.00%</td><td>74.56%</td><td>71.13%</td></tr><tr><td>主语</td><td>67.18%</td><td>73.33%</td><td>70.12%</td></tr></tbody></table><p>测试集</p><table><thead><tr><th>项目</th><th>precision</th><th>recall</th><th>f1</th></tr></thead><tbody><tr><td>全部</td><td>75.07%</td><td>82.18%</td><td>78.46%</td></tr><tr><td>宾语</td><td>78.33%</td><td>85.45%</td><td>81.74%</td></tr><tr><td>谓语</td><td>73.23%</td><td>82.30%</td><td>77.50%</td></tr><tr><td>主语</td><td>73.88%</td><td>79.20%</td><td>76.45%</td></tr></tbody></table><p>效果并没有达到很好，一方面是标注策略的问题，另一方面是标注的数量问题（因为这是一个通用模型），后续我们可以看看，当标注数量提上去后，模型训练的效果是否会有提升。</p><h3 id="模型预测">模型预测</h3><p>接着，我们利用刚才训练好的模型，对新的句子进行预测，记住，预测的级别为句子。当然，预测的结果，只是序列标注模型识别出的结果，我们还要采用一定的策略，将其形成三元组。比如以下的句子：</p><blockquote><p>英媒称，美国农业部长桑尼·珀杜在6月25日播出的一个访谈节目中承认，美国农民是特朗普总统对华贸易战的“受害者”。</p></blockquote><p>预测的结果如下：</p><blockquote><p>[{‘word’: ‘美国’, ‘start’: 4, ‘end’: 6, ‘type’: ‘SUBJ’}, {‘word’: ‘农业部长’, ‘start’: 6, ‘end’: 10, ‘type’: ‘PRED’}, {‘word’: ‘桑尼·珀杜’, ‘start’: 10, ‘end’: 15, ‘type’: ‘OBJ’}, {‘word’: ‘美国’, ‘start’: 34, ‘end’: 36, ‘type’: ‘SUBJ’}]</p></blockquote><p>可以看到，模型识别出主语为美国，谓语为农业部长，宾语为桑尼·珀杜，这是一个完美的三元组。</p><p>我们再来对下面的语句进行预测：</p><blockquote><p>6月25日，华为常务董事、运营商事业部总裁丁耘表示，华为已在全球范围内获得50个5G商用合同，其中2/3是由华为协助其构建的。</p></blockquote><p>预测结果为：</p><blockquote><p>[{‘word’: ‘华为’, ‘start’: 6, ‘end’: 8, ‘type’: ‘SUBJ’}, {‘word’: ‘常务董事’, ‘start’: 8, ‘end’: 12, ‘type’: ‘PRED’}, {‘word’: ‘运营商事业部’, ‘start’: 13, ‘end’: 19, ‘type’: ‘SUBJ’}, {‘word’: ‘总裁’, ‘start’: 19, ‘end’: 21, ‘type’: ‘PRED’}, {‘word’: ‘丁耘’, ‘start’: 21, ‘end’: 23, ‘type’: ‘OBJ’}, {‘word’: ‘华为’, ‘start’: 26, ‘end’: 28, ‘type’: ‘SUBJ’}, {‘word’: ‘华为’, ‘start’: 54, ‘end’: 56, ‘type’: ‘SUBJ’}]</p></blockquote><p>这就需要一定的策略，才能识别出具体的三元组了。笔者采用的策略如下：</p><ul><li>按主语，谓语，宾语进行归类，形成主体集合<code>&#123;华为, 运营商事业部&#125;</code>，谓语集合<code>&#123;常务董事, 总裁&#125;</code>以及宾语集合<code>&#123;丁耘&#125;</code>；</li><li>接着，按照各个元素在句子出现的位置进行组合，比如<code>华为</code>的位置，离<code>常务董事</code>挨得近，那么形成一个三元组[‘华为’, ‘常务董事’, ‘丁耘’]，同理，形成另一个三元组[‘运营商事业部’, ‘总裁’, ‘丁耘’];</li><li>将句子按照逗号进行分割，形成<code>小句子集合</code>，看三元组的三个元素是否都在一个小句子中，如果是，则提取该三元组，如果不是，则放弃该三元组。</li></ul><h3 id="关系抽取可视化">关系抽取可视化</h3><p>对于关系抽取后的节后，我们将三元组导入至Neo4J中，查看可视化的效果。我们一共选择三篇文章进行测试，为了取得较好的效果，我们选择了程序处理+人工check（过滤）的过程，稍微有点工作量。</p><p>第一篇文章来自微信公众号，标题为：<code>哈工大社会计算与信息检索研究中心（HIT-SCIR）拟于7月20日在哈工大举办首届事理图谱研讨会</code>, 访问网址为：<a href="https://mp.weixin.qq.com/s/9H7rxsPdo5S5trwz_CASZw%EF%BC%8C">https://mp.weixin.qq.com/s/9H7rxsPdo5S5trwz_CASZw，</a> 我们抽取出来的实体关系（带原文）如下：</p><blockquote><p>原文,s,p,o<br>2017年10月，研究中心主任刘挺教授在中国计算机大会（CNCC）上正式提出事理图谱的概念，2018年9月，在研究中心丁效老师的主持下，研制出中文金融事理图谱1.0版本。,研究中心,老师,丁效<br>2017年10月，研究中心主任刘挺教授在中国计算机大会（CNCC）上正式提出事理图谱的概念，2018年9月，在研究中心丁效老师的主持下，研制出中文金融事理图谱1.0版本。,研究中心,教授,刘挺<br>2017年10月，研究中心主任刘挺教授在中国计算机大会（CNCC）上正式提出事理图谱的概念，2018年9月，在研究中心丁效老师的主持下，研制出中文金融事理图谱1.0版本。,研究中心,主任,刘挺<br>白硕（上海证券交易所前任总工程师，中科院计算所博导）,上海证券交易所,前任总工程师,白硕<br>荀恩东（北京语言大学信息学院院长）,北京语言大学信息学院,院长,荀恩东<br>赵军（中科院自动化所研究员）,中科院自动化所,研究员,赵军<br>吴华（百度技术委员会主席）,百度技术,主席,吴华<br>吴华（百度技术委员会主席）,百度技术,委员,吴华<br>宋阳秋（香港科技大学助理教授）,香港科技大学,助理教授,宋阳秋<br>李金龙（招商银行人工智能实验室负责人）,招商银行人工智能实验室,负责人,李金龙<br>李世奇（北京西亚财信人工智能科技有限责任公司CEO）,北京西亚财信人工智能科技有限责任公司,CEO,李世奇</p></blockquote><p>对于这篇文章，我们没有抽取出<code>李斌阳（国际关系学院副教授）</code>中的实体关系，并且<code>吴华（百度技术委员会主席</code>这句为抽取有误，正确的应为：百度技术委员会,主席,吴华。</p><p>将上述关系修改下，导入至Neo4J中，得到的实体关系图如下：</p><p><img src="/img/kg1_3.png" alt=""></p><p>第二篇文章为凤凰网的新闻，标题为<code>南阳“水氢车”风波：一个中部城市的招商突围战</code>，访问网址为：<a href="https://news.ifeng.com/c/7ntawxhCDvj">https://news.ifeng.com/c/7ntawxhCDvj</a> ，我们抽取出来的实体关系（带原文）如下表：</p><blockquote><p>原文,s,p,o<br>2017年，因巴铁所属企业北京华赢凯来资产管理有限公司涉嫌非法集资活动，北京警方将“巴铁之父”白丹青依法刑拘。,巴铁,之父,白丹青<br>南阳“神车”下线之后，界面新闻约访南阳市委书记张文深，被告知张文深与市长双双出差，工作人员并不确定张文深何时回到南阳，他的手机则处于忙线状态。,南阳,市委书记,张文深<br>南阳洛特斯新能源汽车有限公司实际控制人庞青年说，水氢汽车并未下线，媒体的报道使他措手不及。,南阳洛特斯新能源汽车有限公司,实际控制人,庞青年<br>从2006年开始，前湖北工业大学学者董仕节带领的团队开始研发一项车载铝合金水解制氢技术，并获得国家973前期研究项目和国家自然基金的支持。,湖北工业大学,学者,董仕节<br>南阳市高新区投资公司负责人尹召翼在接受央视采访时表示，庞青年经常拿“水氢”来混淆“水解制氢”的概念。,南阳市高新区投资公司,负责人,尹召翼<br>南阳市招商局招商二科科长赵怿接受界面新闻采访时表示，他只知道这个项目不是招商科引进的。,南阳市招商局招商二科,科长,赵怿<br>庞青年告诉界面新闻，南阳市高新区投资有限公司已经为他提供了9600万元，用途是南阳高新区投资有限公司给南阳市洛特斯新能源汽车有限公司的注册资金，占股49%。,南阳高新区投资有限公司,南阳市,洛特斯新能<br>曾先后在南阳市委党校、南阳市发改委任职的退休干部张一江（化名）说，“走工业突围道路的冲动在南阳早已有之，所以这几年的巴铁神车项目、加水就能跑的神车项目能被引进南阳，我觉得算不上奇怪。”,南阳市发改委,退休干部,张一江<br>以此次南阳神车项目为例，南阳市科技局局长张梅明确告诉界面新闻，庞青年的企业进入南阳时未有任何部门邀请科技局鉴别其“新能源技术”。,南阳市科技局,局长,张梅<br>官方报道显示，2012年6月18日，一位时任南阳市委主要领导在南阳宾馆会见了青年汽车董事局主席庞青年一行，双方就如何发挥自身优势，谋求合作共赢进行了交流，“南阳的发展需要大项目的带动和支撑，我们欢迎中国青年汽车集团这样有实力、有影响的大企业来南阳投资兴业。,青年汽车,董事局主席,庞青年<br>早在当年5月，在第十九届中国北京国际科技博览会上，时任南阳市副市长郑茂杰与巴铁科技发展有限公司总工程师宋有洲签署战略合作协议。,巴铁科技发展有限公司,总工程师,宋有洲<br>早在当年5月，在第十九届中国北京国际科技博览会上，时任南阳市副市长郑茂杰与巴铁科技发展有限公司总工程师宋有洲签署战略合作协议。,南阳市,副市长,郑茂杰</p></blockquote><p>对于这篇文章，我们没有抽取出一些关系，比如<code>南阳市发展和改革委员会主任乔长恩受访时承认，招商引入南阳洛斯特之前“掌握这个情况。”</code>等，并且<code>庞青年告诉界面新闻，南阳市高新区投资有限公司已经为他提供了9600万元，用途是南阳高新区投资有限公司给南阳市洛特斯新能源汽车有限公司的注册资金，占股49%。</code>这句为抽取有误，应当删除。</p><p>将上述关系修改下，导入至Neo4J中，得到的实体关系图如下：</p><p><img src="/img/kg1_4.png" alt=""></p><p>最后一篇为长篇小说——著名作家路遥的《平凡的世界》第一部。利用我们的关系抽取模型，一共在该小说中抽取了169对实体关系，其中有效实体关系100对。由于我们在该小说中抽取的实体关系过多，因此只展示前10条原文及抽取的实体关系：</p><blockquote><p>原文,s,p,o<br>每天来回二十里路，与他一块上学的金波和大队书记田福堂的儿子润生都有自行车，只有他是两条腿走路。,田福堂,儿子,润生<br>不过，他对润生的姐姐润叶倒怀有一种亲切的感情。,润生,姐姐,润叶<br>“金波是金俊海的小子。”,金俊海,小子,“金波<br>脑子里把前后村庄未嫁的女子一个个想过去，最后选定了双水村孙玉厚的大女子兰花。,双水村孙玉厚,大女子,兰花<br>玉亭是大队党支部委员、农田基建队队长、贫下中农管理学校委员会主任，一身三职，在村里也是一个人物。,贫下中农管理学校,主任,玉亭<br>玉亭是大队党支部委员、农田基建队队长、贫下中农管理学校委员会主任，一身三职，在村里也是一个人物。,农田基建队,队长,玉亭<br>玉亭是大队党支部委员、农田基建队队长、贫下中农管理学校委员会主任，一身三职，在村里也是一个人物。,大队,党支部委员,玉亭<br>会战总指挥是公社副主任徐治功，副总指挥是公社武装专干杨高虎。,公社,武装,杨高虎<br>会战总指挥是公社副主任徐治功，副总指挥是公社武装专干杨高虎。,公社,副主任,徐治功<br>这时候，双水村妇女主任贺凤英，正领着本村和外村的一些“铁姑娘”，忙碌地布置会场。,双水村,妇女主任,贺凤英<br>……</p></blockquote><p>将上述关系修改下，导入至Neo4J中，得到的实体关系图如下：</p><p><img src="/img/kg1_5.png" alt=""></p><p><img src="/img/kg1_6.png" alt=""></p><p><img src="/img/kg1_7.png" alt=""></p><h3 id="总结">总结</h3><p>本次关系抽取仅仅作为笔者的一次尝试，在实际的应用中还存在着许多的不足之处，比如：</p><ul><li><p>对语料的标注，是否可以采用其他更好的办法；</p></li><li><p>作为通用模型，标注的数量还远远不够；</p></li><li><p>模型的选择方面，是否可以其他更好的模型；</p></li><li><p>对预测的结果，如何能更好地提取出三元组；</p></li><li><p>将三元组扫入至图数据库中，能否做到实体对齐，且能做一些实体关系的分析与推理。</p><p>本文用到的语料以及模型会在后续的文章中公开，希望大家能继续关注～</p><p>注意：不妨了解下笔者的微信公众号： NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注~</p></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>NLP（十八）利用ALBERT提升模型预测速度的一次尝试</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%E5%88%A9%E7%94%A8ALBERT%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E9%80%9F%E5%BA%A6%E7%9A%84%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89%E5%88%A9%E7%94%A8ALBERT%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E9%80%9F%E5%BA%A6%E7%9A%84%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<h3 id="前沿">前沿</h3><p>在文章<ahref="https://percent4.github.io/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%E5%88%A9%E7%94%A8tensorflow-serving%E9%83%A8%E7%BD%B2kashgari%E6%A8%A1%E5%9E%8B/">NLP（十七）利用tensorflow-serving部署kashgari模型</a>中，笔者介绍了如何利用tensorflow-serving部署来部署深度模型模型，在那篇文章中，笔者利用kashgari模块实现了经典的BERT+Bi-LSTM+CRF模型结构，在标注了时间的文本语料（大约2000多个训练句子）中也达到了很好的识别效果，但是也存在着不足之处，那就是模型的预测时间过长，平均预测一个句子中的时间耗时约400毫秒，这种预测速度在生产环境或实际应用中是不能忍受的。</p><p>查看该模型的耗时原因，很大一部分原因在于BERT的调用。BERT是当下最火，知名度最高的预训练模型，虽然会使得模型的训练、预测耗时增加，但也是小样本语料下的最佳模型工具之一，因此，BERT在模型的架构上是不可缺少的。那么，该如何避免使用预训练模型带来的模型预测耗时过长的问题呢？</p><p>本文决定尝试使用ALBERT，来验证ALBERT在提升模型预测速度方面的应用，同时，也算是本人对于使用ALBERT的一次实战吧~</p><h3 id="albert简介">ALBERT简介</h3><p>我们不妨花一些时间来简单地了解一下ALBERT。ALBERT是最近一周才开源的预训练模型，其Github的网址为：https://github.com/brightmart/albert_zh，其论文可以参考网址：https://arxiv.org/pdf/1909.11942.pdf 。</p><p>根据ALBERT的Github介绍，ALBERT在海量中文语料上进行了预训练，模型的参数更少，效果更好。以albert_tiny_zh为例，其文件大小16M、参数为1.8M，模型大小仅为BERT的1/25，效果仅比BERT略差或者在某些NLP任务上更好。在本文的预训练模型中，将采用albert_tiny_zh。</p><h3 id="利用albert训练时间识别模型">利用ALBERT训练时间识别模型</h3><p>我们以Github中的bertNER为本次项目的代码模板，在该项目中，实现的模型为BERT+Bi-LSTM+CRF，我们将BERT替换为ALBERT，也就是说笔者的项目中模型为ALBERT+Bi-LSTM+CRF，同时替换bert文件夹的代码为alert_zh，替换预训练模型文件夹chinese_L-12_H-768_A-12（BERT中文预训练模型文件）为albert_tiny。当然，也需要修改一部分的项目源代码，来适应ALBERT的模型训练。</p><p>数据集采用笔者自己标注的时间语料，即标注了时间的句子，大概2000+句子，其中75%作为训练集（time.train文件），10%作为验证集（time.dev文件），15%作为测试集（time.test文件）。在这里笔者不打算给出具体的Python代码，因为工程比较复杂，有兴趣的额读者可以去查看该项目的Github地址：<ahref="https://github.com/percent4/ALBERT_4_Time_Recognition">https://github.com/percent4/ALBERT_4_Time_Recognition</a>。</p><p>一些模型的参数可以如下：</p><ul><li><p>预训练模型：ALBERT（tiny）</p></li><li><p>训练样本的最大字符长度： 128</p></li><li><p>batch_size: 8</p></li><li><p>epoch: 100</p></li><li><p>双向LSTM的个数：100</p><p>ALBERT的模型训练时间也会显著提高，我们耐心地等待模型训练完毕。在time.dev和time.test数据集上的表现如下表：</p></li></ul><table><thead><tr class="header"><th>数据集</th><th>precision</th><th>recall</th><th>f1</th></tr></thead><tbody><tr class="odd"><td>time.dev</td><td>81.41%</td><td>84.95%</td><td>83.14%</td></tr><tr class="even"><td>time.test</td><td>83.03%</td><td>86.38%</td><td>84.67%</td></tr></tbody></table><p>接着笔者利用训练好的模型，用tornado封装了一个模型预测的HTTP服务，具体的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> traceback<br><br><span class="hljs-keyword">import</span> tornado.httpserver<br><span class="hljs-keyword">import</span> tornado.ioloop<br><span class="hljs-keyword">import</span> tornado.options<br><span class="hljs-keyword">import</span> tornado.web<br><span class="hljs-keyword">from</span> tornado.options <span class="hljs-keyword">import</span> define, options<br><br><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> create_model, get_logger<br><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> Model<br><span class="hljs-keyword">from</span> loader <span class="hljs-keyword">import</span> input_from_line<br><span class="hljs-keyword">from</span> train <span class="hljs-keyword">import</span> FLAGS, load_config, train<br><br><span class="hljs-comment"># 定义端口为12306</span><br>define(<span class="hljs-string">&quot;port&quot;</span>, default=<span class="hljs-number">12306</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;run on the given port&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br><span class="hljs-comment"># 导入模型</span><br>config = load_config(FLAGS.config_file)<br>logger = get_logger(FLAGS.log_file)<br><span class="hljs-comment"># limit GPU memory</span><br>tf_config = tf.ConfigProto()<br>tf_config.gpu_options.allow_growth = <span class="hljs-literal">False</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(FLAGS.map_file, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    tag_to_id, id_to_tag = pickle.load(f)<br><br>sess = tf.Session(config=tf_config)<br>model = create_model(sess, Model, FLAGS.ckpt_path, config, logger)<br><br><span class="hljs-comment"># 模型预测的HTTP接口</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResultHandler</span>(tornado.web.RequestHandler):<br>    <span class="hljs-comment"># post函数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">post</span>(<span class="hljs-params">self</span>):<br>        event = self.get_argument(<span class="hljs-string">&#x27;event&#x27;</span>)<br>        result = model.evaluate_line(sess, input_from_line(event, FLAGS.max_seq_len, tag_to_id), id_to_tag)<br>        self.write(json.dumps(result, ensure_ascii=<span class="hljs-literal">False</span>))<br><br><span class="hljs-comment"># 主函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    <span class="hljs-comment"># 开启tornado服务</span><br>    tornado.options.parse_command_line()<br>    <span class="hljs-comment"># 定义app</span><br>    app = tornado.web.Application(<br>            handlers=[<br>                      (<span class="hljs-string">r&#x27;/subj_extract&#x27;</span>, ResultHandler)<br>                     ], <span class="hljs-comment">#网页路径控制</span><br>           )<br>    http_server = tornado.httpserver.HTTPServer(app)<br>    http_server.listen(options.port)<br>    tornado.ioloop.IOLoop.instance().start()<br><br>main()<br></code></pre></td></tr></table></figure><h3 id="模型预测提速了吗">模型预测提速了吗？</h3><p>将模型预测封装成HTTP服务后，我们利用Postman来测试模型预测的效果和时间，如下图所示：</p><p><img src="/img/nlp18_1.png" /></p><p>可以看到，模型预测的结果正确，且耗时仅为38ms。</p><p>接着我们尝试多测试几个句子的测试，测试代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Daxing, Beijing</span><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> time<br><br>url = <span class="hljs-string">&#x27;http://localhost:12306/subj_extract&#x27;</span><br><br>texts = [<span class="hljs-string">&#x27;记者从国家发展改革委、商务部相关方面获悉，日前美方已决定对拟于10月1日实施的中国输美商品加征关税措施做出调整，中方支持相关企业从即日起按照市场化原则和WTO规则，自美采购一定数量大豆、猪肉等农产品，国务院关税税则委员会将对上述采购予以加征关税排除。&#x27;</span>,<br>         <span class="hljs-string">&#x27;据印度Zee新闻网站12日报道，亚洲新闻国际通讯社援引印度军方消息人士的话说，9月11日的对峙事件发生在靠近班公错北岸的实际控制线一带。&#x27;</span>,<br>         <span class="hljs-string">&#x27;儋州市决定，从9月开始，对城市低保、农村低保、特困供养人员、优抚对象、领取失业保险金人员、建档立卡未脱贫人口等低收入群体共3万多人，发放猪肉价格补贴，每人每月发放不低于100元补贴，以后发放标准，将根据猪肉价波动情况进行动态调整。&#x27;</span>,<br>         <span class="hljs-string">&#x27;9月11日，华为心声社区发布美国经济学家托马斯.弗里德曼在《纽约时报》上的专栏内容，弗里德曼透露，在与华为创始人任正非最近一次采访中，任正非表示华为愿意与美国司法部展开话题不设限的讨论。&#x27;</span>,<br>         <span class="hljs-string">&#x27;造血干细胞移植治疗白血病技术已日益成熟，然而，通过该方法同时治愈艾滋病目前还是一道全球尚在攻克的难题。&#x27;</span>,<br>         <span class="hljs-string">&#x27;英国航空事故调查局（AAIB）近日披露，今年2月6日一趟由德国法兰克福飞往墨西哥坎昆的航班上，因飞行员打翻咖啡使操作面板冒烟，导致飞机折返迫降爱尔兰。&#x27;</span>,<br>         <span class="hljs-string">&#x27;当地时间周四（9月12日），印度尼西亚财政部长英卓华（Sri Mulyani Indrawati）明确表示：特朗普的推特是风险之一。&#x27;</span>,<br>         <span class="hljs-string">&#x27;华中科技大学9月12日通过其官方网站发布通报称，9月2日，我校一硕士研究生不幸坠楼身亡。&#x27;</span>,<br>         <span class="hljs-string">&#x27;微博用户@ooooviki 9月12日下午公布发生在自己身上的惊悚遭遇：一个自称网警、名叫郑洋的人利用职务之便，查到她的完备的个人信息，包括但不限于身份证号、家庭地址、电话号码、户籍变动情况等，要求她做他女朋友。&#x27;</span>,<br>         <span class="hljs-string">&#x27;今天，贵阳取消了汽车限购，成为目前全国实行限购政策的9个省市中，首个取消限购的城市。&#x27;</span>,<br>         <span class="hljs-string">&#x27;据悉，与全球同步，中国区此次将于9月13日于iPhone官方渠道和京东正式开启预售，京东成Apple中国区唯一官方授权预售渠道。&#x27;</span>,<br>         <span class="hljs-string">&#x27;根据央行公布的数据，截至2019年6月末，存款类金融机构住户部门短期消费贷款规模为9.11万亿元，2019年上半年该项净增3293.19亿元，上半年增量看起来并不乐观。&#x27;</span>,<br>         <span class="hljs-string">&#x27;9月11日，一段拍摄浙江万里学院学生食堂的视频走红网络，视频显示该学校食堂不仅在用餐区域设置了可以看电影、比赛的大屏幕，还推出了“一人食”餐位。&#x27;</span>,<br>         <span class="hljs-string">&#x27;当日，在北京举行的2019年国际篮联篮球世界杯半决赛中，西班牙队对阵澳大利亚队。&#x27;</span>,<br>         ]<br><br>t1 = time.time()<br><span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>    data = &#123;<span class="hljs-string">&#x27;event&#x27;</span>: text.replace(<span class="hljs-string">&#x27; &#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)&#125;<br>    req = requests.post(url, data)<br>    <span class="hljs-keyword">if</span> req.status_code == <span class="hljs-number">200</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;原文：%s&#x27;</span> % text)<br>        res = json.loads(req.content)[<span class="hljs-string">&#x27;entities&#x27;</span>]<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;抽取结果：%s&#x27;</span> % <span class="hljs-built_in">str</span>([_[<span class="hljs-string">&#x27;word&#x27;</span>] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> res]))<br><br><br>t2 = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;一共耗时：%ss.&#x27;</span> % <span class="hljs-built_in">str</span>(<span class="hljs-built_in">round</span>(t2-t1, <span class="hljs-number">4</span>)))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs prolog">原文：记者从国家发展改革委、商务部相关方面获悉，日前美方已决定对拟于<span class="hljs-number">10</span>月<span class="hljs-number">1</span>日实施的中国输美商品加征关税措施做出调整，中方支持相关企业从即日起按照市场化原则和<span class="hljs-symbol">WTO</span>规则，自美采购一定数量大豆、猪肉等农产品，国务院关税税则委员会将对上述采购予以加征关税排除。<br>抽取结果：[<span class="hljs-string">&#x27;日前&#x27;</span>, <span class="hljs-string">&#x27;10月1日&#x27;</span>]<br>原文：据印度<span class="hljs-symbol">Zee</span>新闻网站<span class="hljs-number">12</span>日报道，亚洲新闻国际通讯社援引印度军方消息人士的话说，<span class="hljs-number">9</span>月<span class="hljs-number">11</span>日的对峙事件发生在靠近班公错北岸的实际控制线一带。<br>抽取结果：[<span class="hljs-string">&#x27;12日&#x27;</span>, <span class="hljs-string">&#x27;9月11日&#x27;</span>]<br>原文：儋州市决定，从<span class="hljs-number">9</span>月开始，对城市低保、农村低保、特困供养人员、优抚对象、领取失业保险金人员、建档立卡未脱贫人口等低收入群体共<span class="hljs-number">3</span>万多人，发放猪肉价格补贴，每人每月发放不低于<span class="hljs-number">100</span>元补贴，以后发放标准，将根据猪肉价波动情况进行动态调整。<br>抽取结果：[<span class="hljs-string">&#x27;9月&#x27;</span>]<br>原文：<span class="hljs-number">9</span>月<span class="hljs-number">11</span>日，华为心声社区发布美国经济学家托马斯.弗里德曼在《纽约时报》上的专栏内容，弗里德曼透露，在与华为创始人任正非最近一次采访中，任正非表示华为愿意与美国司法部展开话题不设限的讨论。<br>抽取结果：[<span class="hljs-string">&#x27;9月11日&#x27;</span>]<br>原文：造血干细胞移植治疗白血病技术已日益成熟，然而，通过该方法同时治愈艾滋病目前还是一道全球尚在攻克的难题。<br>抽取结果：[]<br>原文：英国航空事故调查局（<span class="hljs-symbol">AAIB</span>）近日披露，今年<span class="hljs-number">2</span>月<span class="hljs-number">6</span>日一趟由德国法兰克福飞往墨西哥坎昆的航班上，因飞行员打翻咖啡使操作面板冒烟，导致飞机折返迫降爱尔兰。<br>抽取结果：[<span class="hljs-string">&#x27;近日&#x27;</span>, <span class="hljs-string">&#x27;今年2月6日&#x27;</span>]<br>原文：当地时间周四（<span class="hljs-number">9</span>月<span class="hljs-number">12</span>日），印度尼西亚财政部长英卓华（<span class="hljs-symbol">Sri</span> <span class="hljs-symbol">Mulyani</span> <span class="hljs-symbol">Indrawati</span>）明确表示：特朗普的推特是风险之一。<br>抽取结果：[<span class="hljs-string">&#x27;当地时间周四（9月12日）&#x27;</span>]<br>原文：华中科技大学<span class="hljs-number">9</span>月<span class="hljs-number">12</span>日通过其官方网站发布通报称，<span class="hljs-number">9</span>月<span class="hljs-number">2</span>日，我校一硕士研究生不幸坠楼身亡。<br>抽取结果：[<span class="hljs-string">&#x27;9月12日&#x27;</span>, <span class="hljs-string">&#x27;9月2日&#x27;</span>]<br>原文：微博用户@ooooviki <span class="hljs-number">9</span>月<span class="hljs-number">12</span>日下午公布发生在自己身上的惊悚遭遇：一个自称网警、名叫郑洋的人利用职务之便，查到她的完备的个人信息，包括但不限于身份证号、家庭地址、电话号码、户籍变动情况等，要求她做他女朋友。<br>抽取结果：[<span class="hljs-string">&#x27;9月12日下午&#x27;</span>]<br>原文：今天，贵阳取消了汽车限购，成为目前全国实行限购政策的<span class="hljs-number">9</span>个省市中，首个取消限购的城市。<br>抽取结果：[<span class="hljs-string">&#x27;今天&#x27;</span>, <span class="hljs-string">&#x27;目前&#x27;</span>]<br>原文：据悉，与全球同步，中国区此次将于<span class="hljs-number">9</span>月<span class="hljs-number">13</span>日于iPhone官方渠道和京东正式开启预售，京东成<span class="hljs-symbol">Apple</span>中国区唯一官方授权预售渠道。<br>抽取结果：[<span class="hljs-string">&#x27;9月13日&#x27;</span>]<br>原文：根据央行公布的数据，截至<span class="hljs-number">2019</span>年<span class="hljs-number">6</span>月末，存款类金融机构住户部门短期消费贷款规模为<span class="hljs-number">9.11</span>万亿元，<span class="hljs-number">2019</span>年上半年该项净增<span class="hljs-number">3293.19</span>亿元，上半年增量看起来并不乐观。<br>抽取结果：[<span class="hljs-string">&#x27;2019年6月末&#x27;</span>, <span class="hljs-string">&#x27;2019年上半年&#x27;</span>, <span class="hljs-string">&#x27;上半年&#x27;</span>]<br>原文：<span class="hljs-number">9</span>月<span class="hljs-number">11</span>日，一段拍摄浙江万里学院学生食堂的视频走红网络，视频显示该学校食堂不仅在用餐区域设置了可以看电影、比赛的大屏幕，还推出了“一人食”餐位。<br>抽取结果：[<span class="hljs-string">&#x27;9月11日&#x27;</span>]<br>原文：当日，在北京举行的<span class="hljs-number">2019</span>年国际篮联篮球世界杯半决赛中，西班牙队对阵澳大利亚队。<br>抽取结果：[<span class="hljs-string">&#x27;当日&#x27;</span>, <span class="hljs-string">&#x27;2019年&#x27;</span>]<br>一共耗时：<span class="hljs-number">0.5314</span>s.<br></code></pre></td></tr></table></figure><p>可以看到，对于测试的14个句子，识别的准确率很高，且预测耗时为531ms，平均每个话的预测时间不超过40ms。相比较而言，文章<ahref="https://percent4.github.io/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%E5%88%A9%E7%94%A8tensorflow-serving%E9%83%A8%E7%BD%B2kashgari%E6%A8%A1%E5%9E%8B/">NLP（十七）利用tensorflow-serving部署kashgari模型</a>中的模型，该模型的预测时间为每句话1秒多，模型预测的速度为带ALBERT模型的25倍多。</p><p>因此，ALBERT模型确实提升了模型预测的时间，而且效果非常显著。</p><h3 id="总结">总结</h3><p>由于ALBERT开源不到一周，而且笔者的学识、才能有限，因此，在代码方面可能会存在不足。但是，作为一次使用ALBERT的历经，希望能够与大家分享。</p><p>本文绝不是上述项目代码的抄袭和堆砌，该项目融入了笔者自己的思考，希望不要被误解为是抄袭。笔者使用上述的bertNER和ALBERT，只是为了验证ALBERT在模型预测耗时方面的提速效果，而事实是，ALBERT确实给我带来了很大惊喜，感受源代码作者们～</p><p>最后，附上本文中笔者项目的Github地址：<ahref="https://github.com/percent4/ALBERT_4_Time_Recognition">https://github.com/percent4/ALBERT_4_Time_Recognition</a>。</p><p>众里寻他千百度。蓦然回首，那人却在，灯火阑珊处。</p><h3 id="参考文献">参考文献</h3><ol type="1"><li>超小型BERT中文版横空出世！模型只有16M，训练速度提升10倍：https://mp.weixin.qq.com/s/eVlNpejrxdE4ctDTBM-fiA</li><li>ALBERT的Github地址：https://github.com/brightmart/albert_zh</li><li>bertNER项目的Github地址：https://github.com/yumath/bertNER</li><li>NLP（十七）利用tensorflow-serving部署kashgari模型：https://www.cnblogs.com/jclian91/p/11526547.html</li></ol>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>ALBERT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（十七）利用tensorflow-serving部署kashgari模型</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%E5%88%A9%E7%94%A8tensorflow-serving%E9%83%A8%E7%BD%B2kashgari%E6%A8%A1%E5%9E%8B/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89%E5%88%A9%E7%94%A8tensorflow-serving%E9%83%A8%E7%BD%B2kashgari%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<p>在文章<ahref="https://percent4.github.io/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%E8%AE%A9%E6%A8%A1%E5%9E%8B%E6%9D%A5%E5%91%8A%E8%AF%89%E4%BD%A0%E6%96%87%E6%9C%AC%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4/">NLP（十五）让模型来告诉你文本中的时间</a>中，我们已经学会了如何利用kashgari模块来完成序列标注模型的训练与预测，在本文中，我们将会了解如何tensorflow-serving来部署模型。</p><p>在kashgari的官方文档中，已经有如何利用tensorflow-serving来部署模型的说明了，网址为：<ahref="https://kashgari.bmio.net/advance-use/tensorflow-serving/">https://kashgari.bmio.net/advance-use/tensorflow-serving/</a>。</p><p>下面，本文将介绍tensorflow-serving以及如何利用tensorflow-serving来部署kashgari的模型。</p><h3 id="tensorflow-serving">tensorflow-serving</h3><p>TensorFlow Serving 是一个用于机器学习模型 serving的高性能开源库。它可以将训练好的机器学习模型部署到线上，使用 gRPC作为接口接受外部调用。更加让人眼前一亮的是，它支持模型热更新与自动模型版本管理。这意味着一旦部署TensorFlow Serving后，你再也不需要为线上服务操心，只需要关心你的线下模型训练。</p><p>TensorFlowServing可以方便我们部署TensorFlow模型，本文将使用TensorFlowServing的Docker镜像来使用TensorFlow Serving，安装的命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">docker pull tensorflow/serving<br></code></pre></td></tr></table></figure><h3 id="工程实践">工程实践</h3><p>本项目将演示如何利用tensorflow/serving来部署kashgari中的模型，项目结构如下：</p><p><img src="/img/nlp17_1.png" /></p><p>本项目的data来自之前笔者标注的时间数据集，即标注出文本中的时间，采用BIO标注系统。chinese_wwm_ext文件夹为哈工大的预训练模型文件。</p><p>model_train.py为模型训练的代码，主要功能是完成时间序列标注模型的训练，完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># time: 2019-09-12</span><br><span class="hljs-comment"># place: Huangcun Beijing</span><br><br><span class="hljs-keyword">import</span> kashgari<br><span class="hljs-keyword">from</span> kashgari <span class="hljs-keyword">import</span> utils<br><span class="hljs-keyword">from</span> kashgari.corpus <span class="hljs-keyword">import</span> DataReader<br><span class="hljs-keyword">from</span> kashgari.embeddings <span class="hljs-keyword">import</span> BERTEmbedding<br><span class="hljs-keyword">from</span> kashgari.tasks.labeling <span class="hljs-keyword">import</span> BiLSTM_CRF_Model<br><br><span class="hljs-comment"># 模型训练</span><br><br>train_x, train_y = DataReader().read_conll_format_file(<span class="hljs-string">&#x27;./data/time.train&#x27;</span>)<br>valid_x, valid_y = DataReader().read_conll_format_file(<span class="hljs-string">&#x27;./data/time.dev&#x27;</span>)<br>test_x, test_y = DataReader().read_conll_format_file(<span class="hljs-string">&#x27;./data/time.test&#x27;</span>)<br><br>bert_embedding = BERTEmbedding(<span class="hljs-string">&#x27;chinese_wwm_ext_L-12_H-768_A-12&#x27;</span>,<br>                               task=kashgari.LABELING,<br>                               sequence_length=<span class="hljs-number">128</span>)<br><br>model = BiLSTM_CRF_Model(bert_embedding)<br><br>model.fit(train_x, train_y, valid_x, valid_y, batch_size=<span class="hljs-number">16</span>, epochs=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># Save model</span><br>utils.convert_to_saved_model(model,<br>                             model_path=<span class="hljs-string">&#x27;saved_model/time_entity&#x27;</span>,<br>                             version=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>运行该代码，模型训练完后会生成saved_model文件夹，里面含有模型训练好后的文件，方便我们利用tensorflow/serving进行部署。接着我们利用tensorflow/serving来完成模型的部署，命令如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs routeros">docker <span class="hljs-built_in">run</span> -t --rm -p 8501:8501 -v <span class="hljs-string">&quot;/Users/jclian/PycharmProjects/kashgari_tf_serving/saved_model:/models/&quot;</span> -e <span class="hljs-attribute">MODEL_NAME</span>=time_entity tensorflow/serving<br></code></pre></td></tr></table></figure><p>其中需要注意该模型所在的路径，路径需要写完整路径，以及模型的名称（MODEL_NAME），这在训练代码（train.py）中已经给出（saved_model/time_entity）。</p><p>接着我们使用tornado来搭建HTTP服务，帮助我们方便地进行模型预测，runServer.py的完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> kashgari <span class="hljs-keyword">import</span> utils<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> model_predict <span class="hljs-keyword">import</span> get_predict<br><br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> tornado.httpserver<br><span class="hljs-keyword">import</span> tornado.ioloop<br><span class="hljs-keyword">import</span> tornado.options<br><span class="hljs-keyword">import</span> tornado.web<br><span class="hljs-keyword">from</span> tornado.options <span class="hljs-keyword">import</span> define, options<br><span class="hljs-keyword">import</span> traceback<br><br><span class="hljs-comment"># tornado高并发</span><br><span class="hljs-keyword">import</span> tornado.web<br><span class="hljs-keyword">import</span> tornado.gen<br><span class="hljs-keyword">import</span> tornado.concurrent<br><span class="hljs-keyword">from</span> concurrent.futures <span class="hljs-keyword">import</span> ThreadPoolExecutor<br><br><span class="hljs-comment"># 定义端口为12333</span><br>define(<span class="hljs-string">&quot;port&quot;</span>, default=<span class="hljs-number">16016</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;run on the given port&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br><br><span class="hljs-comment"># 模型预测</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ModelPredictHandler</span>(tornado.web.RequestHandler):<br>    executor = ThreadPoolExecutor(max_workers=<span class="hljs-number">5</span>)<br><br>    <span class="hljs-comment"># get 函数</span><br><span class="hljs-meta">    @tornado.gen.coroutine</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self</span>):<br>        origin_text = self.get_argument(<span class="hljs-string">&#x27;text&#x27;</span>)<br>        result = <span class="hljs-keyword">yield</span> self.function(origin_text)<br>        self.write(json.dumps(result, ensure_ascii=<span class="hljs-literal">False</span>))<br><br><span class="hljs-meta">    @tornado.concurrent.run_on_executor</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">function</span>(<span class="hljs-params">self, text</span>):<br>        <span class="hljs-keyword">try</span>:<br>            text = text.replace(<span class="hljs-string">&#x27; &#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>            x = [_ <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> text]<br><br>            <span class="hljs-comment"># Pre-processor data</span><br>            processor = utils.load_processor(model_path=<span class="hljs-string">&#x27;saved_model/time_entity/1&#x27;</span>)<br>            tensor = processor.process_x_dataset([x])<br><br>            <span class="hljs-comment"># only for bert Embedding</span><br>            tensor = [&#123;<br>                <span class="hljs-string">&quot;Input-Token:0&quot;</span>: i.tolist(),<br>                <span class="hljs-string">&quot;Input-Segment:0&quot;</span>: np.zeros(i.shape).tolist()<br>            &#125; <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tensor]<br><br>            <span class="hljs-comment"># predict</span><br>            r = requests.post(<span class="hljs-string">&quot;http://localhost:8501/v1/models/time_entity:predict&quot;</span>, json=&#123;<span class="hljs-string">&quot;instances&quot;</span>: tensor&#125;)<br>            preds = r.json()[<span class="hljs-string">&#x27;predictions&#x27;</span>]<br><br>            <span class="hljs-comment"># Convert result back to labels</span><br>            labels = processor.reverse_numerize_label_sequences(np.array(preds).argmax(-<span class="hljs-number">1</span>))<br><br>            entities = get_predict(<span class="hljs-string">&#x27;TIME&#x27;</span>, text, labels[<span class="hljs-number">0</span>])<br><br>            <span class="hljs-keyword">return</span> entities<br><br>        <span class="hljs-keyword">except</span> Exception:<br>            self.write(traceback.format_exc().replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&lt;br&gt;&#x27;</span>))<br><br><br><span class="hljs-comment"># get请求</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">HelloHandler</span>(tornado.web.RequestHandler):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self</span>):<br>        self.write(<span class="hljs-string">&#x27;Hello from lmj from Daxing Beijing!&#x27;</span>)<br><br><br><span class="hljs-comment"># 主函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    <span class="hljs-comment"># 开启tornado服务</span><br>    tornado.options.parse_command_line()<br>    <span class="hljs-comment"># 定义app</span><br>    app = tornado.web.Application(<br>            handlers=[(<span class="hljs-string">r&#x27;/model_predict&#x27;</span>, ModelPredictHandler),<br>                      (<span class="hljs-string">r&#x27;/hello&#x27;</span>, HelloHandler),<br>                      ], <span class="hljs-comment">#网页路径控制</span><br>          )<br>    http_server = tornado.httpserver.HTTPServer(app)<br>    http_server.listen(options.port)<br>    tornado.ioloop.IOLoop.instance().start()<br><br>main()<br></code></pre></td></tr></table></figure><p>我们定义了tornado封装HTTP服务来进行模型预测，运行该脚本，启动模型预测的HTTP服务。接着我们再使用Python脚本才测试下模型的预测效果以及预测时间，预测的代码脚本的完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> requests<br><br>t1 = time.time()<br>texts = [<span class="hljs-string">&#x27;记者从国家发展改革委、商务部相关方面获悉，日前美方已决定对拟于10月1日实施的中国输美商品加征关税措施做出调整，中方支持相关企业从即日起按照市场化原则和WTO规则，自美采购一定数量大豆、猪肉等农产品，国务院关税税则委员会将对上述采购予以加征关税排除。&#x27;</span>,<br>         <span class="hljs-string">&#x27;据印度Zee新闻网站12日报道，亚洲新闻国际通讯社援引印度军方消息人士的话说，9月11日的对峙事件发生在靠近班公错北岸的实际控制线一带。&#x27;</span>,<br>         <span class="hljs-string">&#x27;儋州市决定，从9月开始，对城市低保、农村低保、特困供养人员、优抚对象、领取失业保险金人员、建档立卡未脱贫人口等低收入群体共3万多人，发放猪肉价格补贴，每人每月发放不低于100元补贴，以后发放标准，将根据猪肉价波动情况进行动态调整。&#x27;</span>,<br>         <span class="hljs-string">&#x27;9月11日，华为心声社区发布美国经济学家托马斯.弗里德曼在《纽约时报》上的专栏内容，弗里德曼透露，在与华为创始人任正非最近一次采访中，任正非表示华为愿意与美国司法部展开话题不设限的讨论。&#x27;</span>,<br>         <span class="hljs-string">&#x27;造血干细胞移植治疗白血病技术已日益成熟，然而，通过该方法同时治愈艾滋病目前还是一道全球尚在攻克的难题。&#x27;</span>,<br>         <span class="hljs-string">&#x27;英国航空事故调查局（AAIB）近日披露，今年2月6日一趟由德国法兰克福飞往墨西哥坎昆的航班上，因飞行员打翻咖啡使操作面板冒烟，导致飞机折返迫降爱尔兰。&#x27;</span>,<br>         <span class="hljs-string">&#x27;当地时间周四（9月12日），印度尼西亚财政部长英卓华（Sri Mulyani Indrawati）明确表示：特朗普的推特是风险之一。&#x27;</span>,<br>         <span class="hljs-string">&#x27;华中科技大学9月12日通过其官方网站发布通报称，9月2日，我校一硕士研究生不幸坠楼身亡。&#x27;</span>,<br>         <span class="hljs-string">&#x27;微博用户@ooooviki 9月12日下午公布发生在自己身上的惊悚遭遇：一个自称网警、名叫郑洋的人利用职务之便，查到她的完备的个人信息，包括但不限于身份证号、家庭地址、电话号码、户籍变动情况等，要求她做他女朋友。&#x27;</span>,<br>         <span class="hljs-string">&#x27;今天，贵阳取消了汽车限购，成为目前全国实行限购政策的9个省市中，首个取消限购的城市。&#x27;</span>,<br>         <span class="hljs-string">&#x27;据悉，与全球同步，中国区此次将于9月13日于iPhone官方渠道和京东正式开启预售，京东成Apple中国区唯一官方授权预售渠道。&#x27;</span>,<br>         <span class="hljs-string">&#x27;根据央行公布的数据，截至2019年6月末，存款类金融机构住户部门短期消费贷款规模为9.11万亿元，2019年上半年该项净增3293.19亿元，上半年增量看起来并不乐观。&#x27;</span>,<br>         <span class="hljs-string">&#x27;9月11日，一段拍摄浙江万里学院学生食堂的视频走红网络，视频显示该学校食堂不仅在用餐区域设置了可以看电影、比赛的大屏幕，还推出了“一人食”餐位。&#x27;</span>,<br>         <span class="hljs-string">&#x27;当日，在北京举行的2019年国际篮联篮球世界杯半决赛中，西班牙队对阵澳大利亚队。&#x27;</span>,<br>         ]<br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(texts))<br><br><span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>    url = <span class="hljs-string">&#x27;http://localhost:16016/model_predict?text=%s&#x27;</span> % text<br>    req = requests.get(url)<br>    <span class="hljs-built_in">print</span>(json.loads(req.content))<br><br>t2 = time.time()<br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">round</span>(t2-t1, <span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure><p>运行该代码，输出的结果如下：（预测文本中的时间）</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs prolog">一共预测<span class="hljs-number">14</span>个句子。<br>[<span class="hljs-string">&#x27;日前&#x27;</span>, <span class="hljs-string">&#x27;10月1日&#x27;</span>, <span class="hljs-string">&#x27;即日&#x27;</span>]<br>[<span class="hljs-string">&#x27;12日&#x27;</span>, <span class="hljs-string">&#x27;9月11日&#x27;</span>]<br>[<span class="hljs-string">&#x27;9月&#x27;</span>]<br>[<span class="hljs-string">&#x27;9月11日&#x27;</span>]<br>[]<br>[<span class="hljs-string">&#x27;近日&#x27;</span>, <span class="hljs-string">&#x27;今年2月6日&#x27;</span>]<br>[<span class="hljs-string">&#x27;当地时间周四（9月12日）&#x27;</span>]<br>[<span class="hljs-string">&#x27;9月12日&#x27;</span>, <span class="hljs-string">&#x27;9月2日&#x27;</span>]<br>[<span class="hljs-string">&#x27;9月12日下午&#x27;</span>]<br>[<span class="hljs-string">&#x27;今天&#x27;</span>, <span class="hljs-string">&#x27;目前&#x27;</span>]<br>[<span class="hljs-string">&#x27;9月13日&#x27;</span>]<br>[<span class="hljs-string">&#x27;2019年6月末&#x27;</span>, <span class="hljs-string">&#x27;2019年上半年&#x27;</span>, <span class="hljs-string">&#x27;上半年&#x27;</span>]<br>[<span class="hljs-string">&#x27;9月11日&#x27;</span>]<br>[<span class="hljs-string">&#x27;当日&#x27;</span>, <span class="hljs-string">&#x27;2019年&#x27;</span>]<br>预测耗时: <span class="hljs-number">15.1085</span>s.<br></code></pre></td></tr></table></figure><p>模型预测的效果还是不错的，但平均每句话的预测时间为1秒多，模型预测时间还是稍微偏长，后续笔者将会研究如何缩短模型预测的时间。</p><h3 id="总结">总结</h3><p>本项目主要是介绍了如何利用tensorflow-serving部署kashgari模型，该项目已经上传至github，地址为：<ahref="https://github.com/percent4/tensorflow-serving_4_kashgari">https://github.com/percent4/tensorflow-serving_4_kashgari</a>。</p><p>至于如何缩短模型预测的时间，笔者还需要再继续研究，欢迎大家关注～</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（十六）轻松上手文本分类</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%E8%BD%BB%E6%9D%BE%E4%B8%8A%E6%89%8B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89%E8%BD%BB%E6%9D%BE%E4%B8%8A%E6%89%8B%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h3 id="背景介绍">背景介绍</h3><p>文本分类是NLP中的常见的重要任务之一，它的主要功能就是将输入的文本以及文本的类别训练出一个模型，使之具有一定的泛化能力，能够对新文本进行较好地预测。它的应用很广泛，在很多领域发挥着重要作用，例如垃圾邮件过滤、舆情分析以及新闻分类等。</p><p>现阶段的文本分类模型频出，种类繁多，花样百变，既有机器学习中的朴素贝叶斯模型、SVM等，也有深度学习中的各种模型，比如经典的CNN,RNN，以及它们的变形，如CNN-LSTM，还有各种高大上的Attention模型。</p><p>无疑，文本分类是一个相对比较成熟的任务，我们尽可以选择自己喜欢的模型来完成该任务。本文以kashgari-tf为例，它能够支持各种文本分类模型，比如BiLSTM，CNN_LSTM，AVCNN等，且对预训练模型，比如BERT的支持较好，它能让我们轻松地完成文本分类任务。</p><p>下面，让我们一起走进文本分类的世界，分分钟搞定textclassification！</p><h3 id="项目">项目</h3><p>首先，我们需要找一份数据作为例子。我们选择THUCNews，THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19GB），均为UTF-8纯文本格式。我们在原始新浪新闻分类体系的基础上，从中选择10个候选分类类别：体育、娱乐、家居、房产、教育、时尚、时政、游戏、科技、财经。</p><p>数据总量一共为6.5万条，其中训练集数据5万条，每个类别5000条，验证集数据0.5万条，每个类别500条，测试集数据1万条，每个类别1000条。笔者已将数据放在Github上，读者可以在最后的总结中找到。</p><p>项目结构，如下图：</p><p><img src="/img/nlp16_1.png" /></p><p>接着，我们尝试着利用kashgari-tf来训练一个文本分类模型，其中模型我们采用CNN-LSTM，完整的Python代码（text_classification_model_train.py）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># time: 2019-08-13 11:16</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><br><span class="hljs-keyword">from</span> kashgari.tasks.classification <span class="hljs-keyword">import</span> CNN_LSTM_Model<br><br><span class="hljs-comment"># 获取数据集</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>(<span class="hljs-params">data_type</span>):<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./data/cnews.%s.txt&#x27;</span> % data_type, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        content = [_.strip() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> f.readlines() <span class="hljs-keyword">if</span> _.strip()]<br><br>    x, y = [], []<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> content:<br>        label, text = line.split(maxsplit=<span class="hljs-number">1</span>)<br>        y.append(label)<br>        x.append([_ <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> text])<br><br>    <span class="hljs-keyword">return</span> x, y<br><br><span class="hljs-comment"># 获取数据</span><br>train_x, train_y = load_data(<span class="hljs-string">&#x27;train&#x27;</span>)<br>valid_x, valid_y = load_data(<span class="hljs-string">&#x27;val&#x27;</span>)<br>test_x, test_y = load_data(<span class="hljs-string">&#x27;test&#x27;</span>)<br><br><span class="hljs-comment"># 训练模型</span><br>model = CNN_LSTM_Model()<br>model.fit(train_x, train_y, valid_x, valid_y, batch_size=<span class="hljs-number">16</span>, epochs=<span class="hljs-number">5</span>)<br><br><span class="hljs-comment"># 评估模型</span><br>model.evaluate(test_x, test_y)<br><br><span class="hljs-comment"># 保存模型</span><br>model.save(<span class="hljs-string">&#x27;text_classification_model&#x27;</span>)<br></code></pre></td></tr></table></figure><p>输出的模型结果如下：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs markdown"><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_<br><span class="hljs-section">Layer (type)                 Output Shape              Param #</span><br><span class="hljs-section">=================================================================</span><br>input (InputLayer)           (None, 2544)              0<br><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_<br>layer<span class="hljs-emphasis">_embedding (Embedding)  (None, 2544, 100)         553200</span><br><span class="hljs-emphasis"><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_</span><br>conv1d (Conv1D)              (None, 2544, 32)          9632<br><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_<br>max<span class="hljs-emphasis">_pooling1d (MaxPooling1D) (None, 1272, 32)          0</span><br><span class="hljs-emphasis"><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_</span><br>cu<span class="hljs-emphasis">_dnnlstm (CuDNNLSTM)       (None, 100)               53600</span><br><span class="hljs-emphasis"><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span><span class="hljs-strong">____</span>_</span><br><span class="hljs-section">dense (Dense)                (None, 10)                1010</span><br><span class="hljs-section">=================================================================</span><br>Total params: 617,442<br>Trainable params: 617,442<br>Non-trainable params: 0<br></code></pre></td></tr></table></figure><p>设定模型训练次数为5个epoch，batch_size为16。模型训练完后，在训练集、验证集上的结果如下：</p><table><thead><tr class="header"><th>数据集</th><th>accuracy</th><th>loss</th></tr></thead><tbody><tr class="odd"><td>训练集</td><td>0.9661</td><td>0.1184</td></tr><tr class="even"><td>验证集</td><td>0.9204</td><td>0.2567</td></tr></tbody></table><p>在测试集上的结果如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs yaml">             <span class="hljs-string">precision</span>    <span class="hljs-string">recall</span>  <span class="hljs-string">f1-score</span>   <span class="hljs-string">support</span><br><br>          <span class="hljs-string">体育</span>     <span class="hljs-number">0.9852</span>    <span class="hljs-number">0.9970</span>    <span class="hljs-number">0.9911</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">娱乐</span>     <span class="hljs-number">0.9938</span>    <span class="hljs-number">0.9690</span>    <span class="hljs-number">0.9813</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">家居</span>     <span class="hljs-number">0.9384</span>    <span class="hljs-number">0.8830</span>    <span class="hljs-number">0.9098</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">房产</span>     <span class="hljs-number">0.9490</span>    <span class="hljs-number">0.9680</span>    <span class="hljs-number">0.9584</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">教育</span>     <span class="hljs-number">0.9650</span>    <span class="hljs-number">0.8820</span>    <span class="hljs-number">0.9216</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">时尚</span>     <span class="hljs-number">0.9418</span>    <span class="hljs-number">0.9710</span>    <span class="hljs-number">0.9562</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">时政</span>     <span class="hljs-number">0.9732</span>    <span class="hljs-number">0.9450</span>    <span class="hljs-number">0.9589</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">游戏</span>     <span class="hljs-number">0.9454</span>    <span class="hljs-number">0.9700</span>    <span class="hljs-number">0.9576</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">科技</span>     <span class="hljs-number">0.8910</span>    <span class="hljs-number">0.9560</span>    <span class="hljs-number">0.9223</span>      <span class="hljs-number">1000</span><br>          <span class="hljs-string">财经</span>     <span class="hljs-number">0.9566</span>    <span class="hljs-number">0.9920</span>    <span class="hljs-number">0.9740</span>      <span class="hljs-number">1000</span><br><br>    <span class="hljs-string">accuracy</span>                         <span class="hljs-number">0.9533</span>     <span class="hljs-number">10000</span><br>   <span class="hljs-string">macro</span> <span class="hljs-string">avg</span>     <span class="hljs-number">0.9539</span>    <span class="hljs-number">0.9533</span>    <span class="hljs-number">0.9531</span>     <span class="hljs-number">10000</span><br><span class="hljs-string">weighted</span> <span class="hljs-string">avg</span>     <span class="hljs-number">0.9539</span>    <span class="hljs-number">0.9533</span>    <span class="hljs-number">0.9531</span>     <span class="hljs-number">10000</span><br></code></pre></td></tr></table></figure><p>总的来说，上述模型训练的效果还是很不错的。接下来，是考验模型的预测能力的时刻了，看看它是否具体文本分类的泛化能力。</p><h3 id="测试">测试</h3><p>我们已经有了训练好的模型<code>text_classification_model</code>，接着让我们利用该模型来对新的数据进行预测，预测的代码（model_predict.py）如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># time: 2019-08-14 00:21</span><br><span class="hljs-comment"># place: Pudong Shanghai</span><br><br><span class="hljs-keyword">import</span> kashgari<br><br><span class="hljs-comment"># 加载模型</span><br>loaded_model = kashgari.utils.load_model(<span class="hljs-string">&#x27;text_classification_model&#x27;</span>)<br><br>text = <span class="hljs-string">&#x27;华夏幸福成立于 1998 年，前身为廊坊市华夏房地产开发有限公司，初始注册资本 200 万元，其中王文学出资 160 万元，廊坊市融通物资贸易有限公司出资 40 万元，后经多次股权转让和增资，公司于 2007 年整体改制为股份制公司，2011 年完成借壳上市。&#x27;</span><br><br>x = [[_ <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> text]]<br><br>label = loaded_model.predict(x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;预测分类:%s&#x27;</span> % label)<br></code></pre></td></tr></table></figure><p>以下是测试结果：</p><blockquote><p>原文1: 华夏幸福成立于 1998年，前身为廊坊市华夏房地产开发有限公司，初始注册资本 200万元，其中王文学出资 160 万元，廊坊市融通物资贸易有限公司出资 40万元，后经多次股权转让和增资，公司于 2007 年整体改制为股份制公司，2011年完成借壳上市。 分类结果：预测分类:['财经']</p></blockquote><blockquote><p>原文2:现今常见的短袖衬衫大致上可以分为：夏威夷衬衫、古巴衬衫、保龄球衫，三者之间虽有些微分别，但其实有些时候，一件衬衫也可能包含了多种款式的特色。而‘古巴（领）衬衫’最显而易见的特点在于‘领口’，通常会设计为V领，且呈现微微的外翻，也因此缺少衬衫领口常见的‘第一颗钮扣’，衣服到领子的剪裁为一体成形，整体较宽松舒适。分类结果：预测分类:['时尚']</p></blockquote><blockquote><p>原文3:周琦2014年加盟新疆广汇篮球俱乐部，当年就代表俱乐部青年队接连拿下全国篮球青年联赛冠军和全国俱乐部青年联赛冠军。升入一队后，周琦2016年随队出战第25届亚冠杯，获得冠军。2016-2017赛季，周琦为新疆广汇队夺得队史首座总冠军奖杯立下汗马功劳，他在总决赛中带伤出战，更是传为佳话。分类结果：预测分类:['体育']</p></blockquote><blockquote><p>原文4:周杰伦[微博]监制赛车电影《叱咤风云》13日释出花絮导演篇，不仅真实赛车竞速画面大量曝光，几十辆百万赛车在国际专业赛道、山路飙速，场面浩大震撼，更揭开不少现场拍摄的幕后画面。监制周杰伦在现场与导演讨论剧本、范逸臣[微博]与高英轩大打出手、甚至有眼尖网友发现在花絮中闪过“男神”李玉玺[微博]的画面。分类结果：预测分类:['娱乐']</p></blockquote><blockquote><p>原文5:北京时间8月13日上午消息，据《韩国先驱报》网站报道，近日美国知识产权所有者协会（Intellectual Property Owners Association）发布的一份报告显示，在获得的美国专利数量方面，IBM、微软和通用电气等美国企业名列前茅，排在后面的韩国科技巨头三星、LG与之竞争激烈。分类结果：预测分类:['科技']</p></blockquote><h3 id="总结">总结</h3><p>虽然我们上述测试的文本分类效果还不错，但也存在着一些分类错误的情况。</p><p>本文讲述了如何利用kashgari-tf模块来快速地搭建文本分类任务，其实，也没那么难！</p><p>本文代码和数据及已上传至Github, 网址为： <ahref="https://github.com/percent4/cnews_text_classification">https://github.com/percent4/cnews_text_classification</a></p><blockquote><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape），欢迎大家关注~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>Kashgari</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（十五）让模型来告诉你文本中的时间</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%E8%AE%A9%E6%A8%A1%E5%9E%8B%E6%9D%A5%E5%91%8A%E8%AF%89%E4%BD%A0%E6%96%87%E6%9C%AC%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89%E8%AE%A9%E6%A8%A1%E5%9E%8B%E6%9D%A5%E5%91%8A%E8%AF%89%E4%BD%A0%E6%96%87%E6%9C%AC%E4%B8%AD%E7%9A%84%E6%97%B6%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<h3 id="背景介绍">背景介绍</h3><p>在文章<a href="https://percent4.github.io/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%E4%BB%8E%E6%96%87%E6%9C%AC%E4%B8%AD%E6%8F%90%E5%8F%96%E6%97%B6%E9%97%B4/">NLP入门（十一）从文本中提取时间</a> 中，笔者演示了如何利用分词、词性标注的方法从文本中获取时间。当时的想法比较简单快捷，只是利用了词性标注这个功能而已，因此，在某些地方，时间的识别效果并不太好。比如以下的两个例子：</p><p>原文1:</p><blockquote><p>苏北大量农村住房建于上世纪80年代之前。去年9月，江苏省决定全面改善苏北农民住房条件，计划3年内改善30万户，作为决胜全面建成小康社会补短板的重要举措。</p></blockquote><p>用笔者之前的代码，提取的时间结果为：</p><blockquote><p>提取时间： [‘去年9月’]</p></blockquote><p>但实际上，我们提取的时间应该是：</p><blockquote><p>上世纪80年代之前， 去年9月，3年内</p></blockquote><p>原文2:</p><blockquote><p>南宋绍兴十年，金分兵两路向陕西和河南大举进攻，在很快夺回了河南、陕西之后，又率大军向淮南大举进攻。</p></blockquote><p>用笔者之前的代码，提取的时间结果为：</p><blockquote><p>提取时间： [‘南宋’]</p></blockquote><p>但实际上，我们提取的时间应该是：</p><blockquote><p>南宋绍兴十年</p></blockquote><p>因此，利用简单的词性标注功能来提取文本中的时间会存在漏提、错提的情况，鉴于此，笔者想到能否用深度学习模型来实现文本中的时间提取呢？</p><p>该功能类似于命名实体识别（NER）功能，只不过NER是识别文本中的人名、地名、组织机构名，而我们这次需要识别文本中的时间。但是，它们背后的算法原理都是一样的，即采用序列标注模型来解决。</p><h3 id="项目">项目</h3><p>在文章<a href="https://percent4.github.io/2023/07/08/NLP%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%E8%87%AA%E5%88%B6%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E5%B9%B3%E5%8F%B0/">NLP（十四）自制序列标注平台</a>中，笔者提出了一种自制的序列标注平台，利用该标注平台，笔者从新闻网站中标注了大约2000份语料，标注出文本中的时间，其中75%作为训练集（time.train文件），10%作为验证集（time.dev文件），15%作为测试集（time.test文件）。</p><p>虽然我们现在已经有了深度学习框架方便我们来训练模型，比如TensorFlow, Keras, PyTorch等，但目前已有某大神开源了一个序列标注和文本分类的模块，名称为kashgari-tf，它能够方便快速地用几行命令就可以训练一个序列标注或文本分类的模型，容易上手，而且集中了多种模型（BiGRU，CNN， BiLSTM，CRF）以及多种预训练模型（BERT，ERNIE，wwm-ext），对于用户来说算是十分友好了。该模块的参考网址为：<a href="https://kashgari.bmio.net/">https://kashgari.bmio.net/</a> 。</p><p>笔者自己花了几天的时间来标注数据，目前已累计标注2000+数据 ，后续将放到Github供大家参考。我们训练的数据，比如time.train的前几行如下：（每一行中间用空格隔开）</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs mathematica"><span class="hljs-number">1</span> <span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">6</span> <span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">0</span> <span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">9</span> <span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>年 <span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>， <span class="hljs-built_in">O</span><br>日 <span class="hljs-built_in">O</span><br>本 <span class="hljs-built_in">O</span><br>萨 <span class="hljs-built_in">O</span><br>摩 <span class="hljs-built_in">O</span><br>藩 <span class="hljs-built_in">O</span><br>入 <span class="hljs-built_in">O</span><br>侵 <span class="hljs-built_in">O</span><br>琉 <span class="hljs-built_in">O</span><br>球 <span class="hljs-built_in">O</span><br>国 <span class="hljs-built_in">O</span><br>， <span class="hljs-built_in">O</span><br>并 <span class="hljs-built_in">O</span><br>在 <span class="hljs-built_in">O</span><br>一 <span class="hljs-built_in">O</span><br>个 <span class="hljs-built_in">O</span><br>时 <span class="hljs-built_in">O</span><br>期 <span class="hljs-built_in">O</span><br>内 <span class="hljs-built_in">O</span><br>控 <span class="hljs-built_in">O</span><br>制 <span class="hljs-built_in">O</span><br>琉 <span class="hljs-built_in">O</span><br>球 <span class="hljs-built_in">O</span><br>国 <span class="hljs-built_in">O</span><br><span class="hljs-operator">...</span><br></code></pre></td></tr></table></figure><p>接着是模型这块，我们采用经典的BERT+Bi-LSTM+CRF模型，训练1个epoch，batch_size为16，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># time: 2019-08-09 16:47</span><br><span class="hljs-comment"># place: Zhichunlu Beijing</span><br><br><span class="hljs-keyword">import</span> kashgari<br><span class="hljs-keyword">from</span> kashgari.corpus <span class="hljs-keyword">import</span> DataReader<br><span class="hljs-keyword">from</span> kashgari.embeddings <span class="hljs-keyword">import</span> BERTEmbedding<br><span class="hljs-keyword">from</span> kashgari.tasks.labeling <span class="hljs-keyword">import</span> BiLSTM_CRF_Model<br><br>train_x, train_y = DataReader().read_conll_format_file(<span class="hljs-string">&#x27;./data/time.train&#x27;</span>)<br>valid_x, valid_y = DataReader().read_conll_format_file(<span class="hljs-string">&#x27;./data/time.dev&#x27;</span>)<br>test_x, test_y = DataReader().read_conll_format_file(<span class="hljs-string">&#x27;./data/time.test&#x27;</span>)<br><br>bert_embedding = BERTEmbedding(<span class="hljs-string">&#x27;chinese_L-12_H-768_A-12&#x27;</span>,<br>                               task=kashgari.LABELING,<br>                               sequence_length=<span class="hljs-number">128</span>)<br><br>model = BiLSTM_CRF_Model(bert_embedding)<br>model.fit(train_x, train_y, valid_x, valid_y, batch_size=<span class="hljs-number">16</span>, epochs=<span class="hljs-number">1</span>)<br><br>model.save(<span class="hljs-string">&#x27;time_ner.h5&#x27;</span>)<br><br>model.evaluate(test_x, test_y)<br></code></pre></td></tr></table></figure><p>模型训练完后，得到的效果如下：</p><table><thead><tr><th>数据集</th><th>accuracy</th><th>loss</th></tr></thead><tbody><tr><td>训练集</td><td>0.9814</td><td>6.7295</td></tr><tr><td>验证集</td><td>0.6868</td><td>150.8513</td></tr></tbody></table><p>在测试集上的结果如下：</p><table><thead><tr><th>数据集</th><th>precision</th><th>recall</th><th>f1</th></tr></thead><tbody><tr><td>测试集</td><td>0.8547</td><td>0.8934</td><td>0.8736</td></tr></tbody></table><p>由于是小标注量，因此我们选择了用BERT预训练模型。如果不采用BERT预训练模型，在同样的数据集上，即使训练100个epoch，虽然在训练集上的准确率超过95%，但是在测试集上却只有大约50%的准确率，效果不行，因此，需要采用预训练模型。</p><h3 id="测试效果">测试效果</h3><p>在训练完模型后，会在当前目录下生成time_ner.h5模型文件，接着我们需要该模型文件来对新的文件进行预测，提取出文本中的时间。模型预测的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Load saved model</span><br><span class="hljs-keyword">import</span> kashgari<br><br>loaded_model = kashgari.utils.load_model(<span class="hljs-string">&#x27;time_ner.h5&#x27;</span>)<br><br><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>    text = <span class="hljs-built_in">input</span>(<span class="hljs-string">&#x27;sentence: &#x27;</span>)<br>    t = loaded_model.predict([[char <span class="hljs-keyword">for</span> char <span class="hljs-keyword">in</span> text]])<br>    <span class="hljs-built_in">print</span>(t)<br></code></pre></td></tr></table></figure><p>接着我们在几条新的数据上进行预测，看看该模型的表现效果：</p><blockquote><p>“原文”: “绿地控股2018年年度年报显示，截至2018年12月31日，万科金域中央项目的经营状态为“住宅、办公、商业”，项目用地面积18.90万平方米，规划计容建筑面积79.38万平方米，总建筑面积为105.78万平方米，已竣工面积32.90万平方米，总投资额95亿元，报告期实际投资额为10.18亿元。”,<br>“预测时间”: [<br>“2018年年度”,<br>“2018年12月31日”<br>]</p></blockquote><blockquote><p>“原文”: “经过工作人员两天的反复验证、严密测算，记者昨天从上海中心大厦得到确认：被誉为上海中心大厦“定楼神器”的阻尼器，在8月10日出现自2016年正式启用以来的最大摆幅。”,<br>“预测时间”: [<br>“两天”,<br>“昨天”,<br>“8月10日”,<br>“2016年”<br>]</p></blockquote><blockquote><p>“原文”: “不幸的是，在升任内史的同年九月，狄仁杰就在洛阳私宅离世。”,<br>“预测时间”: [<br>“同年九月”<br>]</p></blockquote><blockquote><p>“原文”: “早上9点25分到达北京火车站，火车站在北京市区哦，地铁很方便到达酒店，我们定了王府井大街的锦江之星，409元一晚，有点小贵。下午去了天坛公园，傍晚去了天安门广场。”,<br>“预测时间”: [<br>“早上9点25分”,<br>“下午”,<br>“傍晚”<br>],</p></blockquote><h3 id="总结">总结</h3><p>利用深度学习模型，在小标注量数据上，我们对时间识别取得了不错的效果。后续如果我们想要提高时间识别的准确率，可以再多增加标注数据，目前还只有2000+数据～</p><p>本项目已经开源，Github的地址为：<a href="https://github.com/percent4/Chinese_Time_Recogniztion">https://github.com/percent4/Chinese_Time_Recogniztion</a> 。</p><p>另外，强烈推荐kashgari-tf模块，它能够让你在几分钟内搭建一个序列标注模型，而且方便加载各种预训练模型。</p><blockquote><p>注意：不妨了解下笔者的微信公众号： NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（十四）自制序列标注平台</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%E8%87%AA%E5%88%B6%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E5%B9%B3%E5%8F%B0/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89%E8%87%AA%E5%88%B6%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E5%B9%B3%E5%8F%B0/</url>
    
    <content type="html"><![CDATA[<h3 id="背景介绍">背景介绍</h3><p>在平时的NLP任务中，我们经常用到命名实体识别（NER），常用的识别实体类型为人名、地名、组织机构名，但是我们往往也会有识别其它实体的需求，比如时间、品牌名等。在利用算法做实体识别的时候，我们一般采用序列标注算法，这就对标注的文本格式有一定的要求，因此，一个好的序列标注的平台必不可少，将会大大减少我们标注的工作量，有效提升算法的更新迭代速度。</p><p>本文将介绍笔者的一个工作：自制的序列标注平台。我们以时间识别为例。比如，在下面的文章中：</p><blockquote><p>按计划，2019年8月10日，荣耀智慧屏将在华为开发者大会上正式亮相，在8月6日，荣耀官微表示该产品的预约量已破十万台，8月7日下午，荣耀总裁赵明又在微博上造势率先打出差异化牌，智慧屏没有开关机广告，并表态以后也不会有，消费者体验至上，营销一波接一波，可谓来势汹汹。</p></blockquote><p>我们需要从该文章中标注出三个时间：<code>2019年8月10日</code>，<code>8月6日</code>，<code>8月7日下午</code>，并形成标注序列。</p><p>下面将详细介绍笔者的工作。</p><h3 id="序列标注平台">序列标注平台</h3><p>由于开发时间仓促以及笔者能力有限，因此，序列标注平台的功能还没有很完善，希望笔者的工作能抛砖引玉。</p><p>项目的结构图如下：</p><p><img src="/img/nlp14_1.png" /></p><p>templates中存放静态资源，time_index.html为平台的操作界面，time_output为平台标注完实体后的文件保存路径，time_server.py是用tornado写的服务端路径控制代码，utils.py中是获取某个路径下的txt文件的最大数值的函数。</p><p>其中，utils.py的完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># time: 2019-03-14</span><br><span class="hljs-comment"># place: Xinbeiqiao, Beijing</span><br><br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 获取当前所在目录的txt文本的最大数值</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_max_num</span>(<span class="hljs-params">path</span>):<br>    files = os.listdir(path)<br>    <span class="hljs-keyword">if</span> files:<br>        numbers = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">int</span>(x.replace(<span class="hljs-string">&#x27;.txt&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)), files))<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">max</span>(numbers)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>time_server.py的完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># time: 2019-08-08</span><br><span class="hljs-comment"># place: Xinbeiqiao, Beijing</span><br><br><span class="hljs-keyword">import</span> os.path<br><span class="hljs-keyword">import</span> tornado.httpserver<br><span class="hljs-keyword">import</span> tornado.ioloop<br><span class="hljs-keyword">import</span> tornado.options<br><span class="hljs-keyword">import</span> tornado.web<br><span class="hljs-keyword">from</span> tornado.options <span class="hljs-keyword">import</span> define, options<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> get_max_num<br><br><span class="hljs-comment">#定义端口为9005</span><br>define(<span class="hljs-string">&quot;port&quot;</span>, default=<span class="hljs-number">9005</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;run on the given port&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>)<br><br><span class="hljs-comment"># GET请求</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">QueryHandler</span>(tornado.web.RequestHandler):<br>    <span class="hljs-comment"># get函数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get</span>(<span class="hljs-params">self</span>):<br>        self.render(<span class="hljs-string">&#x27;time_index.html&#x27;</span>, data = [<span class="hljs-string">&#x27;&#x27;</span>, []])<br><br><span class="hljs-comment"># POST请求</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PostHandler</span>(tornado.web.RequestHandler):<br>    <span class="hljs-comment"># post函数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">post</span>(<span class="hljs-params">self</span>):<br><br>        <span class="hljs-comment"># 获取前端参数, event, time, index</span><br>        event = self.get_argument(<span class="hljs-string">&#x27;event&#x27;</span>)<br>        times = self.get_arguments(<span class="hljs-string">&#x27;time&#x27;</span>)<br>        indices = self.get_arguments(<span class="hljs-string">&#x27;index&#x27;</span>)<br>        <span class="hljs-built_in">print</span>(event)<br>        <span class="hljs-built_in">print</span>(times)<br>        <span class="hljs-built_in">print</span>(indices)<br><br>        <span class="hljs-comment"># 前端显示序列标注信息</span><br>        tags = [<span class="hljs-string">&#x27;O&#x27;</span>] * <span class="hljs-built_in">len</span>(event)<br><br>        <span class="hljs-keyword">for</span> time, index <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(times, indices):<br>            index = <span class="hljs-built_in">int</span>(index)<br>            tags[index] = <span class="hljs-string">&#x27;B-TIME&#x27;</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(time)):<br>                tags[index+i] = <span class="hljs-string">&#x27;I-TIME&#x27;</span><br><br>        data = [event, tags]<br><br>        self.render(<span class="hljs-string">&#x27;time_index.html&#x27;</span>, data=data)<br><br>        <span class="hljs-comment"># 保存为txt文件</span><br>        dir_path = <span class="hljs-string">&#x27;./time_output&#x27;</span><br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./%s/%s.txt&#x27;</span> % (dir_path, get_max_num(dir_path)+<span class="hljs-number">1</span>), <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            <span class="hljs-keyword">for</span> char, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(event, tags):<br>                f.write(char+<span class="hljs-string">&#x27;\t&#x27;</span>+tag+<span class="hljs-string">&#x27;\n&#x27;</span>)<br><br><br><span class="hljs-comment"># 主函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    <span class="hljs-comment"># 开启tornado服务</span><br>    tornado.options.parse_command_line()<br>    <span class="hljs-comment"># 定义app</span><br>    app = tornado.web.Application(<br>            handlers=[(<span class="hljs-string">r&#x27;/query&#x27;</span>, QueryHandler),<br>                      (<span class="hljs-string">r&#x27;/result&#x27;</span>, PostHandler)<br>                      ], <span class="hljs-comment">#网页路径控制</span><br>            template_path=os.path.join(os.path.dirname(__file__), <span class="hljs-string">&quot;templates&quot;</span>) <span class="hljs-comment"># 模板路径</span><br>          )<br>    http_server = tornado.httpserver.HTTPServer(app)<br>    http_server.listen(options.port)<br>    tornado.ioloop.IOLoop.instance().start()<br><br>main()<br></code></pre></td></tr></table></figure><p>time_index.html文件如下：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-meta">&lt;!DOCTYPE <span class="hljs-keyword">html</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">html</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">head</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">meta</span> <span class="hljs-attr">charset</span>=<span class="hljs-string">&quot;utf-8&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">title</span>&gt;</span>时间抽取标注平台<span class="hljs-tag">&lt;/<span class="hljs-name">title</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">link</span> <span class="hljs-attr">rel</span>=<span class="hljs-string">&quot;stylesheet&quot;</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;https://cdn.staticfile.org/twitter-bootstrap/3.3.7/css/bootstrap.min.css&quot;</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;https://cdn.staticfile.org/twitter-bootstrap/3.3.7/js/bootstrap.min.js&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">style</span>&gt;</span><span class="language-css"></span><br><span class="language-css">        <span class="hljs-selector-tag">mark</span> &#123;</span><br><span class="language-css">            <span class="hljs-attribute">background-color</span>:<span class="hljs-number">#00ff90</span>; <span class="hljs-attribute">font-weight</span>:bold;</span><br><span class="language-css">        &#125;</span><br><span class="language-css"><span class="hljs-selector-tag">p</span>&#123;<span class="hljs-attribute">text-indent</span>:<span class="hljs-number">2em</span>;&#125;</span><br><span class="language-css">    </span><span class="hljs-tag">&lt;/<span class="hljs-name">style</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">script</span>&gt;</span><span class="language-javascript"></span><br><span class="language-javascript">        <span class="hljs-keyword">var</span> click_cnt = <span class="hljs-number">0</span>;</span><br><span class="language-javascript"></span><br><span class="language-javascript">        <span class="hljs-comment">// 双击第i个select, 添加文字的index</span></span><br><span class="language-javascript">        <span class="hljs-keyword">function</span> <span class="hljs-title function_">select_click</span>(<span class="hljs-params">i</span>)&#123;</span><br><span class="language-javascript">        <span class="hljs-keyword">var</span> content = <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">getElementById</span>(<span class="hljs-string">&#x27;event&#x27;</span>).<span class="hljs-property">value</span>;</span><br><span class="language-javascript">        <span class="hljs-keyword">var</span> time = <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">getElementById</span>(<span class="hljs-string">&#x27;time_&#x27;</span>+i.<span class="hljs-title function_">toString</span>()).<span class="hljs-property">value</span>;</span><br><span class="language-javascript"></span><br><span class="language-javascript">        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">var</span> j=<span class="hljs-number">0</span>; j&lt;=content.<span class="hljs-property">length</span>-time.<span class="hljs-property">length</span>; j++)&#123;</span><br><span class="language-javascript">        <span class="hljs-keyword">if</span>(content.<span class="hljs-title function_">substr</span>(j, time.<span class="hljs-property">length</span>) == time)&#123;</span><br><span class="language-javascript">        <span class="hljs-keyword">var</span> select = <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">getElementById</span>(<span class="hljs-string">&#x27;index_&#x27;</span>+i.<span class="hljs-title function_">toString</span>());</span><br><span class="language-javascript">        <span class="hljs-keyword">var</span> option = <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">createElement</span>(<span class="hljs-string">&quot;option&quot;</span>);</span><br><span class="language-javascript">        option.<span class="hljs-property">value</span> = j;</span><br><span class="language-javascript">        option.<span class="hljs-property">innerHTML</span> = j;</span><br><span class="language-javascript">        select.<span class="hljs-title function_">appendChild</span>(option);</span><br><span class="language-javascript">        &#125;</span><br><span class="language-javascript">        &#125;</span><br><span class="language-javascript">        &#125;</span><br><span class="language-javascript"></span><br><span class="language-javascript"><span class="hljs-comment">// 添加输入框和select框</span></span><br><span class="language-javascript">        $(<span class="hljs-variable language_">document</span>).<span class="hljs-title function_">ready</span>(<span class="hljs-keyword">function</span>(<span class="hljs-params"></span>)&#123;</span><br><span class="language-javascript"></span><br><span class="language-javascript">            $(<span class="hljs-string">&quot;#add_time&quot;</span>).<span class="hljs-title function_">click</span>(<span class="hljs-keyword">function</span>(<span class="hljs-params"></span>)&#123;</span><br><span class="language-javascript">                 click_cnt = click_cnt + <span class="hljs-number">1</span>;</span><br><span class="language-javascript">                 <span class="hljs-keyword">var</span> input_id = <span class="hljs-keyword">new</span> <span class="hljs-title class_">String</span>(<span class="hljs-string">&#x27;time_&#x27;</span>+click_cnt.<span class="hljs-title function_">toString</span>());</span><br><span class="language-javascript">                 <span class="hljs-keyword">var</span> index_id = <span class="hljs-keyword">new</span> <span class="hljs-title class_">String</span>(<span class="hljs-string">&#x27;index_&#x27;</span>+click_cnt.<span class="hljs-title function_">toString</span>());</span><br><span class="language-javascript">                 <span class="hljs-keyword">var</span> content = <span class="hljs-string">&quot;&lt;input type=&#x27;text&#x27; id=&quot;</span> + input_id + <span class="hljs-string">&quot; class=&#x27;form-control&#x27; style=&#x27;width:306px;&#x27; name=&#x27;time&#x27; /&gt; \</span></span><br><span class="hljs-string"><span class="language-javascript">                     &lt;select class=&#x27;form-control&#x27; name=&#x27;index&#x27; id=&quot;</span>+ index_id + <span class="hljs-string">&quot; style=&#x27;width:120px;&#x27; \</span></span><br><span class="hljs-string"><span class="language-javascript">                 ondblclick=&#x27;select_click(&quot;</span>+click_cnt.<span class="hljs-title function_">toString</span>()+<span class="hljs-string">&quot;)&#x27;&gt;&lt;/select&gt;&quot;</span>;</span><br><span class="language-javascript">                 $(content).<span class="hljs-title function_">appendTo</span>($(<span class="hljs-string">&quot;#time_column&quot;</span>));</span><br><span class="language-javascript">            &#125;);</span><br><span class="language-javascript"></span><br><span class="language-javascript">        &#125;);</span><br><span class="language-javascript"></span><br><span class="language-javascript"></span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">head</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">body</span>&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">center</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">br</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">br</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">br</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">form</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-horizontal&quot;</span> <span class="hljs-attr">role</span>=<span class="hljs-string">&quot;form&quot;</span> <span class="hljs-attr">method</span>=<span class="hljs-string">&quot;post&quot;</span> <span class="hljs-attr">action</span>=<span class="hljs-string">&quot;/result&quot;</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;width:600px&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-group&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">label</span> <span class="hljs-attr">for</span>=<span class="hljs-string">&quot;event&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;col-sm-2 control-label&quot;</span>&gt;</span>输入语料<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;col-sm-10&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">textarea</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;text&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-control&quot;</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;event&quot;</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;width:490px; height:200px&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;event&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">textarea</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-inline&quot;</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;text-align:left;&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">label</span> <span class="hljs-attr">for</span>=<span class="hljs-string">&quot;time_0&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;col-sm-2 control-label&quot;</span>&gt;</span>时间<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;col-sm-10&quot;</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;time_column&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;text&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-control&quot;</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;time_0&quot;</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;width:306px;&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;time&quot;</span> /&gt;</span><br>               <br>            <span class="hljs-tag">&lt;<span class="hljs-name">select</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-control&quot;</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;index_0&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;index&quot;</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;width:120px;&quot;</span> <span class="hljs-attr">ondblclick</span>=<span class="hljs-string">&quot;select_click(0)&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">select</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-group&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;col-sm-offset-2 col-sm-10&quot;</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">br</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">button</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;button&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-default&quot;</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;add_time&quot;</span>&gt;</span>添加时间<span class="hljs-tag">&lt;/<span class="hljs-name">button</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">button</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;submit&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-success&quot;</span>&gt;</span>显示标签<span class="hljs-tag">&lt;/<span class="hljs-name">button</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;/query&quot;</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">button</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;button&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-danger&quot;</span>&gt;</span>返回<span class="hljs-tag">&lt;/<span class="hljs-name">button</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">button</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;reset&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-warning&quot;</span>&gt;</span>重置<span class="hljs-tag">&lt;/<span class="hljs-name">button</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><br><span class="hljs-tag">&lt;/<span class="hljs-name">form</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">br</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;width:600px&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">p</span>&gt;</span> 原文：&#123;&#123;data[0]&#125;&#125; <span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">table</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;table table-striped&quot;</span>&gt;</span><br>&#123;% for char, tag in zip(data[0], data[1]) %&#125;<br><span class="hljs-tag">&lt;<span class="hljs-name">tr</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">td</span>&gt;</span>&#123;&#123;char&#125;&#125; <span class="hljs-tag">&lt;/<span class="hljs-name">td</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">td</span>&gt;</span>&#123;&#123;tag&#125;&#125; <span class="hljs-tag">&lt;/<span class="hljs-name">td</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">tr</span>&gt;</span><br>&#123;%end%&#125;<br><span class="hljs-tag">&lt;/<span class="hljs-name">table</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">center</span>&gt;</span><br><br><span class="hljs-tag">&lt;/<span class="hljs-name">body</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">html</span>&gt;</span><br></code></pre></td></tr></table></figure><h3 id="平台使用">平台使用</h3><p>运行上述time_server.py后，在浏览器端输入网址: <ahref="http://localhost:9005/query">http://localhost:9005/query</a> ,则会显示如下界面：</p><p><img src="/img/nlp14_2.png" /></p><p>在<code>输入语料框</code>中，我们输入语料：</p><blockquote><p>8月8日是“全民健身日”，推出重磅微视频《我们要赢的，是自己》。</p></blockquote><p>在时间这个输入框中，可以标注语料中的时间，同时双击同一行中的下拉列表，就能显示该标注时间在语料中的起始位置，有时候同样的标注时间会在语料中出现多次，那么我们在下拉列表中选择我们需要的标注的起始位置即可。</p><p>点击<code>添加时间</code>按钮，它会增加一行标注，允许我们在同一份预料中标注多个时间。我们的一个简单的标注例子如下：</p><p><img src="/img/nlp14_3.png" /></p><p>点击<code>显示标注</code>，则会显示我们标注完后形成的序列标注信息，同时将该序列信息保存为txt文件，该txt文件位于time_output目录下。在网页上的序列标注信息如下：</p><p><img src="/img/nlp14_4.png" /></p><p>同时，我们也可以查看保存的txt文档信息，如下：</p><p><img src="/img/nlp14_5.png" /></p><p>点击<code>返回</code>按钮，它会允许我们进行下一次的标注。刚才展示的只是一个简单例子，稍微复杂的标注如下图：</p><p><img src="/img/nlp14_6.png" /></p><p>它形成的标注序列(部分)如下：</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs mathematica">按<span class="hljs-built_in">O</span><br>计<span class="hljs-built_in">O</span><br>划<span class="hljs-built_in">O</span><br>，<span class="hljs-built_in">O</span><br><span class="hljs-number">2</span><span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">0</span><span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">1</span><span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">9</span><span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>年<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">8</span><span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>月<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">1</span><span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">0</span><span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>日<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>，<span class="hljs-built_in">O</span><br>荣<span class="hljs-built_in">O</span><br>耀<span class="hljs-built_in">O</span><br>智<span class="hljs-built_in">O</span><br>慧<span class="hljs-built_in">O</span><br>屏<span class="hljs-built_in">O</span><br>将<span class="hljs-built_in">O</span><br>在<span class="hljs-built_in">O</span><br>华<span class="hljs-built_in">O</span><br>为<span class="hljs-built_in">O</span><br>开<span class="hljs-built_in">O</span><br>发<span class="hljs-built_in">O</span><br>者<span class="hljs-built_in">O</span><br>大<span class="hljs-built_in">O</span><br>会<span class="hljs-built_in">O</span><br>上<span class="hljs-built_in">O</span><br>正<span class="hljs-built_in">O</span><br>式<span class="hljs-built_in">O</span><br>亮<span class="hljs-built_in">O</span><br>相<span class="hljs-built_in">O</span><br>，<span class="hljs-built_in">O</span><br>在<span class="hljs-built_in">O</span><br><span class="hljs-number">8</span><span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>月<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br><span class="hljs-number">6</span><span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>日<span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">TIME</span><br>，<span class="hljs-built_in">O</span><br>荣<span class="hljs-built_in">O</span><br>耀<span class="hljs-built_in">O</span><br>官<span class="hljs-built_in">O</span><br>微<span class="hljs-built_in">O</span><br>表<span class="hljs-built_in">O</span><br>示<span class="hljs-built_in">O</span><br>该<span class="hljs-built_in">O</span><br>产<span class="hljs-built_in">O</span><br>品<span class="hljs-built_in">O</span><br><span class="hljs-operator">......</span><br></code></pre></td></tr></table></figure><h3 id="总结">总结</h3><p>本平台仅作为序列标注算法的前期标注工具使用，并不涉及具体的算法。另外，后续该平台也会陆续开放出来，如果大家有好的建议，也可以留言～</p><p>本项目已上传只Github, 网址为： <ahref="https://github.com/percent4/entity_tagging_platform">https://github.com/percent4/entity_tagging_platform</a></p><blockquote><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>标注平台</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（十三）中文分词工具的使用尝试</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8%E5%B0%9D%E8%AF%95/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8%E5%B0%9D%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<p>本文将对三种中文分词工具进行使用尝试，这三种工具分别为哈工大的LTP，结巴分词以及北大的pkuseg。</p><p>首先我们先准备好环境，即需要安装三个模块：pyltp, jieba,pkuseg以及LTP的分词模型文件<code>cws.model</code>。在用户字典中添加以下5个词语：</p><blockquote><p>经 少安 贺凤英 F-35战斗机 埃达尔·阿勒坎</p></blockquote><p>测试的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">import</span> pkuseg<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor<br><br>lexicon = [<span class="hljs-string">&#x27;经&#x27;</span>, <span class="hljs-string">&#x27;少安&#x27;</span>, <span class="hljs-string">&#x27;贺凤英&#x27;</span>, <span class="hljs-string">&#x27;F-35战斗机&#x27;</span>, <span class="hljs-string">&#x27;埃达尔·阿勒坎&#x27;</span>] <span class="hljs-comment"># 自定义词典</span><br><br><span class="hljs-comment"># 哈工大LTP分词</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">ltp_segment</span>(<span class="hljs-params">sent</span>):<br>    <span class="hljs-comment"># 加载文件</span><br>    cws_model_path = os.path.join(<span class="hljs-string">&#x27;data/cws.model&#x27;</span>) <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>    lexicon_path = os.path.join(<span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>) <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br>    segmentor = Segmentor()<br>    segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br>    words = <span class="hljs-built_in">list</span>(segmentor.segment(sent))<br>    segmentor.release()<br><br>    <span class="hljs-keyword">return</span> words<br><br><span class="hljs-comment"># 结巴分词</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">jieba_cut</span>(<span class="hljs-params">sent</span>):<br>    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> lexicon:<br>        jieba.add_word(word)<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">list</span>(jieba.cut(sent))<br><br><span class="hljs-comment"># pkuseg分词</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">pkuseg_cut</span>(<span class="hljs-params">sent</span>):<br>    seg = pkuseg.pkuseg(user_dict=lexicon)<br>    words = seg.cut(sent)<br>    <span class="hljs-keyword">return</span> words<br><br>sent = <span class="hljs-string">&#x27;尽管玉亭成家以后，他老婆贺凤英那些年把少安妈欺负上一回又一回，怕老婆的玉亭连一声也不敢吭，但少安他妈不计较他。&#x27;</span><br><span class="hljs-comment">#sent = &#x27;据此前报道，以色列于去年5月成为世界上第一个在实战中使用F-35战斗机的国家。&#x27;</span><br><span class="hljs-comment">#sent = &#x27;小船4月8日经长江前往小鸟岛。&#x27;</span><br><span class="hljs-comment">#sent = &#x27;1958年，埃达尔·阿勒坎出生在土耳其首都安卡拉，但他的求学生涯多在美国度过。&#x27;</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;ltp:&#x27;</span>, ltp_segment(sent))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;jieba:&#x27;</span>, jieba_cut(sent))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;pkuseg:&#x27;</span>, pkuseg_cut(sent))<br></code></pre></td></tr></table></figure><p>&amp;emsp 对于第一句话，输出结果如下：</p><blockquote><p>原文:尽管玉亭成家以后，他老婆贺凤英那些年把少安妈欺负上一回又一回，怕老婆的玉亭连一声也不敢吭，但少安他妈不计较他。</p></blockquote><blockquote><p>ltp: ['尽管', '玉亭', '成家', '以后', '，', '他', '老婆', '贺凤英','那些', '年', '把', '少安', '妈', '欺负', '上', '一', '回', '又', '一','回', '，', '怕', '老婆', '的', '玉亭', '连', '一', '声', '也', '不','敢', '吭', '，', '但', '少安', '他妈', '不', '计较', '他', '。']</p></blockquote><blockquote><p>jieba: ['尽管', '玉亭', '成家', '以后', '，', '他', '老婆', '贺凤英','那些', '年', '把', '少安', '妈', '欺负', '上', '一回', '又', '一回','，', '怕老婆', '的', '玉亭', '连', '一声', '也', '不敢', '吭', '，','但少安', '他妈', '不', '计较', '他', '。']</p></blockquote><blockquote><p>pkuseg: ['尽管', '玉亭', '成家', '以后', '，', '他', '老婆','贺凤英', '那些', '年', '把', '少安', '妈', '欺负', '上', '一', '回','又', '一', '回', '，', '怕', '老婆', '的', '玉亭', '连', '一', '声','也', '不', '敢', '吭', '，', '但', '少安', '他妈', '不', '计较', '他','。']</p></blockquote><p>对于第二句话，输出结果如下：</p><blockquote><p>原文:据此前报道，以色列于去年5月成为世界上第一个在实战中使用F-35战斗机的国家。</p></blockquote><blockquote><p>ltp: ['据', '此前', '报道', '，', '以色列', '于', '去年', '5月','成为', '世界', '上', '第一', '个', '在', '实战', '中', '使用', 'F-35','战斗机', '的', '国家', '。']</p></blockquote><blockquote><p>jieba: ['据此', '前', '报道', '，', '以色列', '于', '去年', '5','月', '成为', '世界', '上', '第一个', '在', '实战', '中', '使用', 'F','-', '35', '战斗机', '的', '国家', '。']</p></blockquote><blockquote><p>pkuseg: ['据', '此前', '报道', '，', '以色列', '于', '去年', '5月','成为', '世界', '上', '第一', '个', '在', '实战', '中', '使用','F-35战斗机', '的', '国家', '。']</p></blockquote><p>对于第三句话，输出结果如下：</p><blockquote><p>原文: 小船4月8日经长江前往小鸟岛。</p></blockquote><blockquote><p>ltp: ['小船', '4月', '8日', '经长江', '前往', '小鸟岛', '。']</p></blockquote><blockquote><p>jieba: ['小船', '4', '月', '8', '日经', '长江', '前往', '小', '鸟岛','。']</p></blockquote><blockquote><p>pkuseg: ['小船', '4月', '8日', '经', '长江', '前往', '小鸟', '岛','。']</p></blockquote><p>对于第四句话，输出结果如下：</p><blockquote><p>原文:1958年，埃达尔·阿勒坎出生在土耳其首都安卡拉，但他的求学生涯多在美国度过。</p></blockquote><blockquote><p>ltp: ['1958年', '，', '埃达尔·阿勒坎', '出生', '在', '土耳其','首都', '安卡拉', '，', '但', '他', '的', '求学', '生涯', '多', '在','美国', '度过', '。']</p></blockquote><blockquote><p>jieba: ['1958', '年', '，', '埃', '达尔', '·', '阿勒', '坎', '出生','在', '土耳其', '首都', '安卡拉', '，', '但', '他', '的', '求学','生涯', '多', '在', '美国', '度过', '。']</p></blockquote><blockquote><p>pkuseg: ['1958年', '，', '埃达尔·阿勒坎', '出生', '在', '土耳其','首都', '安卡拉', '，', '但', '他', '的', '求学', '生涯', '多', '在','美国', '度过', '。']</p></blockquote><p>接着，对以上的测试情况做一个简单的总结：</p><ol type="1"><li><p>用户词典方面：LTP和pkuseg的效果都很好，jieba的表现不尽如人意，这主要是因为自定义的字典的词语里面含有标点符号，关于该问题的解决办法，可以参考网址：<ahref="https://blog.csdn.net/weixin_42471956/article/details/80795534">https://blog.csdn.net/weixin_42471956/article/details/80795534</a></p></li><li><p>从第二句话的效果来看，pkuseg的分词效果应该是最好的，‘经’应该作为单个的词语切分出来，而LTP和jieba即使加了自定义词典，也没有效果，同理，‘F-35战斗机’也是类似的情形。</p></li></ol><p>总的来说，三者的分词效果都很优秀，差距不是很大，但在自定义词典这块，无疑pkuseg的效果更加稳定些。笔者也会在以后的分词使用中多多考虑pkuseg～有关pkuseg的介绍与使用，可以参考网址：<ahref="https://github.com/lancopku/PKUSeg-python">https://github.com/lancopku/PKUSeg-python</a></p><blockquote><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape），欢迎大家关注~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>分词</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP（十二）依存句法分析的可视化及图分析</title>
    <link href="/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E5%8F%8A%E5%9B%BE%E5%88%86%E6%9E%90/"/>
    <url>/2023/07/08/NLP%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E5%8F%8A%E5%9B%BE%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<p>依存句法分析的效果虽然没有像分词、NER的效果来的好，但也有其使用价值，在日常的工作中，我们免不了要和其打交道。笔者这几天一直在想如何分析依存句法分析的结果，一个重要的方面便是其可视化和它的图分析。</p><p>我们使用的NLP工具为jieba和LTP，其中jieba用于分词，LTP用于词性标注和句法分析，需要事件下载<code>pos.model</code>和<code>parser.model</code>文件。</p><p>本文使用的示例句子为：</p><blockquote><p>2018年7月26日，华为创始人任正非向5G极化码（Polar码）之父埃尔达尔教授举行颁奖仪式，表彰其对于通信领域做出的贡献。</p></blockquote><p>首先，让我们来看一下没有可视化效果之前的句法分析结果。Python代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span>  Postagger, Parser<br><br>sent = <span class="hljs-string">&#x27;2018年7月26日，华为创始人任正非向5G极化码（Polar码）之父埃尔达尔教授举行颁奖仪式，表彰其对于通信领域做出的贡献。&#x27;</span><br><br>jieba.add_word(<span class="hljs-string">&#x27;Polar码&#x27;</span>)<br>jieba.add_word(<span class="hljs-string">&#x27;5G极化码&#x27;</span>)<br>jieba.add_word(<span class="hljs-string">&#x27;埃尔达尔&#x27;</span>)<br>jieba.add_word(<span class="hljs-string">&#x27;之父&#x27;</span>)<br>words = <span class="hljs-built_in">list</span>(jieba.cut(sent))<br><br><span class="hljs-built_in">print</span>(words)<br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)<br>postagger = Postagger()<br>postagger.load(pos_model_path)<br>postags = postagger.postag(words)<br><br><span class="hljs-comment"># 依存句法分析</span><br>par_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/parser.model&#x27;</span>)<br>parser = Parser()<br>parser.load(par_model_path)<br>arcs = parser.parse(words, postags)<br><br>rely_id = [arc.head <span class="hljs-keyword">for</span> arc <span class="hljs-keyword">in</span> arcs]  <span class="hljs-comment"># 提取依存父节点id</span><br>relation = [arc.relation <span class="hljs-keyword">for</span> arc <span class="hljs-keyword">in</span> arcs]  <span class="hljs-comment"># 提取依存关系</span><br>heads = [<span class="hljs-string">&#x27;Root&#x27;</span> <span class="hljs-keyword">if</span> <span class="hljs-built_in">id</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> words[<span class="hljs-built_in">id</span>-<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> rely_id]  <span class="hljs-comment"># 匹配依存父节点词语</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(words)):<br>    <span class="hljs-built_in">print</span>(relation[i] + <span class="hljs-string">&#x27;(&#x27;</span> + words[i] + <span class="hljs-string">&#x27;, &#x27;</span> + heads[i] + <span class="hljs-string">&#x27;)&#x27;</span>)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;2018&#x27;</span>, <span class="hljs-string">&#x27;年&#x27;</span>, <span class="hljs-string">&#x27;7&#x27;</span>, <span class="hljs-string">&#x27;月&#x27;</span>, <span class="hljs-string">&#x27;26&#x27;</span>, <span class="hljs-string">&#x27;日&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>, <span class="hljs-string">&#x27;华为&#x27;</span>, <span class="hljs-string">&#x27;创始人&#x27;</span>, <span class="hljs-string">&#x27;任正非&#x27;</span>, <span class="hljs-string">&#x27;向&#x27;</span>, <span class="hljs-string">&#x27;5G极化码&#x27;</span>, <span class="hljs-string">&#x27;（&#x27;</span>, <span class="hljs-string">&#x27;Polar码&#x27;</span>, <span class="hljs-string">&#x27;）&#x27;</span>, <span class="hljs-string">&#x27;之父&#x27;</span>, <span class="hljs-string">&#x27;埃尔达尔&#x27;</span>, <span class="hljs-string">&#x27;教授&#x27;</span>, <span class="hljs-string">&#x27;举行&#x27;</span>, <span class="hljs-string">&#x27;颁奖仪式&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>, <span class="hljs-string">&#x27;表彰&#x27;</span>, <span class="hljs-string">&#x27;其&#x27;</span>, <span class="hljs-string">&#x27;对于&#x27;</span>, <span class="hljs-string">&#x27;通信&#x27;</span>, <span class="hljs-string">&#x27;领域&#x27;</span>, <span class="hljs-string">&#x27;做出&#x27;</span>, <span class="hljs-string">&#x27;的&#x27;</span>, <span class="hljs-string">&#x27;贡献&#x27;</span>, <span class="hljs-string">&#x27;。&#x27;</span>]</span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">2018</span>, 年)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(年, 日)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">7</span>, 月)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(月, 日)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">26</span>, 日)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(日, 举行)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(，, 日)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(华为, 创始人)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(创始人, 任正非)</span></span><br><span class="hljs-function"><span class="hljs-title">SBV</span><span class="hljs-params">(任正非, 举行)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(向, 举行)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">5</span>G极化码, 之父)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(（, Polar码)</span></span><br><span class="hljs-function"><span class="hljs-title">COO</span><span class="hljs-params">(Polar码, <span class="hljs-number">5</span>G极化码)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(）, Polar码)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(之父, 埃尔达尔)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(埃尔达尔, 教授)</span></span><br><span class="hljs-function"><span class="hljs-title">POB</span><span class="hljs-params">(教授, 向)</span></span><br><span class="hljs-function"><span class="hljs-title">HED</span><span class="hljs-params">(举行, Root)</span></span><br><span class="hljs-function"><span class="hljs-title">VOB</span><span class="hljs-params">(颁奖仪式, 举行)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(，, 举行)</span></span><br><span class="hljs-function"><span class="hljs-title">COO</span><span class="hljs-params">(表彰, 举行)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(其, 贡献)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(对于, 做出)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(通信, 领域)</span></span><br><span class="hljs-function"><span class="hljs-title">POB</span><span class="hljs-params">(领域, 对于)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(做出, 贡献)</span></span><br><span class="hljs-function"><span class="hljs-title">RAD</span><span class="hljs-params">(的, 做出)</span></span><br><span class="hljs-function"><span class="hljs-title">VOB</span><span class="hljs-params">(贡献, 表彰)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(。, 举行)</span></span><br></code></pre></td></tr></table></figure><p>我们得到了该句子的依存句法分析的结果，但是其可视化效果却不好。</p><p>我们使用Graphviz工具来得到上述依存句法分析的可视化结果，代码（接上述代码）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> graphviz <span class="hljs-keyword">import</span> Digraph<br><br>g = Digraph(<span class="hljs-string">&#x27;测试图片&#x27;</span>)<br><br>g.node(name=<span class="hljs-string">&#x27;Root&#x27;</span>)<br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>    g.node(name=word)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(words)):<br>    <span class="hljs-keyword">if</span> relation[i] <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;HED&#x27;</span>]:<br>        g.edge(words[i], heads[i], label=relation[i])<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">if</span> heads[i] == <span class="hljs-string">&#x27;Root&#x27;</span>:<br>            g.edge(words[i], <span class="hljs-string">&#x27;Root&#x27;</span>, label=relation[i])<br>        <span class="hljs-keyword">else</span>:<br>            g.edge(heads[i], <span class="hljs-string">&#x27;Root&#x27;</span>, label=relation[i])<br><br>g.view()<br></code></pre></td></tr></table></figure><p>得到的依存句法分析的可视化图片如下：</p><p><img src="/img/nlp12_1.png" /></p><p>在这张图片中，我们有了对依存句法分析结果的直观感觉，效果也非常好，但是遗憾的是，我们并不能对上述可视化结果形成的图（Graph）进行图分析，因为Graphviz仅仅只是一个可视化工具。那么，我们该用什么样的工具来进行图分析呢？</p><p>答案就是NetworkX。以下是笔者对于NetworkX应用于依存句法分析的可视化和图分析的展示，其中图分析展示了两个节点之间的最短路径。示例的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 利用networkx绘制句法分析结果</span><br><span class="hljs-keyword">import</span> networkx <span class="hljs-keyword">as</span> nx<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> pylab <span class="hljs-keyword">import</span> mpl<br><br>mpl.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>] = [<span class="hljs-string">&#x27;Arial Unicode MS&#x27;</span>]  <span class="hljs-comment"># 指定默认字体</span><br><br><br>G = nx.Graph()  <span class="hljs-comment"># 建立无向图G</span><br><br><span class="hljs-comment"># 添加节点</span><br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>    G.add_node(word)<br><br>G.add_node(<span class="hljs-string">&#x27;Root&#x27;</span>)<br><br><span class="hljs-comment"># 添加边</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(words)):<br>    G.add_edge(words[i], heads[i])<br><br>source = <span class="hljs-string">&#x27;5G极化码&#x27;</span><br>target1 = <span class="hljs-string">&#x27;任正非&#x27;</span><br>distance1 = nx.shortest_path_length(G, source=source, target=target1)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#x27;%s&#x27;与&#x27;%s&#x27;在依存句法分析图中的最短距离为:  %s&quot;</span> % (source, target1, distance1))<br><br>target2 = <span class="hljs-string">&#x27;埃尔达尔&#x27;</span><br>distance2 = nx.shortest_path_length(G, source=source, target=target2)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#x27;%s&#x27;与&#x27;%s&#x27;在依存句法分析图中的最短距离为:  %s&quot;</span> % (source, target2, distance2))<br><br>nx.draw(G, with_labels=<span class="hljs-literal">True</span>)<br>plt.savefig(<span class="hljs-string">&quot;undirected_graph.png&quot;</span>)<br></code></pre></td></tr></table></figure><p>得到的可视化图片如下：</p><p><img src="/img/nlp12_2.png" /></p><p>输出的结果如下：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scheme"><span class="hljs-symbol">&#x27;5G极化码</span><span class="hljs-symbol">&#x27;与</span><span class="hljs-symbol">&#x27;任正非</span><span class="hljs-symbol">&#x27;在依存句法分析图中的最短距离为:</span>  <span class="hljs-number">6</span><br><span class="hljs-symbol">&#x27;5G极化码</span><span class="hljs-symbol">&#x27;与</span><span class="hljs-symbol">&#x27;埃尔达尔</span><span class="hljs-symbol">&#x27;在依存句法分析图中的最短距离为:</span>  <span class="hljs-number">2</span><br></code></pre></td></tr></table></figure><p>本次到此结束，希望这篇简短的文章能够给读者带来一些启发～</p><blockquote><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>依存句法分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（十一）从文本中提取时间</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%E4%BB%8E%E6%96%87%E6%9C%AC%E4%B8%AD%E6%8F%90%E5%8F%96%E6%97%B6%E9%97%B4/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89%E4%BB%8E%E6%96%87%E6%9C%AC%E4%B8%AD%E6%8F%90%E5%8F%96%E6%97%B6%E9%97%B4/</url>
    
    <content type="html"><![CDATA[<p>在我们的日常生活和工作中，从文本中提取时间是一项非常基础却重要的工作，因此，本文将介绍如何从文本中有效地提取时间。</p><p>举个简单的例子，我们需要从下面的文本中提取时间：</p><blockquote><p>6月28日，杭州市统计局权威公布《2019年5月月报》，杭州市医保参保人数达到1006万，相比于2月份的989万，三个月暴涨16万人参保，傲视新一线城市。</p></blockquote><p>我们可以从文本有提取<code>6月28日</code>，<code>2019年5月</code>，<code>2月份</code>这三个有效时间。</p><p>通常情况下，较好的解决思路是利用深度学习模型来识别文本中的时间，通过一定数量的标记文本和合适的模型。本文尝试利用现有的NLP工具来解决如何从文本中提取时间。</p><p>本文使用的工具为哈工大的pyltp，可以在Python的第三方模块中找到，实现下载好分词模型<code>cws.model</code>和词性标注<code>pos.model</code>这两个模型文件。</p><p>话不多说，我们直接上Python代码，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Postagger<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LTP</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>        pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br>        self.segmentor = Segmentor()  <span class="hljs-comment"># 初始化实例</span><br>        self.segmentor.load(cws_model_path) <span class="hljs-comment"># 加载模型</span><br>        self.postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>        self.postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br><br>    <span class="hljs-comment"># 分词</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">segment</span>(<span class="hljs-params">self, text</span>):<br>        words = <span class="hljs-built_in">list</span>(self.segmentor.segment(text))<br>        <span class="hljs-keyword">return</span> words<br><br>    <span class="hljs-comment"># 词性标注</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">postag</span>(<span class="hljs-params">self, words</span>):<br>        postags = <span class="hljs-built_in">list</span>(self.postagger.postag(words))<br>        <span class="hljs-keyword">return</span> postags<br><br>    <span class="hljs-comment"># 获取文本中的时间</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_time</span>(<span class="hljs-params">self, text</span>):<br><br>        <span class="hljs-comment"># 开始分词及词性标注</span><br>        words = self.segment(text)<br>        postags = self.postag(words)<br><br>        time_lst = []<br><br>        i = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> tag, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(postags, words):<br>            <span class="hljs-keyword">if</span> tag == <span class="hljs-string">&#x27;nt&#x27;</span>:<br>                j = i<br>                <span class="hljs-keyword">while</span> postags[j] == <span class="hljs-string">&#x27;nt&#x27;</span> <span class="hljs-keyword">or</span> words[j] <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;至&#x27;</span>, <span class="hljs-string">&#x27;到&#x27;</span>]:<br>                    j += <span class="hljs-number">1</span><br>                time_lst.append(<span class="hljs-string">&#x27;&#x27;</span>.join(words[i:j]))<br>            i += <span class="hljs-number">1</span><br><br>        <span class="hljs-comment"># 去重子字符串的情形</span><br>        remove_lst = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> time_lst:<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> time_lst:<br>                <span class="hljs-keyword">if</span> i != j <span class="hljs-keyword">and</span> i <span class="hljs-keyword">in</span> j:<br>                    remove_lst.append(i)<br><br>        text_time_lst = []<br>        <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> time_lst:<br>            <span class="hljs-keyword">if</span> item <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> remove_lst:<br>                text_time_lst.append(item)<br><br>        <span class="hljs-comment"># print(text_time_lst)</span><br>        <span class="hljs-keyword">return</span> text_time_lst<br><br>    <span class="hljs-comment"># 释放模型</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">free_ltp</span>(<span class="hljs-params">self</span>):<br>        self.segmentor.release()<br>        self.postagger.release()<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    ltp = LTP()<br><br>    <span class="hljs-comment"># 输入文本</span><br>    sent = <span class="hljs-string">&#x27;6月28日，杭州市统计局权威公布《2019年5月月报》，杭州市医保参保人数达到1006万，相比于2月份的989万，三个月暴涨16万人参保，傲视新一线城市。&#x27;</span><br>    time_lst = ltp.get_time(sent)<br>    ltp.free_ltp()<br><br>    <span class="hljs-comment"># 输出文本中提取的时间</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;提取时间： %s&#x27;</span> % <span class="hljs-built_in">str</span>(time_lst))<br></code></pre></td></tr></table></figure><p>接着，我们测试几个例子。</p><p>输入文本为：</p><blockquote><p>今天，央行举行了2019年6月份金融统计数据解读吹风会，发布了2019年6月份金融统计数据并就当前的一些热点问题进行了解读和回应。</p></blockquote><p>文本中提取的时间为：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">提取时间： [&#x27;今天&#x27;, &#x27;<span class="hljs-number">2019</span>年6月份&#x27;, &#x27;<span class="hljs-number">2019</span>年6月份&#x27;, &#x27;当前&#x27;]<br></code></pre></td></tr></table></figure><p>输入文本为：</p><blockquote><p>2006年，上海的国内生产总值达到10296.97亿元，是中国内地第一个GDP突破万亿元的城市。2008年，北京GDP破万亿。两年后，广州GDP超过万亿。2011年，深圳、天津、苏州、重庆4城的GDP也进入了万亿行列。武汉、成都在2014年跻身“万亿俱乐部”，杭州、南京和青岛、无锡和长沙的GDP依次在2015年、2016年和2017年过万亿。宁波和郑州则成为2018年万亿俱乐部的新成员。</p></blockquote><p>文本中提取的时间为：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">提取时间： [&#x27;<span class="hljs-number">2006</span>年&#x27;, &#x27;<span class="hljs-number">2008</span>年&#x27;, &#x27;<span class="hljs-number">2011</span>年&#x27;, &#x27;<span class="hljs-number">2014</span>年&#x27;, &#x27;<span class="hljs-number">2015</span>年&#x27;, &#x27;<span class="hljs-number">2016</span>年&#x27;, &#x27;<span class="hljs-number">2018</span>年&#x27;]<br></code></pre></td></tr></table></figure><p>输入文本为：</p><blockquote><p>此后，6月28日、7月9日和7月11日下午，武威市政协、市人大、市政府分别召开坚决全面彻底肃清火荣贵流毒和影响专题民主生活会。</p></blockquote><p>文本中提取的时间为：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">提取时间： <span class="hljs-selector-attr">[<span class="hljs-string">&#x27;此后&#x27;</span>, <span class="hljs-string">&#x27;6月28日&#x27;</span>, <span class="hljs-string">&#x27;7月9日&#x27;</span>, <span class="hljs-string">&#x27;7月11日下午&#x27;</span>]</span><br></code></pre></td></tr></table></figure><p>输入文本为：</p><blockquote><p>姜保红出生于1974年4月，她于2016年11月至2018年9月任武威市副市长，履新时，武威市的一把手正是火荣贵。</p></blockquote><p>文本中提取的时间为：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">提取时间： [&#x27;<span class="hljs-number">1974</span>年4月&#x27;, &#x27;<span class="hljs-number">2016</span>年11月至<span class="hljs-number">2018</span>年9月&#x27;]<br></code></pre></td></tr></table></figure><p>本次分享到此结束，欢迎大家批评指正。</p><blockquote><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BERT的几个可能的应用</title>
    <link href="/2023/07/08/BERT%E7%9A%84%E5%87%A0%E4%B8%AA%E5%8F%AF%E8%83%BD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
    <url>/2023/07/08/BERT%E7%9A%84%E5%87%A0%E4%B8%AA%E5%8F%AF%E8%83%BD%E7%9A%84%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<p>BERT是谷歌公司于2018年11月发布的一款新模型，它一种预训练语言表示的方法，在大量文本语料（维基百科）上训练了一个通用的“语言理解”模型，然后用这个模型去执行想做的NLP任务。一经公布，它便引爆了整个NLP界，其在11个主流NLP任务中都取得优异的结果，因此成为NLP领域最吸引人的一个模型。简单来说，BERT就是在训练了大量的文本语料（无监督）之后，能够在对英语中的单词（或中文的汉字）给出一个向量表示，使得该单词（或汉字）具有一定的语义表示能力，因此，BERT具有一定的先验知识，在NLP任务中表现十分抢眼。</p><p>在文章<ahref="https://blog.csdn.net/Vancl_Wang/article/details/90349047">利用bert-serving-server搭建bert词向量服务(一)</a>中，作者简洁明了地介绍了如何利用bert-serving-server来获取中文汉字的词向量，这大大降低了一般从业者使用BERT的门槛。</p><p>结合笔者这段时间的工作体会以及思考，笔者尝试着给出BERT的几个可能的应用，如下：</p><ul><li>NLP基本任务</li><li>查找相似词语</li><li>提取文本中的实体</li><li>问答中的实体对齐</li></ul><p>由于笔者才疏学浅且撰写文章时间仓促，文章中有不足之处，请读者多多批评指正！</p><h3 id="nlp基本任务">NLP基本任务</h3><p>BERT公布已经半年多了，现在已经成为NLP中的深度学习模型中必不可少的工具，一般会加载在模型中的Embedding层。由于篇幅原因，笔者不再介绍自己的BERT项目，而是介绍几个BERT在基本任务中的Github项目：</p><ul><li>英语文本分类： <strong><ahref="https://github.com/Socialbird-AILab/BERT-Classification-Tutorial">BERT-Classification-Tutorial</a></strong></li><li>中文情感分类： <strong><ahref="https://github.com/renxingkai/BERT_Chinese_Classification">BERT_Chinese_Classification</a></strong></li><li>中文命名实体识别（NER）: <strong><ahref="https://github.com/yumath/bertNER">bertNER</a></strong></li></ul><p>可以看到，BERT已经广泛应用于NLP基本任务中，在开源项目中导出可以见到它的身影，并且这些项目的作者也写了非常细致的代码工程，便于上手。</p><p>在具体讲述下面的三个应用前，我们先了解下BERT应用的项目结构，如下：</p><p><imgsrc="https://img-blog.csdnimg.cn/20190607111211990.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2pjbGlhbjkx,size_16,color_FFFFFF,t_70" /></p><p>其中，bert_client_lmj.py为调用BERT词向量服务，具体可参考文章<ahref="https://blog.csdn.net/Vancl_Wang/article/details/90349047">利用bert-serving-server搭建bert词向量服务(一)</a>，完整的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><span class="hljs-keyword">from</span> bert_serving.client <span class="hljs-keyword">import</span> BertClient<br><span class="hljs-keyword">from</span> sklearn.metrics.pairwise <span class="hljs-keyword">import</span> cosine_similarity<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Encoding</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.server_ip = <span class="hljs-string">&quot;127.0.0.1&quot;</span><br>        self.bert_client = BertClient(ip=self.server_ip)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, query</span>):<br>        tensor = self.bert_client.encode([query])<br>        <span class="hljs-keyword">return</span> tensor<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query_similarity</span>(<span class="hljs-params">self, query_list</span>):<br>        tensors = self.bert_client.encode(query_list)<br>        <span class="hljs-keyword">return</span> cosine_similarity(tensors)[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br>    ec = Encoding()<br>    <span class="hljs-built_in">print</span>(ec.encode(<span class="hljs-string">&quot;中国&quot;</span>).shape)<br>    <span class="hljs-built_in">print</span>(ec.encode(<span class="hljs-string">&quot;美国&quot;</span>).shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;中国和美国的向量相似度:&quot;</span>, ec.query_similarity([<span class="hljs-string">&quot;中国&quot;</span>, <span class="hljs-string">&quot;美国&quot;</span>]))<br></code></pre></td></tr></table></figure><h3 id="查找相似词语">查找相似词语</h3><p>利用词向量可以查找文章中与指定词语最相近的几个词语。具体的做法为：现将文章分词，对分词后的每个词，查询其与指定词语的相似度，最后按相似度输出词语即可。我们的示例文章为老舍的《养花》，内容如下：</p><blockquote><p>我爱花，所以也爱养花。我可还没成为养花专家，因为没有工夫去研究和试验。我只把养花当做生活中的一种乐趣，花开得大小好坏都不计较，只要开花，我就高兴。在我的小院子里，一到夏天满是花草，小猫只好上房去玩，地上没有它们的运动场。花虽然多，但是没有奇花异草。珍贵的花草不易养活，看着一棵好花生病要死，是件难过的事。北京的气候，对养花来说不算很好，冬天冷，春天多风，夏天不是干旱就是大雨倾盆，秋天最好，可是会忽然闹霜冻。在这种气候里，想把南方的好花养活，我还没有那么大的本事。因此，我只养些好种易活、自己会奋斗的花草。不过，尽管花草自己会奋斗，我若是置之不理，任其自生自灭，大半还是会死的。我得天天照管它们，像好朋友似的关心它们。一来二去，我摸着一些门道：有的喜阴，就别放在太阳地里；有的喜干，就别多浇水。摸着门道，花草养活了，而且三年五载老活着、开花，多么有意思啊！不是乱吹，这就是知识呀！多得些知识决不是坏事。我不是有腿病吗，不但不利于行，也不利于久坐。我不知道花草们受我的照顾，感谢我不感谢；我可得感谢它们。我工作的时候，我总是写一会儿就到院中去看看，浇浇这棵，搬搬那盆，然后回到屋里再写一会儿，然后再出去。如此循环，让脑力劳动和体力劳动得到适当的调节，有益身心，胜于吃药。要是赶上狂风暴雨或天气突变，就得全家动员，抢救花草，十分紧张。几百盆花，都要很快地抢到屋里去，使人腰酸腿疼，热汗直流。第二天，天气好了，又得把花都搬出去，就又一次腰酸腿疼，热汗直流。可是，这多么有意思呀！不劳动，连棵花也养不活，这难道不是真理吗？送牛奶的同志进门就夸“好香”，这使我们全家都感到骄傲。赶到昙花开放的时候，约几位朋友来看看，更有秉烛夜游的味道——昙花总在夜里开放。花分根了，一棵分为几棵，就赠给朋友们一些。看着友人拿走自己的劳动果实，心里自然特别欢喜。当然，也有伤心的时候，今年夏天就有这么一回。三百棵菊秧还在地上（没到移入盆中的时候），下了暴雨，邻家的墙倒了，菊秧被砸死三十多种，一百多棵。全家人几天都没有笑容。有喜有忧，有笑有泪，有花有果，有香有色，既须劳动，又长见识，这就是养花的乐趣。</p></blockquote><p>指定词语为“开心”，查询《养花》一文中与“开心”最为接近的5个词语，完整的Python代码如下：（find_similar_words.py）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">from</span> bert_client_lmj <span class="hljs-keyword">import</span> Encoding<br><span class="hljs-keyword">from</span> operator <span class="hljs-keyword">import</span> itemgetter<br><br><span class="hljs-comment"># 读取文章</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./doc.txt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    content = f.read().replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br><br>ec = Encoding()<br>similar_word_dict = &#123;&#125;<br><br><span class="hljs-comment"># 查找文章中与&#x27;开心&#x27;的最接近的词语</span><br>words = <span class="hljs-built_in">list</span>(jieba.cut(content))<br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>    <span class="hljs-built_in">print</span>(word)<br>    <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> similar_word_dict.keys():<br>        similar_word_dict[word] = ec.query_similarity([word, <span class="hljs-string">&#x27;开心&#x27;</span>])<br><br><span class="hljs-comment"># 按相似度从高到低排序</span><br>sorted_dict = <span class="hljs-built_in">sorted</span>(similar_word_dict.items(), key=itemgetter(<span class="hljs-number">1</span>), reverse=<span class="hljs-literal">True</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;与%s最接近的5个词语及相似度如下：&#x27;</span> % <span class="hljs-string">&#x27;开心&#x27;</span>)<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> sorted_dict[:<span class="hljs-number">5</span>]:<br>    <span class="hljs-built_in">print</span>(_)<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs 1c">与开心最接近的<span class="hljs-number">5</span>个词语及相似度如下：<br>(&#x27;难过&#x27;, <span class="hljs-number">0.9070794</span>)<br>(&#x27;高兴&#x27;, <span class="hljs-number">0.89517105</span>)<br>(&#x27;乐趣&#x27;, <span class="hljs-number">0.89260685</span>)<br>(&#x27;骄傲&#x27;, <span class="hljs-number">0.87363803</span>)<br>(&#x27;我爱花&#x27;, <span class="hljs-number">0.86954254</span>)<br></code></pre></td></tr></table></figure><h3 id="提取文本中的实体">提取文本中的实体</h3><p>在事件抽取中，我们往往需要抽取一些指定的元素，比如在下面的句子中，</p><blockquote><p>巴基斯坦当地时间2014年12月16日早晨，巴基斯坦塔利班运动武装分子袭击了西北部白沙瓦市一所军人子弟学校，打死141人，其中132人为12岁至16岁的学生。</p></blockquote><p>我们需要抽取袭击者，也就是恐怖组织这个元素。</p><p>直接从句法分析，也许可以得到一定的效果，但由于事件描述方式多变，句法分析会显得比较复杂且效果不一定能保证。这时候，我们尝试BERT词向量，它在一定程度上可以作为补充策略，帮助我们定位到事件的元素。具体的想法如下：</p><ul><li>指定事件元素模板</li><li>句子分词，对词语做n-gram</li><li>查询每个n-gram与模板的相似度</li><li>按相似度对n-gram排序，取相似度最高的n-gram</li></ul><p>在这里，我们的事件元素为恐怖组织，指定的模板为“伊斯兰组织”，完整的Python程序如下（find_similar_entity_in_sentence.py）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">from</span> operator <span class="hljs-keyword">import</span> itemgetter<br><span class="hljs-keyword">from</span> bert_client_lmj <span class="hljs-keyword">import</span> Encoding<br><br><span class="hljs-comment"># 创建n-gram</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_ngrams</span>(<span class="hljs-params">sequence, n</span>):<br>    lst = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">zip</span>(*[sequence[index:] <span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)]))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(lst)):<br>        lst[i] = <span class="hljs-string">&#x27;&#x27;</span>.join(lst[i])<br>    <span class="hljs-keyword">return</span> lst<br><br><span class="hljs-comment"># 模板</span><br>template = <span class="hljs-string">&#x27;伊斯兰组织&#x27;</span><br><span class="hljs-comment"># 示例句子</span><br>doc = <span class="hljs-string">&quot;巴基斯坦当地时间2014年12月16日早晨，巴基斯坦塔利班运动武装分子袭击了西北部白沙瓦市一所军人子弟学校，打死141人，其中132人为12岁至16岁的学生。&quot;</span><br><br>words = <span class="hljs-built_in">list</span>(jieba.cut(doc))<br>all_lst = []<br><span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>):<br>    all_lst.extend(compute_ngrams(words, j))<br><br>ec = Encoding()<br>similar_word_dict = &#123;&#125;<br><br><span class="hljs-comment"># 查找文章中与template的最接近的词语</span><br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> all_lst:<br>    <span class="hljs-built_in">print</span>(word)<br>    <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> similar_word_dict.keys():<br>        similar_word_dict[word] = ec.query_similarity([word, template])<br><br><span class="hljs-comment"># 按相似度从高到低排序</span><br>sorted_dict = <span class="hljs-built_in">sorted</span>(similar_word_dict.items(), key=itemgetter(<span class="hljs-number">1</span>), reverse=<span class="hljs-literal">True</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;与%s最接近的实体是: %s，相似度为 %s.&#x27;</span> %(template, sorted_dict[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], sorted_dict[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]))<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs dns">与伊斯兰组织最接近的实体是: 塔利班运动武装分子，相似度为 <span class="hljs-number">0.8953854</span>.<br></code></pre></td></tr></table></figure><p>可以看到，该算法成功地帮助我们定位到了恐怖组织：塔利班运动武装分子，效果很好，但是由于是无监督产生的词向量，效果不一定可控，而且该算法运行速度较慢，这点可以从工程上加以改进。</p><h3 id="问答中的实体对齐">问答中的实体对齐</h3><p>在智能问答中，我们往往会采用知识图谱或者数据库存储实体，其中一个难点就是实体对齐。举个例子，我们在数据库中储存的实体如下：（entities.txt）</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-number">094</span>型/晋级<br><span class="hljs-number">052</span>C型（旅洋Ⅱ级）<br>辽宁舰<span class="hljs-regexp">/瓦良格/</span>Varyag<br>杰拉尔德·R·福特号航空母舰<br><span class="hljs-number">052</span>D型（旅洋III级）<br><span class="hljs-number">054</span>A型<br>CVN-<span class="hljs-number">72</span><span class="hljs-regexp">/林肯号/</span>Lincoln<br></code></pre></td></tr></table></figure><p>这样的实体名字很复杂，如果用户想查询实体“辽宁舰”，就会碰到困难，但是由于实体以储存在数据库或知识图谱中，实体不好直接修改。一种办法是通过关键字匹配定位实体，在这里，我们可以借助BERT词向量来实现，完整的Python代码如下：（Entity_Alignment.py）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding:utf-8 -*-</span><br><span class="hljs-keyword">from</span> bert_client_lmj <span class="hljs-keyword">import</span> Encoding<br><span class="hljs-keyword">from</span> operator <span class="hljs-keyword">import</span> itemgetter<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;entities.txt&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    entities = [_.strip() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> f.readlines()]<br><br>ec = Encoding()<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">entity_alignment</span>(<span class="hljs-params">query</span>):<br><br>    similar_word_dict = &#123;&#125;<br><br>    <span class="hljs-comment"># 查找已有实体中与query最接近的实体</span><br>    <span class="hljs-keyword">for</span> entity <span class="hljs-keyword">in</span> entities:<br>        <span class="hljs-keyword">if</span> entity <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> similar_word_dict.keys():<br>            similar_word_dict[entity] = ec.query_similarity([entity, query])<br><br>    <span class="hljs-comment"># 按相似度从高到低排序</span><br>    sorted_dict = <span class="hljs-built_in">sorted</span>(similar_word_dict.items(), key=itemgetter(<span class="hljs-number">1</span>), reverse=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">return</span> sorted_dict[<span class="hljs-number">0</span>]<br><br>query = <span class="hljs-string">&#x27;辽宁舰&#x27;</span><br>result = entity_alignment(query)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;查询实体：%s，匹配实体：%s 。&#x27;</span> %(query, result))<br><br>query = <span class="hljs-string">&#x27;林肯号&#x27;</span><br>result = entity_alignment(query)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;查询实体：%s，匹配实体：%s 。&#x27;</span> %(query, result))<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs 1c">查询实体：辽宁舰，匹配实体：(&#x27;辽宁舰/瓦良格/Varyag&#x27;, <span class="hljs-number">0.8534695</span>) 。<br>查询实体：林肯号，匹配实体：(&#x27;CVN-72/林肯号/Lincoln&#x27;, <span class="hljs-number">0.8389378</span>) 。<br></code></pre></td></tr></table></figure><p>在这里，查询的速度应该不是困难，因为我们可以将已储存的实体以离线的方式查询其词向量并储存，这样进来一个查询到实体，只查询一次词向量，并计算其与离线的词向量的相似度。这种方法也存在缺陷，主要是由于词向量的无监督，实体对齐有时候不会很准，但作为一种补充策略，也许可以考虑。</p><h3 id="总结">总结</h3><p>本文介绍了笔者这段时间所思考的BERT词向量的几个应用，由于能力有限，文章中会存在考虑不当的地方，还请读者多多批评指正。</p><p>另外，笔者将会持续调研词向量方面的技术，比如腾讯词向量，百度词向量等，欢迎大家关注～</p><blockquote><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注~</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>BERT</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（十）使用LSTM进行文本情感分析</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%8D%81%EF%BC%89%E4%BD%BF%E7%94%A8LSTM%E8%BF%9B%E8%A1%8C%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%8D%81%EF%BC%89%E4%BD%BF%E7%94%A8LSTM%E8%BF%9B%E8%A1%8C%E6%96%87%E6%9C%AC%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h4 id="情感分析简介">情感分析简介</h4><p>文本情感分析（SentimentAnalysis）是自然语言处理（NLP）方法中常见的应用，也是一个有趣的基本任务，尤其是以提炼文本情绪内容为目的的分类。它是对带有情感色彩的主观性文本进行分析、处理、归纳和推理的过程。</p><p>本文将介绍情感分析中的情感极性（倾向）分析。所谓情感极性分析，指的是对文本进行褒义、贬义、中性的判断。在大多应用场景下，只分为两类。例如对于“喜爱”和“厌恶”这两个词，就属于不同的情感倾向。</p><p>本文将详细介绍如何使用深度学习模型中的LSTM模型来实现文本的情感分析。</p><h3 id="文本介绍及语料分析">文本介绍及语料分析</h3><p>我们以某电商网站中某个商品的评论作为语料（corpus.csv），该数据集的下载网址为：<ahref="https://github.com/renjunxiang/Text-Classification/blob/master/TextClassification/data/data_single.csv">https://github.com/renjunxiang/Text-Classification/blob/master/TextClassification/data/data_single.csv</a>，该数据集一共有4310条评论数据，文本的情感分为两类：“正面”和“反面”，该数据集的前几行如下：</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey"><span class="hljs-built_in">evaluation,</span>label<br>用了一段时间，感觉还不错，可以,正面<br>电视非常好，已经是家里的第二台了。第一天下单，第二天就到本地了，可是物流的人说车坏了，一直催，客服也帮着催，到第三天下午<span class="hljs-number">5</span>点才送过来。父母年纪大了，买个大电视画面清晰，趁着耳朵还好使，享受几年。,正面<br>电视比想象中的大好多，画面也很清晰，系统很智能，更多功能还在摸索中,正面<br>不错,正面<br>用了这么多天了，感觉还不错。夏普的牌子还是比较可靠。希望以后比较耐用，现在是考量质量的时候。,正面<br>物流速度很快，非常棒，今天就看了电视，非常清晰，非常流畅，一次非常完美的购物体验,正面<br>非常好，客服还特意打电话做回访,正面<br>物流小哥不错，辛苦了，东西还没用,正面<br>送货速度快，质量有保障，活动价格挺好的。希望用的久，不出问题。,正面<br></code></pre></td></tr></table></figure><p>接着我们需要对语料做一个简单的分析：</p><ul><li><p>数据集中的情感分布；</p></li><li><p>数据集中的评论句子长度分布。</p><p>使用以下Python脚本，我们可以统计出数据集中的情感分布以及评论句子长度分布。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> font_manager<br><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> accumulate<br><br><span class="hljs-comment"># 设置matplotlib绘图时的字体</span><br>my_font = font_manager.FontProperties(fname=<span class="hljs-string">&quot;/Library/Fonts/Songti.ttc&quot;</span>)<br><br><span class="hljs-comment"># 统计句子长度及长度出现的频数</span><br>df = pd.read_csv(<span class="hljs-string">&#x27;./corpus.csv&#x27;</span>)<br><span class="hljs-built_in">print</span>(df.groupby(<span class="hljs-string">&#x27;label&#x27;</span>)[<span class="hljs-string">&#x27;label&#x27;</span>].count())<br><br>df[<span class="hljs-string">&#x27;length&#x27;</span>] = df[<span class="hljs-string">&#x27;evaluation&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x))<br>len_df = df.groupby(<span class="hljs-string">&#x27;length&#x27;</span>).count()<br>sent_length = len_df.index.tolist()<br>sent_freq = len_df[<span class="hljs-string">&#x27;evaluation&#x27;</span>].tolist()<br><br><span class="hljs-comment"># 绘制句子长度及出现频数统计图</span><br>plt.bar(sent_length, sent_freq)<br>plt.title(<span class="hljs-string">&quot;句子长度及出现频数统计图&quot;</span>, fontproperties=my_font)<br>plt.xlabel(<span class="hljs-string">&quot;句子长度&quot;</span>, fontproperties=my_font)<br>plt.ylabel(<span class="hljs-string">&quot;句子长度出现的频数&quot;</span>, fontproperties=my_font)<br>plt.savefig(<span class="hljs-string">&quot;./句子长度及出现频数统计图.png&quot;</span>)<br>plt.close()<br><br><span class="hljs-comment"># 绘制句子长度累积分布函数(CDF)</span><br>sent_pentage_list = [(count/<span class="hljs-built_in">sum</span>(sent_freq)) <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> accumulate(sent_freq)]<br><br><span class="hljs-comment"># 绘制CDF</span><br>plt.plot(sent_length, sent_pentage_list)<br><br><span class="hljs-comment"># 寻找分位点为quantile的句子长度</span><br>quantile = <span class="hljs-number">0.91</span><br><span class="hljs-comment">#print(list(sent_pentage_list))</span><br><span class="hljs-keyword">for</span> length, per <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(sent_length, sent_pentage_list):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">round</span>(per, <span class="hljs-number">2</span>) == quantile:<br>        index = length<br>        <span class="hljs-keyword">break</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n分位点为%s的句子长度:%d.&quot;</span> % (quantile, index))<br><br><span class="hljs-comment"># 绘制句子长度累积分布函数图</span><br>plt.plot(sent_length, sent_pentage_list)<br>plt.hlines(quantile, <span class="hljs-number">0</span>, index, colors=<span class="hljs-string">&quot;c&quot;</span>, linestyles=<span class="hljs-string">&quot;dashed&quot;</span>)<br>plt.vlines(index, <span class="hljs-number">0</span>, quantile, colors=<span class="hljs-string">&quot;c&quot;</span>, linestyles=<span class="hljs-string">&quot;dashed&quot;</span>)<br>plt.text(<span class="hljs-number">0</span>, quantile, <span class="hljs-built_in">str</span>(quantile))<br>plt.text(index, <span class="hljs-number">0</span>, <span class="hljs-built_in">str</span>(index))<br>plt.title(<span class="hljs-string">&quot;句子长度累积分布函数图&quot;</span>, fontproperties=my_font)<br>plt.xlabel(<span class="hljs-string">&quot;句子长度&quot;</span>, fontproperties=my_font)<br>plt.ylabel(<span class="hljs-string">&quot;句子长度累积频率&quot;</span>, fontproperties=my_font)<br>plt.savefig(<span class="hljs-string">&quot;./句子长度累积分布函数图.png&quot;</span>)<br>plt.close()<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">label</span><br><span class="hljs-string">正面</span>    <span class="hljs-number">1908</span><br><span class="hljs-string">负面</span>    <span class="hljs-number">2375</span><br><span class="hljs-attr">Name:</span> <span class="hljs-string">label,</span> <span class="hljs-attr">dtype:</span> <span class="hljs-string">int64</span><br><br><span class="hljs-string">分位点为0.91的句子长度:183.</span><br></code></pre></td></tr></table></figure><p>可以看到，正反面两类情感的比例差不多。句子长度及出现频数统计图如下：</p><p><img src="/img/nlp10_1.png" /></p><p>句子长度累积分布函数图如下：</p><p><img src="/img/nlp10_2.png" /></p><p>可以看到，大多数样本的句子长度集中在1-200之间，句子长度累计频率取0.91分位点，则长度为183左右。</p><h3 id="使用lstm模型">使用LSTM模型</h3><p>接着我们使用深度学习中的LSTM模型来对上述数据集做情感分析，笔者实现的模型框架如下：</p><figure><img src="/img/nlp10_3.png" alt="模型结构图" /><figcaption aria-hidden="true">模型结构图</figcaption></figure><p>完整的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> np_utils, plot_model<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> LSTM, Dense, Embedding, Dropout<br><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> accuracy_score<br><br><span class="hljs-comment"># 导入数据</span><br><span class="hljs-comment"># 文件的数据中，特征为evaluation, 类别为label.</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>(<span class="hljs-params">filepath, input_shape=<span class="hljs-number">20</span></span>):<br>    df = pd.read_csv(filepath)<br><br>    <span class="hljs-comment"># 标签及词汇表</span><br>    labels, vocabulary = <span class="hljs-built_in">list</span>(df[<span class="hljs-string">&#x27;label&#x27;</span>].unique()), <span class="hljs-built_in">list</span>(df[<span class="hljs-string">&#x27;evaluation&#x27;</span>].unique())<br><br>    <span class="hljs-comment"># 构造字符级别的特征</span><br>    string = <span class="hljs-string">&#x27;&#x27;</span><br>    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> vocabulary:<br>        string += word<br><br>    vocabulary = <span class="hljs-built_in">set</span>(string)<br><br>    <span class="hljs-comment"># 字典列表</span><br>    word_dictionary = &#123;word: i+<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocabulary)&#125;<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;word_dict.pk&#x27;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        pickle.dump(word_dictionary, f)<br>    inverse_word_dictionary = &#123;i+<span class="hljs-number">1</span>: word <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocabulary)&#125;<br>    label_dictionary = &#123;label: i <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels)&#125;<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;label_dict.pk&#x27;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        pickle.dump(label_dictionary, f)<br>    output_dictionary = &#123;i: labels <span class="hljs-keyword">for</span> i, labels <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels)&#125;<br><br>    vocab_size = <span class="hljs-built_in">len</span>(word_dictionary.keys()) <span class="hljs-comment"># 词汇表大小</span><br>    label_size = <span class="hljs-built_in">len</span>(label_dictionary.keys()) <span class="hljs-comment"># 标签类别数量</span><br><br>    <span class="hljs-comment"># 序列填充，按input_shape填充，长度不足的按0补充</span><br>    x = [[word_dictionary[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> df[<span class="hljs-string">&#x27;evaluation&#x27;</span>]]<br>    x = pad_sequences(maxlen=input_shape, sequences=x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br>    y = [[label_dictionary[sent]] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> df[<span class="hljs-string">&#x27;label&#x27;</span>]]<br>    y = [np_utils.to_categorical(label, num_classes=label_size) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> y]<br>    y = np.array([<span class="hljs-built_in">list</span>(_[<span class="hljs-number">0</span>]) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> y])<br><br>    <span class="hljs-keyword">return</span> x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary<br><br><span class="hljs-comment"># 创建深度学习模型， Embedding + LSTM + Softmax.</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_LSTM</span>(<span class="hljs-params">n_units, input_shape, output_dim, filepath</span>):<br>    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = load_data(filepath)<br>    model = Sequential()<br>    model.add(Embedding(input_dim=vocab_size + <span class="hljs-number">1</span>, output_dim=output_dim,<br>                        input_length=input_shape, mask_zero=<span class="hljs-literal">True</span>))<br>    model.add(LSTM(n_units, input_shape=(x.shape[<span class="hljs-number">0</span>], x.shape[<span class="hljs-number">1</span>])))<br>    model.add(Dropout(<span class="hljs-number">0.2</span>))<br>    model.add(Dense(label_size, activation=<span class="hljs-string">&#x27;softmax&#x27;</span>))<br>    model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>, optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>, metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br><br>    plot_model(model, to_file=<span class="hljs-string">&#x27;./model_lstm.png&#x27;</span>, show_shapes=<span class="hljs-literal">True</span>)<br>    model.summary()<br><br>    <span class="hljs-keyword">return</span> model<br><br><span class="hljs-comment"># 模型训练</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_train</span>(<span class="hljs-params">input_shape, filepath, model_save_path</span>):<br><br>    <span class="hljs-comment"># 将数据集分为训练集和测试集，占比为9:1</span><br>    <span class="hljs-comment"># input_shape = 100</span><br>    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = load_data(filepath, input_shape)<br>    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = <span class="hljs-number">0.1</span>, random_state = <span class="hljs-number">42</span>)<br><br>    <span class="hljs-comment"># 模型输入参数，需要自己根据需要调整</span><br>    n_units = <span class="hljs-number">100</span><br>    batch_size = <span class="hljs-number">32</span><br>    epochs = <span class="hljs-number">5</span><br>    output_dim = <span class="hljs-number">20</span><br><br>    <span class="hljs-comment"># 模型训练</span><br>    lstm_model = create_LSTM(n_units, input_shape, output_dim, filepath)<br>    lstm_model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 模型保存</span><br>    lstm_model.save(model_save_path)<br><br>    N = test_x.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 测试的条数</span><br>    predict = []<br>    label = []<br>    <span class="hljs-keyword">for</span> start, end <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, N, <span class="hljs-number">1</span>), <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, N+<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)):<br>        sentence = [inverse_word_dictionary[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> test_x[start] <span class="hljs-keyword">if</span> i != <span class="hljs-number">0</span>]<br>        y_predict = lstm_model.predict(test_x[start:end])<br>        label_predict = output_dictionary[np.argmax(y_predict[<span class="hljs-number">0</span>])]<br>        label_true = output_dictionary[np.argmax(test_y[start:end])]<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#x27;</span>.join(sentence), label_true, label_predict) <span class="hljs-comment"># 输出预测结果</span><br>        predict.append(label_predict)<br>        label.append(label_true)<br><br>    acc = accuracy_score(predict, label) <span class="hljs-comment"># 预测准确率</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;模型在测试集上的准确率为: %s.&#x27;</span> % acc)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    filepath = <span class="hljs-string">&#x27;./corpus.csv&#x27;</span><br>    input_shape = <span class="hljs-number">180</span><br>    model_save_path = <span class="hljs-string">&#x27;./corpus_model.h5&#x27;</span><br>    model_train(input_shape, filepath, model_save_path)<br></code></pre></td></tr></table></figure><p>对上述模型，共训练5次，训练集和测试集比例为9:1，输出的结果为：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">......</span><br><span class="hljs-string">Epoch</span> <span class="hljs-number">5</span><span class="hljs-string">/5</span><br><span class="hljs-string">......</span><br><span class="hljs-number">3424</span><span class="hljs-string">/3854</span> [<span class="hljs-string">=========================&gt;....</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 2s - loss: 0.1280 - acc:</span> <span class="hljs-number">0.9565</span><br><span class="hljs-number">3456</span><span class="hljs-string">/3854</span> [<span class="hljs-string">=========================&gt;....</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1274 - acc:</span> <span class="hljs-number">0.9569</span><br><span class="hljs-number">3488</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==========================&gt;...</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1274 - acc:</span> <span class="hljs-number">0.9570</span><br><span class="hljs-number">3520</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==========================&gt;...</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1287 - acc:</span> <span class="hljs-number">0.9568</span><br><span class="hljs-number">3552</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==========================&gt;...</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1290 - acc:</span> <span class="hljs-number">0.9564</span><br><span class="hljs-number">3584</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==========================&gt;...</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1284 - acc:</span> <span class="hljs-number">0.9568</span><br><span class="hljs-number">3616</span><span class="hljs-string">/3854</span> [<span class="hljs-string">===========================&gt;..</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 1s - loss: 0.1284 - acc:</span> <span class="hljs-number">0.9569</span><br><span class="hljs-number">3648</span><span class="hljs-string">/3854</span> [<span class="hljs-string">===========================&gt;..</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1278 - acc:</span> <span class="hljs-number">0.9572</span><br><span class="hljs-number">3680</span><span class="hljs-string">/3854</span> [<span class="hljs-string">===========================&gt;..</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1271 - acc:</span> <span class="hljs-number">0.9576</span><br><span class="hljs-number">3712</span><span class="hljs-string">/3854</span> [<span class="hljs-string">===========================&gt;..</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1268 - acc:</span> <span class="hljs-number">0.9580</span><br><span class="hljs-number">3744</span><span class="hljs-string">/3854</span> [<span class="hljs-string">============================&gt;.</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1279 - acc:</span> <span class="hljs-number">0.9575</span><br><span class="hljs-number">3776</span><span class="hljs-string">/3854</span> [<span class="hljs-string">============================&gt;.</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1272 - acc:</span> <span class="hljs-number">0.9579</span><br><span class="hljs-number">3808</span><span class="hljs-string">/3854</span> [<span class="hljs-string">============================&gt;.</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1279 - acc:</span> <span class="hljs-number">0.9580</span><br><span class="hljs-number">3840</span><span class="hljs-string">/3854</span> [<span class="hljs-string">============================&gt;.</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">ETA: 0s - loss: 0.1281 - acc:</span> <span class="hljs-number">0.9581</span><br><span class="hljs-number">3854</span><span class="hljs-string">/3854</span> [<span class="hljs-string">==============================</span>] <span class="hljs-bullet">-</span> <span class="hljs-attr">18s 5ms/step - loss: 0.1298 - acc:</span> <span class="hljs-number">0.9577</span><br><span class="hljs-string">......</span><br><span class="hljs-string">给父母买的，特意用了一段时间再来评价，电视非常好，没有坏点和损坏，界面也很简洁，便于操作，稍微不足就是开机会比普通电视慢一些，这应该是智能电视的通病吧，如果可以希望微鲸大大可以更新系统优化下开机时间~电视真的很棒，性价比爆棚，值得大家考虑购买。</span> <span class="hljs-string">客服很细心，快递小哥很耐心的等我通电验货，态度非常好。</span> <span class="hljs-string">负面</span> <span class="hljs-string">正面</span><br><span class="hljs-string">长须鲸和海狮回答都很及时，虽然物流不够快但是服务不错电视不错，对比了乐视小米和微鲸论性价比还是微鲸好点</span> <span class="hljs-string">负面</span> <span class="hljs-string">负面</span><br><span class="hljs-string">所以看不到4k效果，但是应该可以。</span> <span class="hljs-string">自带音响，中规中矩吧，好像没有别人说的好。而且，到现在没连接上我的漫步者，这个非常不满意，因为看到网上说好像普通3.5mm的连不上或者连上了声音小。希望厂家接下来开发的电视有改进。不知道我要不要换个音响。其他的用用再说。</span> <span class="hljs-string">放在地上的是跟我混了两年的tcl，天气受潮，修了一次，下岗了。</span> <span class="hljs-string">最后，我也觉得底座不算太稳，凑合着用。</span> <span class="hljs-string">负面</span> <span class="hljs-string">负面</span><br><span class="hljs-string">电视机一般，低端机不要求那么高咯。</span> <span class="hljs-string">负面</span> <span class="hljs-string">负面</span><br><span class="hljs-string">很好，两点下单上午就到了，服务很好。</span> <span class="hljs-string">正面</span> <span class="hljs-string">正面</span><br><span class="hljs-string">帮朋友买的，好好好好好好好好</span> <span class="hljs-string">正面</span> <span class="hljs-string">正面</span><br><span class="hljs-string">......</span><br><span class="hljs-string">模型在测试集上的准确率为:</span> <span class="hljs-number">0.9020979020979021</span><span class="hljs-string">.</span><br></code></pre></td></tr></table></figure><p>可以看到，该模型在训练集上的准确率为95%以上，在测试集上的准确率为90%以上，效果还是相当不错的。</p><h3 id="模型预测">模型预测</h3><p>接着，我们利用刚刚训练好的模型，对新的数据进行测试。笔者随机改造上述样本的评论，然后预测其情感倾向。情感预测的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-comment"># Import the necessary modules</span><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><br><br><span class="hljs-comment"># 导入字典</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;word_dict.pk&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    word_dictionary = pickle.load(f)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;label_dict.pk&#x27;</span>, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    output_dictionary = pickle.load(f)<br><br><span class="hljs-keyword">try</span>:<br>    <span class="hljs-comment"># 数据预处理</span><br>    input_shape = <span class="hljs-number">180</span><br>    sent = <span class="hljs-string">&quot;电视刚安装好，说实话，画质不怎么样，很差！&quot;</span><br>    x = [[word_dictionary[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent]]<br>    x = pad_sequences(maxlen=input_shape, sequences=x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># 载入模型</span><br>    model_save_path = <span class="hljs-string">&#x27;./sentiment_analysis.h5&#x27;</span><br>    lstm_model = load_model(model_save_path)<br><br>    <span class="hljs-comment"># 模型预测</span><br>    y_predict = lstm_model.predict(x)<br>    label_dict = &#123;v:k <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> output_dictionary.items()&#125;<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;输入语句: %s&#x27;</span> % sent)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;情感预测结果: %s&#x27;</span> % label_dict[np.argmax(y_predict)])<br><br><span class="hljs-keyword">except</span> KeyError <span class="hljs-keyword">as</span> err:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;您输入的句子有汉字不在词汇表中，请重新输入！&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;不在词汇表中的单词为：%s.&quot;</span> % err)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><span class="hljs-section">输入语句: 电视刚安装好，说实话，画质不怎么样，很差！</span><br><span class="hljs-section">情感预测结果: 负面</span><br></code></pre></td></tr></table></figure><p>让我们再尝试着测试一些其他的评论：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs makefile"><span class="hljs-section">输入语句: 物超所值，真心不错</span><br><span class="hljs-section">情感预测结果: 正面</span><br><span class="hljs-section">输入语句: 很大很好，方便安装！</span><br><span class="hljs-section">情感预测结果: 正面</span><br><span class="hljs-section">输入语句: 卡，慢，死机，闪退。</span><br><span class="hljs-section">情感预测结果: 负面</span><br><span class="hljs-section">输入语句: 这种货色就这样吧，别期待怎样。</span><br><span class="hljs-section">情感预测结果: 负面</span><br><span class="hljs-section">输入语句: 啥服务态度码，出了事情一个推一个，送货安装还收我50</span><br><span class="hljs-section">情感预测结果: 负面</span><br><span class="hljs-section">输入语句: 京东服务很好！但我买的这款电视两天后就出现这样的问题，很后悔买了这样的电视</span><br><span class="hljs-section">情感预测结果: 负面</span><br><span class="hljs-section">输入语句: 产品质量不错，就是这位客服的态度十分恶劣，对相关服务不予解释说明，缺乏耐心，</span><br><span class="hljs-section">情感预测结果: 负面</span><br><span class="hljs-section">输入语句: 很满意，电视非常好。护眼模式，很好，也很清晰。</span><br><span class="hljs-section">情感预测结果: 负面</span><br></code></pre></td></tr></table></figure><h3 id="总结">总结</h3><p>当然，该模型并不是对一切该商品的评论都会有好的效果，还是应该针对特定的语料去训练，去预测。</p><p>本文主要介绍了LSTM模型在文本情感分析方面的应用，该项目已上传Github，地址为：<ahref="https://github.com/percent4/Sentiment_Analysis">https://github.com/percent4/Sentiment_Analysis</a>。</p><p>注意：不妨了解下笔者的微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注~</p><h3 id="参考文献">参考文献</h3><ol type="1"><li>Python机器学习 -- NLP情感分析：<ahref="https://blog.csdn.net/qq_38328378/article/details/81198322">https://blog.csdn.net/qq_38328378/article/details/81198322</a></li><li>数据集来源：<ahref="https://github.com/renjunxiang/Text-Classification/blob/master/TextClassification/data/data_single.csv">https://github.com/renjunxiang/Text-Classification/blob/master/TextClassification/data/data_single.csv</a></li></ol>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>情感分析</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（九）词义消岐（WSD）的简介与实现</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B9%9D%EF%BC%89%E8%AF%8D%E4%B9%89%E6%B6%88%E5%B2%90%EF%BC%88WSD%EF%BC%89%E7%9A%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%9E%E7%8E%B0/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B9%9D%EF%BC%89%E8%AF%8D%E4%B9%89%E6%B6%88%E5%B2%90%EF%BC%88WSD%EF%BC%89%E7%9A%84%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    
    <content type="html"><![CDATA[<h3 id="词义消岐简介">词义消岐简介</h3><p>词义消岐，英文名称为Word SenseDisambiguation，英语缩写为WSD，是自然语言处理（NLP）中一个非常有趣的基本任务。</p><p>那么，什么是词义消岐呢？通常，在我们的自然语言中，不管是英语，还是中文，都有多义词存在。这些多义词的存在，会让人对句子的意思产生混淆，但人通过学习又是可以正确地区分出来的。</p><p>以<strong>“小米”</strong>这个词为例，如果仅仅只是说“小米”这个词语，你并不知道它实际指的到底是小米科技公司还是谷物。但当我们把词语置于某个特定的语境中，我们能很好地区分出这个词语的意思。比如，</p><blockquote><p>雷军是小米的创始人。</p></blockquote><p>在这个句子中，我们知道这个“小米”指的是小米科技公司。比如</p><blockquote><p>我今天早上喝了一碗小米粥。</p></blockquote><p>在这个句子中，“小米”指的是谷物、农作物。</p><p>所谓词义消岐，指的是在特定的语境中，识别出某个歧义词的正确含义。</p><p>那么，词义消岐有什么作用呢？词义消岐可以很好地服务于语言翻译和智能问答领域，当然，还有许多应用有待开发～</p><h3 id="词义消岐实现">词义消岐实现</h3><p>在目前的词义消岐算法中，有不少原创算法，有些实现起来比较简单，有些想法较为复杂，但实现的效果普遍都不是很好。比较经典的词义消岐的算法为Lesk算法，该算法的想法很简单，通过对某个歧义词构建不同含义的语料及待判别句子中该词语与语料的重合程度来实现，具体的算法原理可参考网址：<ahref="https://en.wikipedia.org/wiki/Lesk_algorithm">https://en.wikipedia.org/wiki/Lesk_algorithm</a>.</p><p>在下面的部分中，笔者将会介绍自己想的一种实现词义消岐的算法，仅仅是一个想法，仅供参考。</p><p>我们以词语“火箭”为例，选取其中的两个<strong>义项</strong>（同一个词语的不同含义）：<ahref="https://baike.baidu.com/item/%E7%81%AB%E7%AE%AD/8794081#viewPageContent"title="NBA球队名">NBA球队名</a> 和 <ahref="https://baike.baidu.com/item/%E7%81%AB%E7%AE%AD/6308#viewPageContent"title="燃气推进装置">燃气推进装置</a> ，如下：</p><p><img src="/img/nlp9_1.png" /></p><h4 id="获取语料">获取语料</h4><p>首先，我们利用爬虫爬取这两个义项的百度百科网页，以句子为单位，只要句子中出现该词语，则把这句话加入到这个义项的预料中。爬虫的完整Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> requests<br><span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> SentenceSplitter<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WebScrape</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, word, url</span>):<br>        self.url = url<br>        self.word = word<br><br>    <span class="hljs-comment"># 爬取百度百科页面</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">web_parse</span>(<span class="hljs-params">self</span>):<br>        headers = &#123;<span class="hljs-string">&#x27;User-Agent&#x27;</span>: <span class="hljs-string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 \</span><br><span class="hljs-string">                                             (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36&#x27;</span>&#125;<br>        req = requests.get(url=self.url, headers=headers)<br><br>        <span class="hljs-comment"># 解析网页，定位到main-content部分</span><br>        <span class="hljs-keyword">if</span> req.status_code == <span class="hljs-number">200</span>:<br>            soup = BeautifulSoup(req.text.encode(req.encoding), <span class="hljs-string">&#x27;lxml&#x27;</span>)<br>            <span class="hljs-keyword">return</span> soup<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># 获取该词语的义项</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_gloss</span>(<span class="hljs-params">self</span>):<br>        soup = self.web_parse()<br>        <span class="hljs-keyword">if</span> soup:<br>            lis = soup.find(<span class="hljs-string">&#x27;ul&#x27;</span>, class_=<span class="hljs-string">&quot;polysemantList-wrapper cmn-clearfix&quot;</span>)<br>            <span class="hljs-keyword">if</span> lis:<br>                <span class="hljs-keyword">for</span> li <span class="hljs-keyword">in</span> lis(<span class="hljs-string">&#x27;li&#x27;</span>):<br>                    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;&lt;a&#x27;</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">str</span>(li):<br>                        gloss = li.text.replace(<span class="hljs-string">&#x27;▪&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>                        <span class="hljs-keyword">return</span> gloss<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># 获取该义项的语料，以句子为单位</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_content</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 发送HTTP请求</span><br>        result = []<br>        soup = self.web_parse()<br>        <span class="hljs-keyword">if</span> soup:<br>            paras = soup.find(<span class="hljs-string">&#x27;div&#x27;</span>, class_=<span class="hljs-string">&#x27;main-content&#x27;</span>).text.split(<span class="hljs-string">&#x27;\n&#x27;</span>)<br>            <span class="hljs-keyword">for</span> para <span class="hljs-keyword">in</span> paras:<br>                <span class="hljs-keyword">if</span> self.word <span class="hljs-keyword">in</span> para:<br>                    sents = <span class="hljs-built_in">list</span>(SentenceSplitter.split(para))<br>                    <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>                        <span class="hljs-keyword">if</span> self.word <span class="hljs-keyword">in</span> sent:<br>                            sent = sent.replace(<span class="hljs-string">&#x27;\xa0&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>).replace(<span class="hljs-string">&#x27;\u3000&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>                            result.append(sent)<br><br>        result = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(result))<br><br>        <span class="hljs-keyword">return</span> result<br><br>    <span class="hljs-comment"># 将该义项的语料写入到txt</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">write_2_file</span>(<span class="hljs-params">self</span>):<br>        gloss = self.get_gloss()<br>        result = self.get_content()<br>        <span class="hljs-built_in">print</span>(gloss)<br>        <span class="hljs-built_in">print</span>(result)<br>        <span class="hljs-keyword">if</span> result <span class="hljs-keyword">and</span> gloss:<br>            <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./%s_%s.txt&#x27;</span>% (self.word, gloss), <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>                f.writelines([_+<span class="hljs-string">&#x27;\n&#x27;</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> result])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">run</span>(<span class="hljs-params">self</span>):<br>        self.write_2_file()<br><br><span class="hljs-comment"># NBA球队名</span><br><span class="hljs-comment">#url = &#x27;https://baike.baidu.com/item/%E4%BC%91%E6%96%AF%E6%95%A6%E7%81%AB%E7%AE%AD%E9%98%9F/370758?fromtitle=%E7%81%AB%E7%AE%AD&amp;fromid=8794081#viewPageContent&#x27;</span><br><span class="hljs-comment"># 燃气推进装置</span><br>url = <span class="hljs-string">&#x27;https://baike.baidu.com/item/%E7%81%AB%E7%AE%AD/6308#viewPageContent&#x27;</span><br>WebScrape(<span class="hljs-string">&#x27;火箭&#x27;</span>, url).run()<br></code></pre></td></tr></table></figure><p>利用这个爬虫，我们爬取了“火箭”这个词语的两个义项的语料，生成了火箭_燃气推进装置.txt文件和火箭_NBA球队名.txt文件，这两个文件分别含有361和171个句子。以火箭_燃气推进装置.txt文件为例，前10个句子如下：</p><blockquote><p>火箭技术的飞速发展，不仅可提供更加完善的各类导弹和推动相关科学的发展，还将使开发空间资源、建立空间产业、空间基地及星际航行等成为可能。火箭技术是一项十分复杂的综合性技术，主要包括火箭推进技术、总体设计技术、火箭结构技术、控制和制导技术、计划管理技术、可靠性和质量控制技术、试验技术，对导弹来说还有弹头制导和控制、1903年，俄国的К.E.齐奥尔科夫斯基提出了制造大型液体火箭的设想和设计原理。火箭有很多种，原始的火箭是用引火物附在弓箭头上，然后射到敌人身上引起焚烧的一种箭矢。“长征三号丙”火箭是在 “长征三号乙”火箭的基础上，减少了两个助推器并取消了助推器上的尾翼。 火箭与导弹有什么区别为了能够在未来大规模的将人类送入太空，不可能依赖传统的火箭和飞船。火箭V2火箭探测高层大气的物理特征（如气压、温度、湿度等）和现象的探空火箭。可一次发射一发至数十发火箭弹。</p></blockquote><h4 id="实现算法">实现算法</h4><p>我们以句子为单位进行词义消岐，即输入一句话，识别出该句子中某个歧义词的含义。笔者使用的算法比较简单，是以TF-IDF为权重的频数判别。以句子</p><blockquote><p>赛季初的时候，火箭是众望所归的西部决赛球队。</p></blockquote><p>为例，对该句子分词后，去掉停用词（stopwords），然后分别统计除了“火箭”这个词以外的TF-IDF值，累加起来,比较在两个义项下这个值的大小即可。</p><p>实现这个算法的完整Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> jieba<br><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> log2<br><br><span class="hljs-comment"># 读取每个义项的语料</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_file</span>(<span class="hljs-params">path</span>):<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(path, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        lines = [_.strip() <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> f.readlines()]<br>        <span class="hljs-keyword">return</span> lines<br><br><span class="hljs-comment"># 对示例句子分词</span><br>sent = <span class="hljs-string">&#x27;赛季初的时候，火箭是众望所归的西部决赛球队。&#x27;</span><br>wsd_word = <span class="hljs-string">&#x27;火箭&#x27;</span><br><br>jieba.add_word(wsd_word)<br>sent_words = <span class="hljs-built_in">list</span>(jieba.cut(sent, cut_all=<span class="hljs-literal">False</span>))<br><br><span class="hljs-comment"># 去掉停用词</span><br>stopwords = [wsd_word, <span class="hljs-string">&#x27;我&#x27;</span>, <span class="hljs-string">&#x27;你&#x27;</span>, <span class="hljs-string">&#x27;它&#x27;</span>, <span class="hljs-string">&#x27;他&#x27;</span>, <span class="hljs-string">&#x27;她&#x27;</span>, <span class="hljs-string">&#x27;了&#x27;</span>, <span class="hljs-string">&#x27;是&#x27;</span>, <span class="hljs-string">&#x27;的&#x27;</span>, <span class="hljs-string">&#x27;啊&#x27;</span>, <span class="hljs-string">&#x27;谁&#x27;</span>, <span class="hljs-string">&#x27;什么&#x27;</span>,<span class="hljs-string">&#x27;都&#x27;</span>,\<br>             <span class="hljs-string">&#x27;很&#x27;</span>, <span class="hljs-string">&#x27;个&#x27;</span>, <span class="hljs-string">&#x27;之&#x27;</span>, <span class="hljs-string">&#x27;人&#x27;</span>, <span class="hljs-string">&#x27;在&#x27;</span>, <span class="hljs-string">&#x27;上&#x27;</span>, <span class="hljs-string">&#x27;下&#x27;</span>, <span class="hljs-string">&#x27;左&#x27;</span>, <span class="hljs-string">&#x27;右&#x27;</span>, <span class="hljs-string">&#x27;。&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>, <span class="hljs-string">&#x27;！&#x27;</span>, <span class="hljs-string">&#x27;？&#x27;</span>]<br><br>sent_cut = []<br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent_words:<br>    <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> stopwords:<br>        sent_cut.append(word)<br><br><span class="hljs-built_in">print</span>(sent_cut)<br><br><br><span class="hljs-comment"># 计算其他词的TF-IDF以及频数</span><br>wsd_dict = &#123;&#125;<br><span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> os.listdir(<span class="hljs-string">&#x27;.&#x27;</span>):<br>    <span class="hljs-keyword">if</span> wsd_word <span class="hljs-keyword">in</span> file:<br>        wsd_dict[file.replace(<span class="hljs-string">&#x27;.txt&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)] = read_file(file)<br><br><span class="hljs-comment"># 统计每个词语在语料中出现的次数</span><br>tf_dict = &#123;&#125;<br><span class="hljs-keyword">for</span> meaning, sents <span class="hljs-keyword">in</span> wsd_dict.items():<br>    tf_dict[meaning] = []<br>    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent_cut:<br>        word_count = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>            example = <span class="hljs-built_in">list</span>(jieba.cut(sent, cut_all=<span class="hljs-literal">False</span>))<br>            word_count += example.count(word)<br><br>        <span class="hljs-keyword">if</span> word_count:<br>            tf_dict[meaning].append((word, word_count))<br><br>idf_dict = &#123;&#125;<br><span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent_cut:<br>    document_count = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> meaning, sents <span class="hljs-keyword">in</span> wsd_dict.items():<br>        <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> sent:<br>                document_count += <span class="hljs-number">1</span><br><br>    idf_dict[word] = document_count<br><br><span class="hljs-comment"># 输出值</span><br>total_document = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> meaning, sents <span class="hljs-keyword">in</span> wsd_dict.items():<br>    total_document += <span class="hljs-built_in">len</span>(sents)<br><br><span class="hljs-comment"># 计算tf_idf值</span><br>mean_tf_idf = []<br><span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> tf_dict.items():<br>    <span class="hljs-built_in">print</span>(k+<span class="hljs-string">&#x27;:&#x27;</span>)<br>    tf_idf_sum = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> v:<br>        word = item[<span class="hljs-number">0</span>]<br>        tf = item[<span class="hljs-number">1</span>]<br>        tf_idf = item[<span class="hljs-number">1</span>]*log2(total_document/(<span class="hljs-number">1</span>+idf_dict[word]))<br>        tf_idf_sum += tf_idf<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;%s, 频数为: %s, TF-IDF值为: %s&#x27;</span>% (word, tf, tf_idf))<br><br>    mean_tf_idf.append((k, tf_idf_sum))<br><br>sort_array = <span class="hljs-built_in">sorted</span>(mean_tf_idf, key=<span class="hljs-keyword">lambda</span> x:x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)<br>true_meaning = sort_array[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].split(<span class="hljs-string">&#x27;_&#x27;</span>)[<span class="hljs-number">1</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;\n经过词义消岐，%s在该句子中的意思为 %s .&#x27;</span> % (wsd_word, true_meaning))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">[&#x27;赛季&#x27;, &#x27;初&#x27;, &#x27;时候&#x27;, &#x27;众望所归&#x27;, &#x27;西部&#x27;, &#x27;决赛&#x27;, &#x27;球队&#x27;]</span><br><span class="hljs-attribute">火箭_燃气推进装置</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">初, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 12.49585502688717</span><br><span class="hljs-attribute">火箭_NBA球队名</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">赛季, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">63, TF-IDF值为: 204.6194333469459</span><br><span class="hljs-attribute">初, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">1, TF-IDF值为: 6.247927513443585</span><br><span class="hljs-attribute">时候, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">1, TF-IDF值为: 8.055282435501189</span><br><span class="hljs-attribute">西部, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">16, TF-IDF值为: 80.88451896801904</span><br><span class="hljs-attribute">决赛, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">7, TF-IDF值为: 33.13348038429679</span><br><span class="hljs-attribute">球队, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">40, TF-IDF值为: 158.712783770034</span><br><br>经过词义消岐，火箭在该句子中的意思为 NBA球队名 .<br></code></pre></td></tr></table></figure><h4 id="测试">测试</h4><p>接着，我们对上面的算法和程序进行更多的测试。</p><p>输入句子为:</p><blockquote><p>三十多年前，战士们在戈壁滩白手起家，建起了我国的火箭发射基地。</p></blockquote><p>输出结果为:</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs prolog">[<span class="hljs-string">&#x27;三十多年&#x27;</span>, <span class="hljs-string">&#x27;前&#x27;</span>, <span class="hljs-string">&#x27;战士&#x27;</span>, <span class="hljs-string">&#x27;们&#x27;</span>, <span class="hljs-string">&#x27;戈壁滩&#x27;</span>, <span class="hljs-string">&#x27;白手起家&#x27;</span>, <span class="hljs-string">&#x27;建起&#x27;</span>, <span class="hljs-string">&#x27;我国&#x27;</span>, <span class="hljs-string">&#x27;发射&#x27;</span>, <span class="hljs-string">&#x27;基地&#x27;</span>]<br>火箭<span class="hljs-symbol">_</span>燃气推进装置:<br>前, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">9.063440958888354</span><br>们, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">6.05528243550119</span><br>我国, 频数为: <span class="hljs-number">3</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">22.410959804340102</span><br>发射, 频数为: <span class="hljs-number">89</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">253.27878721862933</span><br>基地, 频数为: <span class="hljs-number">7</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">42.38697704850833</span><br>火箭<span class="hljs-symbol">_NBA</span>球队名:<br>前, 频数为: <span class="hljs-number">3</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">13.59516143833253</span><br>们, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">6.05528243550119</span><br><br>经过词义消岐，火箭在该句子中的意思为 燃气推进装置 .<br></code></pre></td></tr></table></figure><p>输入句子为：</p><blockquote><p>对于马刺这样级别的球队，常规赛只有屈指可数的几次交锋具有真正的意义，今天对火箭一役是其中之一。</p></blockquote><p>输出结果为：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs prolog">[<span class="hljs-string">&#x27;对于&#x27;</span>, <span class="hljs-string">&#x27;马刺&#x27;</span>, <span class="hljs-string">&#x27;这样&#x27;</span>, <span class="hljs-string">&#x27;级别&#x27;</span>, <span class="hljs-string">&#x27;球队&#x27;</span>, <span class="hljs-string">&#x27;常规赛&#x27;</span>, <span class="hljs-string">&#x27;只有&#x27;</span>, <span class="hljs-string">&#x27;屈指可数&#x27;</span>, <span class="hljs-string">&#x27;几次&#x27;</span>, <span class="hljs-string">&#x27;交锋&#x27;</span>, <span class="hljs-string">&#x27;具有&#x27;</span>, <span class="hljs-string">&#x27;真正&#x27;</span>, <span class="hljs-string">&#x27;意义&#x27;</span>, <span class="hljs-string">&#x27;今天&#x27;</span>, <span class="hljs-string">&#x27;对&#x27;</span>, <span class="hljs-string">&#x27;一役&#x27;</span>, <span class="hljs-string">&#x27;其中&#x27;</span>, <span class="hljs-string">&#x27;之一&#x27;</span>]<br>火箭<span class="hljs-symbol">_</span>燃气推进装置:<br>只有, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">7.470319934780034</span><br>具有, 频数为: <span class="hljs-number">5</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">32.35159967390017</span><br>真正, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">14.940639869560068</span><br>意义, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">8.055282435501189</span><br>对, 频数为: <span class="hljs-number">5</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">24.03677461028802</span><br>其中, 频数为: <span class="hljs-number">3</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">21.16584730650357</span><br>之一, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">14.11056487100238</span><br>火箭<span class="hljs-symbol">_NBA</span>球队名:<br>马刺, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">7.470319934780034</span><br>球队, 频数为: <span class="hljs-number">40</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">158.712783770034</span><br>常规赛, 频数为: <span class="hljs-number">14</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">73.4709851882102</span><br>只有, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">7.470319934780034</span><br>对, 频数为: <span class="hljs-number">10</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">48.07354922057604</span><br>之一, 频数为: <span class="hljs-number">1</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">7.05528243550119</span><br><br>经过词义消岐，火箭在该句子中的意思为 <span class="hljs-symbol">NBA</span>球队名 .<br></code></pre></td></tr></table></figure><p>输入句子为：</p><blockquote><p>姚明是火箭队的主要得分手之一。</p></blockquote><p>输出结果为：</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">[&#x27;姚明&#x27;, &#x27;火箭队&#x27;, &#x27;主要&#x27;, &#x27;得分手&#x27;, &#x27;之一&#x27;]</span><br><span class="hljs-attribute">火箭_燃气推进装置</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">主要, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">9, TF-IDF值为: 51.60018906552445</span><br><span class="hljs-attribute">之一, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 14.11056487100238</span><br><span class="hljs-attribute">火箭_NBA球队名</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">姚明, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">18, TF-IDF值为: 90.99508383902142</span><br><span class="hljs-attribute">火箭队, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">133, TF-IDF值为: 284.1437533641371</span><br><span class="hljs-attribute">之一, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">1, TF-IDF值为: 7.05528243550119</span><br><br>经过词义消岐，火箭在该句子中的意思为 NBA球队名 .<br></code></pre></td></tr></table></figure><p>输入的句子为:</p><blockquote><p>从1992年开始研制的长征二号F型火箭，是中国航天史上技术最复杂、可靠性和安全性指标最高的运载火箭。</p></blockquote><p>输出结果为：</p><figure class="highlight nestedtext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs nestedtext"><span class="hljs-attribute">[&#x27;从&#x27;, &#x27;1992&#x27;, &#x27;年&#x27;, &#x27;开始&#x27;, &#x27;研制&#x27;, &#x27;长征二号&#x27;, &#x27;F&#x27;, &#x27;型&#x27;, &#x27;中国&#x27;, &#x27;航天史&#x27;, &#x27;技术&#x27;, &#x27;最&#x27;, &#x27;复杂&#x27;, &#x27;、&#x27;, &#x27;可靠性&#x27;, &#x27;和&#x27;, &#x27;安全性&#x27;, &#x27;指标&#x27;, &#x27;最高&#x27;, &#x27;运载火箭&#x27;]</span><br><span class="hljs-attribute">火箭_燃气推进装置</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">从, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">6, TF-IDF值为: 29.312144604353264</span><br><span class="hljs-attribute">1992, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">1, TF-IDF值为: 6.733354340613827</span><br><span class="hljs-attribute">年, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">43, TF-IDF值为: 107.52982410441274</span><br><span class="hljs-attribute">开始, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">5, TF-IDF值为: 30.27641217750595</span><br><span class="hljs-attribute">研制, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">25, TF-IDF值为: 110.28565614316162</span><br><span class="hljs-attribute">长征二号, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">37, TF-IDF值为: 159.11461253349566</span><br><span class="hljs-attribute">F, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">7, TF-IDF值为: 40.13348038429679</span><br><span class="hljs-attribute">中国, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">45, TF-IDF值为: 153.51418105769093</span><br><span class="hljs-attribute">技术, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">27, TF-IDF值为: 119.10850863461454</span><br><span class="hljs-attribute">最, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 7.614709844115208</span><br><span class="hljs-attribute">、, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">117, TF-IDF值为: 335.25857156467714</span><br><span class="hljs-attribute">可靠性, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">5, TF-IDF值为: 30.27641217750595</span><br><span class="hljs-attribute">和, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">76, TF-IDF值为: 191.22539545388003</span><br><span class="hljs-attribute">安全性, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 14.940639869560068</span><br><span class="hljs-attribute">运载火箭, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">95, TF-IDF值为: 256.28439093389505</span><br><span class="hljs-attribute">火箭_NBA球队名</span><span class="hljs-punctuation">:</span><br><span class="hljs-attribute">从, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">5, TF-IDF值为: 24.42678717029439</span><br><span class="hljs-attribute">1992, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 13.466708681227654</span><br><span class="hljs-attribute">年, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">52, TF-IDF值为: 130.0360663588247</span><br><span class="hljs-attribute">开始, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">2, TF-IDF值为: 12.11056487100238</span><br><span class="hljs-attribute">中国, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">4, TF-IDF值为: 13.64570498290586</span><br><span class="hljs-attribute">最, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">3, TF-IDF值为: 11.422064766172813</span><br><span class="hljs-attribute">、, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">16, TF-IDF值为: 45.847326025938756</span><br><span class="hljs-attribute">和, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">31, TF-IDF值为: 77.99983235618791</span><br><span class="hljs-attribute">最高, 频数为</span><span class="hljs-punctuation">:</span> <span class="hljs-string">8, TF-IDF值为: 59.76255947824027</span><br><br>经过词义消岐，火箭在该句子中的意思为 燃气推进装置 .<br></code></pre></td></tr></table></figure><p>输入句子为：</p><blockquote><p>到目前为止火箭已经在休斯顿进行了电视宣传，并在大街小巷竖起广告栏。</p></blockquote><p>输出结果为：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs prolog">[<span class="hljs-string">&#x27;到&#x27;</span>, <span class="hljs-string">&#x27;目前为止&#x27;</span>, <span class="hljs-string">&#x27;已经&#x27;</span>, <span class="hljs-string">&#x27;休斯顿&#x27;</span>, <span class="hljs-string">&#x27;进行&#x27;</span>, <span class="hljs-string">&#x27;电视&#x27;</span>, <span class="hljs-string">&#x27;宣传&#x27;</span>, <span class="hljs-string">&#x27;并&#x27;</span>, <span class="hljs-string">&#x27;大街小巷&#x27;</span>, <span class="hljs-string">&#x27;竖起&#x27;</span>, <span class="hljs-string">&#x27;广告栏&#x27;</span>]<br>火箭<span class="hljs-symbol">_</span>燃气推进装置:<br>到, 频数为: <span class="hljs-number">11</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">39.19772273088667</span><br>已经, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">13.466708681227654</span><br>进行, 频数为: <span class="hljs-number">14</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">68.39500407682429</span><br>并, 频数为: <span class="hljs-number">11</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">49.17351928258037</span><br>火箭<span class="hljs-symbol">_NBA</span>球队名:<br>到, 频数为: <span class="hljs-number">6</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">21.38057603502909</span><br>已经, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">13.466708681227654</span><br>休斯顿, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">14.940639869560068</span><br>进行, 频数为: <span class="hljs-number">2</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">9.770714868117755</span><br>并, 频数为: <span class="hljs-number">5</span>, <span class="hljs-symbol">TF</span>-<span class="hljs-symbol">IDF</span>值为: <span class="hljs-number">22.351599673900168</span><br><br>经过词义消岐，火箭在该句子中的意思为 燃气推进装置 .<br></code></pre></td></tr></table></figure><h3 id="总结">总结</h3><p>对于笔者的这个算法，虽然有一定的效果，但是也不总是识别正确。比如，对于最后一个测试的句子，识别的结果就是错误的，其实“休斯顿”才是识别该词语义项的关键词，但很遗憾，在笔者的算法中，“休斯顿”的权重并不高。</p><p>对于词义消岐算法，如果还是笔者的这个思路，那么有以下几方面需要改进：</p><ul><li><p>语料大小及丰富程度；</p></li><li><p>停用词的扩充；</p></li><li><p>更好的算法。</p><p>笔者的这篇文章仅作为词义消岐的简介以及简单实现，希望能对读者有所启发～</p></li></ul><p><strong>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</strong></p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>词义消岐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（八）使用CRF++实现命名实体识别(NER)</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AB%EF%BC%89%E4%BD%BF%E7%94%A8CRF-%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB-NER/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AB%EF%BC%89%E4%BD%BF%E7%94%A8CRF-%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB-NER/</url>
    
    <content type="html"><![CDATA[<h3 id="crf与ner简介">CRF与NER简介</h3><p>CRF，英文全称为conditional random field,中文名为条件随机场，是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型，其特点是假设输出随机变量构成马尔可夫（Markov）随机场。</p><p>较为简单的条件随机场是定义在线性链上的条件随机场，称为线性链条件随机场（linearchain conditional random field）.线性链条件随机场可以用于序列标注等问题，而本文需要解决的命名实体识别(NER)任务正好可通过序列标注方法解决。这时，在条件概率模型P(Y|X)中，Y是输出变量，表示标记序列（或状态序列），X是输入变量，表示需要标注的观测序列。学习时，利用训练数据集通过极大似然估计或正则化的极大似然估计得到条件概率模型p(Y|X)；预测时，对于给定的输入序列x，求出条件概率p(y|x)最大的输出序列y0.</p><p><img src="/img/nlp8_1.jpeg" /></p><p>命名实体识别（Named EntityRecognition，简称NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。常见的实现NER的算法如下：</p><p><img src="/img/nlp8_2.jpeg" /></p><p>本文不准备详细介绍条件随机场的原理与实现算法，关于具体的原理与实现算法，可以参考《统计学习算法》一书。我们将借助已实现条件随机场的工具——CRF++来实现命名实体识别。关于用深度学习算法来实现命名实体识别，可以参考文章：<ahref="https://www.jianshu.com/p/ee750877ab6f">NLP入门（五）用深度学习实现命名实体识别（NER）</a>。</p><h3 id="crf">CRF++</h3><h4 id="简介">简介</h4><p>CRF++是著名的条件随机场的开源工具，也是目前综合性能最佳的CRF工具，采用C++语言编写而成。其最重要的功能我认为是采用了特征模板。这样就可以自动生成一系列的特征函数，而不用我们自己生成特征函数，我们要做的就是寻找特征，比如词性等。关于CRF++的特性，可以参考网址：<ahref="http://taku910.github.io/crfpp/">http://taku910.github.io/crfpp/</a>。</p><h4 id="安装">安装</h4><p>CRF++的安装可分为Windows环境和Linux环境下的安装。关于Linux环境下的安装，可以参考文章：<ahref="https://blog.51cto.com/wutengfei/2095715">CRFPP/CRF++编译安装与部署</a>。在Windows中CRF++不需要安装，下载解压CRF++0.58文件即可以使用，下载网址为：<ahref="https://blog.csdn.net/lilong117194/article/details/81160265">https://blog.csdn.net/lilong117194/article/details/81160265</a>。</p><h4 id="使用">使用</h4><h5 id="语料">1. 语料</h5><p>以我们本次使用的命名实体识别的语料为例，作为CRF++训练的语料（前20行，每一句话以空格隔开。）如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs bash">played VBD O<br>on IN O<br>Monday NNP O<br>( ( O<br>home NN O<br>team NN O<br><span class="hljs-keyword">in</span> IN O<br>CAPS NNP O<br>) ) O<br>: : O<br><br>American NNP B-MISC<br>League NNP I-MISC<br><br>Cleveland NNP B-ORG<br>2 CD O<br>DETROIT NNP B-ORG<br>1 CD O<br><br>BALTIMORE VB B-ORG<br></code></pre></td></tr></table></figure><p>需要注意字与标签之间的分隔符为制表符否则会导致feature_index.cpp(86)[max_size == size] inconsistent column size错误。 ##### 2. 模板模板是使用CRF++的关键，它能帮助我们自动生成一系列的特征函数，而不用我们自己生成特征函数，而特征函数正是CRF算法的核心概念之一。一个简单的模板文件如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># Unigram</span><br>U00:%x[-2,0]<br>U01:%x[0,1]<br>U02:%x[0,0]<br>U03:%x[1,0]<br>U04:%x[2,0]<br>U05:%x[-2,0]/%x[-1,0]/%x[0,0]<br>U06:%x[-1,0]/%x[0,0]/%x[1,0]<br>U07:%x[0,0]/%x[1,0]/%x[2,0]<br>U08:%x[-1,0]/%x[0,0]<br>U09:%x[0,0]/%x[1,0]<br> <br><span class="hljs-comment"># Bigram</span><br>B<br></code></pre></td></tr></table></figure><p>在这里，我们需要好好理解下模板文件的规则。T**:%x[#,#]中的T表示模板类型，两个"#"分别表示相对的行偏移与列偏移。一共有两种模板：</p><ul><li>第一种模板是Unigram template:第一个字符是U，用于描述unigramfeature的模板。每一行%x[#,#]生成一个CRF中的点(state)函数: f(s, o),其中s为t时刻的的标签(output)，o为t时刻的上下文。假设<code>home NN O</code>所在行为<code>CURRENT TOKEN</code>，</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">played VBD O<br>on IN O<br>Monday NNP O<br>( ( O<br>home NN O &lt;&lt; <span class="hljs-string">CURRENT TOKEN</span><br><span class="hljs-string">team NN O</span><br><span class="hljs-string">in IN O</span><br><span class="hljs-string">CAPS NNP O</span><br><span class="hljs-string">) ) O</span><br><span class="hljs-string">: : O</span><br></code></pre></td></tr></table></figure><p>那么%x[#,#]的对应规则如下：</p><table><thead><tr class="header"><th>template</th><th>expanded feature</th></tr></thead><tbody><tr class="odd"><td>%x[0,0]</td><td>home</td></tr><tr class="even"><td>%x[0,1]</td><td>NN</td></tr><tr class="odd"><td>%x[-1,0]</td><td>(</td></tr><tr class="even"><td>%x[-2,1]</td><td>NNP</td></tr><tr class="odd"><td>%x[0,0]/%x[0,1]</td><td>home/NN</td></tr><tr class="even"><td>ABC%x[0,1]123</td><td>ABCNN123</td></tr></tbody></table><p>以“U01:%x[0,1]”为例，它在该语料中生成的示例函数如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">func1 = <span class="hljs-keyword">if</span> (output = O and feature=<span class="hljs-string">&quot;U01:NN&quot;</span>) <span class="hljs-built_in">return</span> 1 <span class="hljs-keyword">else</span> <span class="hljs-built_in">return</span> 0<br>func2 = <span class="hljs-keyword">if</span> (output = O and feature=<span class="hljs-string">&quot;U01:N&quot;</span>) <span class="hljs-built_in">return</span> 1 <span class="hljs-keyword">else</span> <span class="hljs-built_in">return</span> 0<br>func3 = <span class="hljs-keyword">if</span> (output = O and feature=<span class="hljs-string">&quot;U01:NNP&quot;</span>) <span class="hljs-built_in">return</span> 1  <span class="hljs-keyword">else</span> <span class="hljs-built_in">return</span> 0<br>....<br></code></pre></td></tr></table></figure><ul><li>第二种模板是Bigramtemplate:第一个字符是B，每一行%x[#,#]生成一个CRFs中的边(Edge)函数:f(s',s, o),其中s'为t–1时刻的标签。也就是说,Bigram类型与Unigram大致相同,只是还要考虑到t–1时刻的标签。如果只写一个B的话,默认生成f(s',s)，这意味着前一个output token和current token将组合成bigramfeatures。</li></ul><h5 id="训练">3. 训练</h5><p>CRF++的训练命令一般格式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">crf_learn  -f 3 -c 4.0 template train.data model -t<br></code></pre></td></tr></table></figure><p>其中，template为模板文件，train.data为训练语料，-t表示可以得到一个model文件和一个model.txt文件，其他可选参数说明如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs bash">-f, –freq=INT使用属性的出现次数不少于INT(默认为1)<br><br>-m, –maxiter=INT设置INT为LBFGS的最大迭代次数 (默认10k)<br><br>-c, –cost=FLOAT    设置FLOAT为代价参数，过大会过度拟合 (默认1.0)<br><br>-e, –eta=FLOAT设置终止标准FLOAT(默认0.0001)<br><br>-C, –convert将文本模式转为二进制模式<br><br>-t, –textmodel为调试建立文本模型文件<br><br>-a, –algorithm=(CRF|MIRA)    选择训练算法，默认为CRF-L2<br><br>-p, –thread=INT线程数(默认1)，利用多个CPU减少训练时间<br><br>-H, –shrinking-size=INT    设置INT为最适宜的跌代变量次数 (默认20)<br><br>-v, –version显示版本号并退出<br><br>-h, –<span class="hljs-built_in">help</span>显示帮助并退出<br></code></pre></td></tr></table></figure><p>在训练过程中，会输出一些信息，其意义如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">iter：迭代次数。当迭代次数达到maxiter时，迭代终止<br><br>terr：标记错误率<br><br>serr：句子错误率<br><br>obj：当前对象的值。当这个值收敛到一个确定值的时候，训练完成<br><br>diff：与上一个对象值之间的相对差。当此值低于eta时，训练完成<br></code></pre></td></tr></table></figure><h5 id="预测">4. 预测</h5><p>在训练完模型后，我们可以使用训练好的模型对新数据进行预测，预测命令格式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">crf_test -m model NER_predict.data &gt; predict.txt<br></code></pre></td></tr></table></figure><p><code>-m model</code>表示使用我们刚刚训练好的model模型，预测的数据文件为NER_predict.data,<code>&gt; predict.txt</code>表示将预测后的数据写入到predict.txt中。</p><h3 id="ner实现实例">NER实现实例</h3><p>接下来，我们将利用CRF++来实现英文命名实体识别功能。</p><p>本项目实现NER的语料库如下(文件名为train.txt，一共42000行，这里只展示前15行，可以在文章最后的Github地址下载该语料库)：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">played on Monday ( home team <span class="hljs-keyword">in</span> CAPS ) :<br>VBD IN NNP ( NN NN IN NNP ) :<br>O O O O O O O O O O<br>American League<br>NNP NNP<br>B-MISC I-MISC<br>Cleveland 2 DETROIT 1<br>NNP CD NNP CD<br>B-ORG O B-ORG O<br>BALTIMORE 12 Oakland 11 ( 10 innings )<br>VB CD NNP CD ( CD NN )<br>B-ORG O B-ORG O O O O O<br>TORONTO 5 Minnesota 3<br>TO CD NNP CD<br>B-ORG O B-ORG O<br>......<br></code></pre></td></tr></table></figure><p>简单介绍下该语料库的结构：该语料库一共42000行，每三行为一组，其中，第一行为英语句子，第二行为句子中每个单词的词性，第三行为NER系统的标注，共分4个标注类别：PER（人名），LOC（位置），ORG（组织）以及MISC，其中B表示开始，I表示中间，O表示单字词，不计入NER，sO表示特殊单字词。</p><p>首先我们将该语料分为训练集和测试集，比例为9:1，实现的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-comment"># NER预料train.txt所在的路径</span><br><span class="hljs-built_in">dir</span> = <span class="hljs-string">&quot;/Users/Shared/CRF_4_NER/CRF_TEST&quot;</span><br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/train.txt&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    sents = [line.strip() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines()]<br><br><span class="hljs-comment"># 训练集与测试集的比例为9:1</span><br>RATIO = <span class="hljs-number">0.9</span><br>train_num = <span class="hljs-built_in">int</span>((<span class="hljs-built_in">len</span>(sents)//<span class="hljs-number">3</span>)*RATIO)<br><br><span class="hljs-comment"># 将文件分为训练集与测试集</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/NER_train.data&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&quot;w&quot;</span>) <span class="hljs-keyword">as</span> g:<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(train_num):<br>        words = sents[<span class="hljs-number">3</span>*i].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        postags = sents[<span class="hljs-number">3</span>*i+<span class="hljs-number">1</span>].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        tags = sents[<span class="hljs-number">3</span>*i+<span class="hljs-number">2</span>].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        <span class="hljs-keyword">for</span> word, postag, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(words, postags, tags):<br>            g.write(word+<span class="hljs-string">&#x27; &#x27;</span>+postag+<span class="hljs-string">&#x27; &#x27;</span>+tag+<span class="hljs-string">&#x27;\n&#x27;</span>)<br>        g.write(<span class="hljs-string">&#x27;\n&#x27;</span>)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/NER_test.data&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&quot;w&quot;</span>) <span class="hljs-keyword">as</span> h:<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(train_num+<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(sents)//<span class="hljs-number">3</span>):<br>        words = sents[<span class="hljs-number">3</span>*i].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        postags = sents[<span class="hljs-number">3</span>*i+<span class="hljs-number">1</span>].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        tags = sents[<span class="hljs-number">3</span>*i+<span class="hljs-number">2</span>].split(<span class="hljs-string">&#x27;\t&#x27;</span>)<br>        <span class="hljs-keyword">for</span> word, postag, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(words, postags, tags):<br>            h.write(word+<span class="hljs-string">&#x27; &#x27;</span>+postag+<span class="hljs-string">&#x27; &#x27;</span>+tag+<span class="hljs-string">&#x27;\n&#x27;</span>)<br>        h.write(<span class="hljs-string">&#x27;\n&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;OK!&#x27;</span>)<br></code></pre></td></tr></table></figure><p>运行此程序，得到NER_train.data,此为训练集数据，NER_test.data，此为测试集数据。NER_train.data的前20行数据如下（以）：</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs mathematica"><span class="hljs-variable">played</span> <span class="hljs-variable">VBD</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">on</span> <span class="hljs-variable">IN</span> <span class="hljs-built_in">O</span><br><span class="hljs-built_in">Monday</span> <span class="hljs-variable">NNP</span> <span class="hljs-built_in">O</span><br><span class="hljs-punctuation">(</span> <span class="hljs-punctuation">(</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">home</span> <span class="hljs-variable">NN</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">team</span> <span class="hljs-variable">NN</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">in</span> <span class="hljs-variable">IN</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">CAPS</span> <span class="hljs-variable">NNP</span> <span class="hljs-built_in">O</span><br><span class="hljs-punctuation">)</span> <span class="hljs-punctuation">)</span> <span class="hljs-built_in">O</span><br><span class="hljs-operator">:</span> <span class="hljs-operator">:</span> <span class="hljs-built_in">O</span><br><br><span class="hljs-variable">American</span> <span class="hljs-variable">NNP</span> <span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">MISC</span><br><span class="hljs-variable">League</span> <span class="hljs-variable">NNP</span> <span class="hljs-built_in">I</span><span class="hljs-operator">-</span><span class="hljs-variable">MISC</span><br><br><span class="hljs-variable">Cleveland</span> <span class="hljs-variable">NNP</span> <span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">ORG</span><br><span class="hljs-number">2</span> <span class="hljs-variable">CD</span> <span class="hljs-built_in">O</span><br><span class="hljs-variable">DETROIT</span> <span class="hljs-variable">NNP</span> <span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">ORG</span><br><span class="hljs-number">1</span> <span class="hljs-variable">CD</span> <span class="hljs-built_in">O</span><br><br><span class="hljs-variable">BALTIMORE</span> <span class="hljs-variable">VB</span> <span class="hljs-variable">B</span><span class="hljs-operator">-</span><span class="hljs-variable">ORG</span><br></code></pre></td></tr></table></figure><p>我们使用的模板文件template内容如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-comment"># Unigram</span><br><span class="hljs-attribute">U00</span>:%x[-<span class="hljs-number">2</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U01</span>:%x[-<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U02</span>:%x[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U03</span>:%x[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U04</span>:%x[<span class="hljs-number">2</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U05</span>:%x[-<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]/%x[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]<br><span class="hljs-attribute">U06</span>:%x[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>]/%x[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]<br><br><span class="hljs-attribute">U10</span>:%x[-<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U11</span>:%x[-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U12</span>:%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U13</span>:%x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U14</span>:%x[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U15</span>:%x[-<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]/%x[-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U16</span>:%x[-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U17</span>:%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U18</span>:%x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]<br><br><span class="hljs-attribute">U20</span>:%x[-<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]/%x[-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U21</span>:%x[-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]<br><span class="hljs-attribute">U22</span>:%x[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]/%x[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]<br><br><span class="hljs-comment"># Bigram</span><br><span class="hljs-attribute">B</span><br></code></pre></td></tr></table></figure><p>接着训练该数据，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">crf_learn -c 3.0 template NER_train.data model -t<br></code></pre></td></tr></table></figure><p>运行时的输出信息如下：</p><p><img src="/img/nlp8_3.jpeg" /></p><p>在笔者的电脑上一共迭代了193次，运行时间为490.32秒，标记错误率为0.00004，句子错误率为0.00056。</p><p>接着，我们需要在测试集上对该模型的预测表现做评估。预测命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">crf_test -m model NER_test.data &gt; result.txt<br></code></pre></td></tr></table></figure><p>使用Python脚本统计预测的准确率，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-built_in">dir</span> = <span class="hljs-string">&quot;/Users/Shared/CRF_4_NER/CRF_TEST&quot;</span><br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/result.txt&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    sents = [line.strip() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines() <span class="hljs-keyword">if</span> line.strip()]<br><br>total = <span class="hljs-built_in">len</span>(sents)<br><span class="hljs-built_in">print</span>(total)<br><br>count = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>    words = sent.split()<br>    <span class="hljs-comment"># print(words)</span><br>    <span class="hljs-keyword">if</span> words[-<span class="hljs-number">1</span>] == words[-<span class="hljs-number">2</span>]:<br>        count += <span class="hljs-number">1</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Accuracy: %.4f&quot;</span> %(count/total))<br><span class="hljs-comment"># 0.9706</span><br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">21487</span><br><span class="hljs-attribute">Accuracy</span>: <span class="hljs-number">0</span>.<span class="hljs-number">9706</span><br></code></pre></td></tr></table></figure><p>由此可见，在测试集上的准确率高达0.9706，效果相当好。</p><p>最后，我们对新数据进行命名实体识别，看看模型在新数据上的识别效果。实现的Python代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> nltk<br><br><span class="hljs-built_in">dir</span> = <span class="hljs-string">&quot;/Users/Shared/CRF_4_NER/CRF_TEST&quot;</span><br><br>sentence = <span class="hljs-string">&quot;Venezuelan opposition leader and self-proclaimed interim president Juan Guaidó said Thursday he will return to his country by Monday, and that a dialogue with President Nicolas Maduro won&#x27;t be possible without discussing elections.&quot;</span><br><span class="hljs-comment">#sentence = &quot;Real Madrid&#x27;s season on the brink after 3-0 Barcelona defeat&quot;</span><br><span class="hljs-comment"># sentence = &quot;British artist David Hockney is known as a voracious smoker, but the habit got him into a scrape in Amsterdam on Wednesday.&quot;</span><br><span class="hljs-comment"># sentence = &quot;India is waiting for the release of an pilot who has been in Pakistani custody since he was shot down over Kashmir on Wednesday, a goodwill gesture which could defuse the gravest crisis in the disputed border region in years.&quot;</span><br><span class="hljs-comment"># sentence = &quot;Instead, President Donald Trump&#x27;s second meeting with North Korean despot Kim Jong Un ended in a most uncharacteristic fashion for a showman commander in chief: fizzle.&quot;</span><br><span class="hljs-comment"># sentence = &quot;And in a press conference at the Civic Leadership Academy in Queens, de Blasio said the program is already working.&quot;</span><br><span class="hljs-comment">#sentence = &quot;The United States is a founding member of the United Nations, World Bank, International Monetary Fund.&quot;</span><br><br>default_wt = nltk.word_tokenize <span class="hljs-comment"># 分词</span><br>words = default_wt(sentence)<br><span class="hljs-built_in">print</span>(words)<br>postags = nltk.pos_tag(words)<br><span class="hljs-built_in">print</span>(postags)<br><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/NER_predict.data&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&#x27;w&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> postags:<br>        f.write(item[<span class="hljs-number">0</span>]+<span class="hljs-string">&#x27; &#x27;</span>+item[<span class="hljs-number">1</span>]+<span class="hljs-string">&#x27; O\n&#x27;</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;write successfully!&quot;</span>)<br><br>os.chdir(<span class="hljs-built_in">dir</span>)<br>os.system(<span class="hljs-string">&quot;crf_test -m model NER_predict.data &gt; predict.txt&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;get predict file!&quot;</span>)<br><br><span class="hljs-comment"># 读取预测文件redict.txt</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;%s/predict.txt&quot;</span> % <span class="hljs-built_in">dir</span>, <span class="hljs-string">&#x27;r&#x27;</span>, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    sents = [line.strip() <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines() <span class="hljs-keyword">if</span> line.strip()]<br><br>word = []<br>predict = []<br><br><span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>    words = sent.split()<br>    word.append(words[<span class="hljs-number">0</span>])<br>    predict.append(words[-<span class="hljs-number">1</span>])<br><br><span class="hljs-comment"># print(word)</span><br><span class="hljs-comment"># print(predict)</span><br><br><span class="hljs-comment"># 去掉NER标注为O的元素</span><br>ner_reg_list = []<br><span class="hljs-keyword">for</span> word, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(word, predict):<br>    <span class="hljs-keyword">if</span> tag != <span class="hljs-string">&#x27;O&#x27;</span>:<br>        ner_reg_list.append((word, tag))<br><br><span class="hljs-comment"># 输出模型的NER识别结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;NER识别结果：&quot;</span>)<br><span class="hljs-keyword">if</span> ner_reg_list:<br>    <span class="hljs-keyword">for</span> i, item <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(ner_reg_list):<br>        <span class="hljs-keyword">if</span> item[<span class="hljs-number">1</span>].startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>            end = i+<span class="hljs-number">1</span><br>            <span class="hljs-keyword">while</span> end &lt;= <span class="hljs-built_in">len</span>(ner_reg_list)-<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> ner_reg_list[end][<span class="hljs-number">1</span>].startswith(<span class="hljs-string">&#x27;I&#x27;</span>):<br>                end += <span class="hljs-number">1</span><br><br>            ner_type = item[<span class="hljs-number">1</span>].split(<span class="hljs-string">&#x27;-&#x27;</span>)[<span class="hljs-number">1</span>]<br>            ner_type_dict = &#123;<span class="hljs-string">&#x27;PER&#x27;</span>: <span class="hljs-string">&#x27;PERSON: &#x27;</span>,<br>                             <span class="hljs-string">&#x27;LOC&#x27;</span>: <span class="hljs-string">&#x27;LOCATION: &#x27;</span>,<br>                             <span class="hljs-string">&#x27;ORG&#x27;</span>: <span class="hljs-string">&#x27;ORGANIZATION: &#x27;</span>,<br>                             <span class="hljs-string">&#x27;MISC&#x27;</span>: <span class="hljs-string">&#x27;MISC: &#x27;</span><br>                            &#125;<br>            <span class="hljs-built_in">print</span>(ner_type_dict[ner_type], <span class="hljs-string">&#x27; &#x27;</span>.join([item[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> ner_reg_list[i:end]]))<br></code></pre></td></tr></table></figure><p>识别的结果如下：</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">MISC:</span>  Venezuelan<br><span class="hljs-symbol">PERSON:</span>  Juan Guaidó<br><span class="hljs-symbol">PERSON:</span>  Nicolas Maduro<br></code></pre></td></tr></table></figure><p>识别有个地方不准确， Venezuelan应该是LOC，而不是MISC.我们再接着测试其它的新数据：</p><p>输入语句1：</p><blockquote><p>Real Madrid's season on the brink after 3-0 Barcelona defeat</p></blockquote><p>识别效果1：</p><blockquote><p>ORGANIZATION: Real Madrid LOCATION: Barcelona</p></blockquote><p>输入语句2：</p><blockquote><p>British artist David Hockney is known as a voracious smoker, but thehabit got him into a scrape in Amsterdam on Wednesday.</p></blockquote><p>识别效果2：</p><blockquote><p>MISC: British PERSON: David Hockney LOCATION: Amsterdam</p></blockquote><p>输入语句3：</p><blockquote><p>India is waiting for the release of an pilot who has been inPakistani custody since he was shot down over Kashmir on Wednesday, agoodwill gesture which could defuse the gravest crisis in the disputedborder region in years.</p></blockquote><p>识别效果3：</p><blockquote><p>LOCATION: India LOCATION: Pakistani LOCATION: Kashmir</p></blockquote><p>输入语句4：</p><blockquote><p>Instead, President Donald Trump's second meeting with North Koreandespot Kim Jong Un ended in a most uncharacteristic fashion for ashowman commander in chief: fizzle.</p></blockquote><p>识别效果4：</p><blockquote><p>PERSON: Donald Trump PERSON: Kim Jong Un</p></blockquote><p>输入语句5：</p><blockquote><p>And in a press conference at the Civic Leadership Academy in Queens,de Blasio said the program is already working.</p></blockquote><p>识别效果5：</p><blockquote><p>ORGANIZATION: Civic Leadership Academy LOCATION: Queens PERSON: deBlasio</p></blockquote><p>输入语句6：</p><blockquote><p>The United States is a founding member of the United Nations, WorldBank, International Monetary Fund.</p></blockquote><p>识别效果6：</p><blockquote><p>LOCATION: United States ORGANIZATION: United Nations PERSON: WorldBank ORGANIZATION: International Monetary Fund</p></blockquote><p>在这些例子中，有让我们惊喜之处：识别出了人物Donald Trump, Kim JongUn. 但也有些不足指出，如将WorldBank识别为人物，而不是组织机构。总的来说，识别效果还是让人满意的。</p><h3 id="总结">总结</h3><p>最近由于工作繁忙，无暇顾及博客。但转念一想，技术输出也是比较重要的，需要长期坚持下去～</p><p>本项目的Github地址为：<ahref="https://github.com/percent4/CRF_4_NER">https://github.com/percent4/CRF_4_NER</a>。</p><p>五一将至，祝大家假期愉快～</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>NER</tag>
      
      <tag>CRF++</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（七）中文预处理之繁简体转换及获取拼音</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%83%EF%BC%89%E4%B8%AD%E6%96%87%E9%A2%84%E5%A4%84%E7%90%86%E4%B9%8B%E7%B9%81%E7%AE%80%E4%BD%93%E8%BD%AC%E6%8D%A2%E5%8F%8A%E8%8E%B7%E5%8F%96%E6%8B%BC%E9%9F%B3/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%83%EF%BC%89%E4%B8%AD%E6%96%87%E9%A2%84%E5%A4%84%E7%90%86%E4%B9%8B%E7%B9%81%E7%AE%80%E4%BD%93%E8%BD%AC%E6%8D%A2%E5%8F%8A%E8%8E%B7%E5%8F%96%E6%8B%BC%E9%9F%B3/</url>
    
    <content type="html"><![CDATA[<p>在日常的中文NLP中，经常会涉及到中文的繁简体转换以及拼音的标注等问题，本文将介绍这两个方面的实现。</p><p>首先是中文的繁简体转换，不需要使用额外的Python模块，至需要以下两个Python代码文件即可：</p><ul><li><p>langconv.py 地址： <ahref="https://raw.githubusercontent.com/skydark/nstools/master/zhtools/langconv.py">https://raw.githubusercontent.com/skydark/nstools/master/zhtools/langconv.py</a></p></li><li><p>zh_wiki.py 地址：<ahref="https://raw.githubusercontent.com/skydark/nstools/master/zhtools/zh_wiki.py">https://raw.githubusercontent.com/skydark/nstools/master/zhtools/zh_wiki.py</a></p><p>示例代码如下（将代码文件与langconv.py与zh_wiki.py放在同一目录下）：</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langconv <span class="hljs-keyword">import</span> *<br><br><span class="hljs-comment"># 转换繁体到简体</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cht_2_chs</span>(<span class="hljs-params">line</span>):<br>    line = Converter(<span class="hljs-string">&#x27;zh-hans&#x27;</span>).convert(line)<br>    line.encode(<span class="hljs-string">&#x27;utf-8&#x27;</span>)<br>    <span class="hljs-keyword">return</span> line<br><br>line_cht= <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">台北市長柯文哲今在臉書開直播，先向網友報告自己3月16日至24日要出訪美國東部4城市，接著他無預警宣布，</span><br><span class="hljs-string">2月23日要先出訪以色列，預計停留4至5天。雖他強調台北市、以色列已在資安方面有所交流，也可到當地城市交流、</span><br><span class="hljs-string">參觀產業創新等內容，但柯也說「也是去看看一個小國在這麼惡劣環境，howtosurvive，他的祕訣是什麼？」這番話，</span><br><span class="hljs-string">也被解讀，頗有更上層樓、直指總統大位的思維。</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br>line_cht = line_cht.replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>ret_chs = cht_2_chs(line_cht)<br><span class="hljs-built_in">print</span>(ret_chs)<br><br><span class="hljs-comment"># 转换简体到繁体</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">chs_2_cht</span>(<span class="hljs-params">sentence</span>):<br>    sentence = Converter(<span class="hljs-string">&#x27;zh-hant&#x27;</span>).convert(sentence)<br>    <span class="hljs-keyword">return</span> sentence<br><br>line_chs = <span class="hljs-string">&#x27;忧郁的台湾乌龟&#x27;</span><br>line_cht = chs_2_cht(line_chs)<br><span class="hljs-built_in">print</span>(line_cht)<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><blockquote><p>台北市长柯文哲今在脸书开直播，先向网友报告自己3月16日至24日要出访美国东部4城市，接着他无预警宣布，2月23日要先出访以色列，预计停留4至5天。虽他强调台北市、以色列已在资安方面有所交流，也可到当地城市交流、参观产业创新等内容，但柯也说「也是去看看一个小国在这么恶劣环境，howtosurvive，他的祕诀是什么？」这番话，也被解读，颇有更上层楼、直指总统大位的思维。憂郁的臺灣烏龜</p></blockquote><p>接着是获取中文汉字的拼音，这方面的Python模块有xpinyin,pypinyin等。本文以xpinyin为例，展示如何获取汉字的拼音。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> xpinyin <span class="hljs-keyword">import</span> Pinyin<br><br>p = Pinyin()<br><br><span class="hljs-comment"># 默认分隔符为-</span><br><span class="hljs-built_in">print</span>(p.get_pinyin(<span class="hljs-string">&quot;上海&quot;</span>))<br><br><span class="hljs-comment"># 显示声调</span><br><span class="hljs-built_in">print</span>(p.get_pinyin(<span class="hljs-string">&quot;上海&quot;</span>, tone_marks=<span class="hljs-string">&#x27;marks&#x27;</span>))<br><span class="hljs-built_in">print</span>(p.get_pinyin(<span class="hljs-string">&quot;上海&quot;</span>, tone_marks=<span class="hljs-string">&#x27;numbers&#x27;</span>))<br><br><span class="hljs-comment"># 去掉分隔符</span><br><span class="hljs-built_in">print</span>(p.get_pinyin(<span class="hljs-string">&quot;上海&quot;</span>, <span class="hljs-string">&#x27;&#x27;</span>))<br><span class="hljs-comment"># 设为分隔符为空格</span><br><span class="hljs-built_in">print</span>(p.get_pinyin(<span class="hljs-string">&quot;上海&quot;</span>, <span class="hljs-string">&#x27; &#x27;</span>))<br><br><span class="hljs-comment"># 获取拼音首字母</span><br><span class="hljs-built_in">print</span>(p.get_initial(<span class="hljs-string">&quot;上&quot;</span>))<br><span class="hljs-built_in">print</span>(p.get_initials(<span class="hljs-string">&quot;上海&quot;</span>))<br><span class="hljs-built_in">print</span>(p.get_initials(<span class="hljs-string">&quot;上海&quot;</span>, <span class="hljs-string">&#x27;&#x27;</span>))<br><span class="hljs-built_in">print</span>(p.get_initials(<span class="hljs-string">&quot;上海&quot;</span>, <span class="hljs-string">&#x27; &#x27;</span>))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm"><span class="hljs-keyword">shang-hai</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">shàng-hǎi</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">shang4-hai3</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">shanghai</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">shang </span>hai<br>S<br>S-H<br><span class="hljs-keyword">SH</span><br><span class="hljs-keyword"></span>S H<br></code></pre></td></tr></table></figure><p>本次分享到此结束，感谢大家阅读~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>繁简体转换</tag>
      
      <tag>拼音</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（六）pyltp的介绍与使用</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AD%EF%BC%89pyltp%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%85%AD%EF%BC%89pyltp%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h3 id="pyltp的简介">pyltp的简介</h3><p>语言技术平台(LTP)经过哈工大社会计算与信息检索研究中心 11年的持续研发和推广，是国内外最具影响力的中文处理基础平台。它提供的功能包括中文分词、词性标注、命名实体识别、依存句法分析、语义角色标注等。</p><figure><img src="/img/nlp6_1.png" alt="语言技术平台架构" /><figcaption aria-hidden="true">语言技术平台架构</figcaption></figure><p>pyltp 是 LTP 的 Python封装，同时支持Python2和Python3版本。Python3的安装方法为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip3 install pyltp<br></code></pre></td></tr></table></figure><ul><li><p>官网下载网址：https://pypi.org/project/pyltp/0.1.7/</p></li><li><p>官方使用说明文档：https://pyltp.readthedocs.io/zh_CN/develop/api.html</p><p>在使用该模块前，需要下载完整的模型文件，文件下载地址为：<ahref="https://pan.baidu.com/share/link?shareid=1988562907&amp;uk=2738088569#list/path=%2F">https://pan.baidu.com/share/link?shareid=1988562907&amp;uk=2738088569#list/path=%2F</a>。pyltp 的所有输入的分析文本和输出的结果的编码均为UTF-8。模型的数据文件如下：</p></li></ul><figure><img src="/img/nlp6_2.png" alt="模型数据" /><figcaption aria-hidden="true">模型数据</figcaption></figure><p>其中，cws.model用于分词模型，lexicon.txt为分词时添加的用户字典，ner.model为命名实体识别模型，parser.model为依存句法分析模型，pisrl.model为语义角色标注模型，pos为词性标注模型。</p><h3 id="pyltp的使用">pyltp的使用</h3><p>pyltp的使用示例项目结构如下：</p><figure><img src="/img/nlp6_3.png" alt="示例项目" /><figcaption aria-hidden="true">示例项目</figcaption></figure><h5 id="分句">分句</h5><p>分句指的是将一段话或一片文章中的文字按句子分开，按句子形成独立的单元。示例的Python代码sentenct_split.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> SentenceSplitter<br><br><span class="hljs-comment"># 分句</span><br>doc = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span> \<br>      <span class="hljs-string">&#x27;盖茨原计划从明年1月9日至14日陆续访问中国和日本，目前，他决定在行程中增加对韩国的访问。莫莱尔表示，&#x27;</span> \<br>      <span class="hljs-string">&#x27;盖茨在访韩期间将会晤韩国国防部长官金宽镇，就朝鲜近日的行动交换意见，同时商讨加强韩美两军同盟关系等问题，&#x27;</span> \<br>      <span class="hljs-string">&#x27;拟定共同应对朝鲜挑衅和核计划的方案。&#x27;</span><br>sents = SentenceSplitter.split(doc)  <span class="hljs-comment"># 分句</span><br><br><br><span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>    <span class="hljs-built_in">print</span>(sent)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs dns">据韩联社<span class="hljs-number">12</span>月<span class="hljs-number">28</span>日反映，美国防部发言人杰夫·莫莱尔<span class="hljs-number">27</span>日表示，美国防部长盖茨将于<span class="hljs-number">2011年1月14</span>日访问韩国。<br>盖茨原计划从明年<span class="hljs-number">1</span>月<span class="hljs-number">9</span>日至<span class="hljs-number">14</span>日陆续访问中国和日本，目前，他决定在行程中增加对韩国的访问。<br>莫莱尔表示，盖茨在访韩期间将会晤韩国国防部长官金宽镇，就朝鲜近日的行动交换意见，同时商讨加强韩美两军同盟关系等问题，拟定共同应对朝鲜挑衅和核计划的方案。<br></code></pre></td></tr></table></figure><h5 id="分词">分词</h5><p>分词指的是将一句话按词语分开，按词语形成独立的单元。示例的Python代码words_split.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor<br><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;/&#x27;</span>.join(words))<br><br>segmentor.release()<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">据<span class="hljs-regexp">/韩联社/</span><span class="hljs-number">12</span>月<span class="hljs-regexp">/28日/</span>反映<span class="hljs-regexp">/，/</span>美<span class="hljs-regexp">/国防部/</span>发言人<span class="hljs-regexp">/杰夫·莫莱尔/</span><span class="hljs-number">27</span>日<span class="hljs-regexp">/表示/</span>，<span class="hljs-regexp">/美/</span>国防部长<span class="hljs-regexp">/盖茨/</span>将<span class="hljs-regexp">/于/</span><span class="hljs-number">2011</span>年<span class="hljs-regexp">/1月/</span><span class="hljs-number">14</span>日<span class="hljs-regexp">/访问/</span>韩国/。<br></code></pre></td></tr></table></figure><h5 id="词性标注">词性标注</h5><p>词性标注指的是一句话分完词后，制定每个词语的词性。示例的Python代码postagger.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor, Postagger<br><br><span class="hljs-comment"># 分词</span><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br><br>postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br>postags = postagger.postag(words)  <span class="hljs-comment"># 词性标注</span><br><br><span class="hljs-keyword">for</span> word, postag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(words, postags):<br>    <span class="hljs-built_in">print</span>(word, postag)<br><br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">词性标注结果说明</span><br><span class="hljs-string">https://ltp.readthedocs.io/zh_CN/latest/appendix.html#id3</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs excel">据 p<br>韩联社 ni<br><span class="hljs-number">12</span>月 nt<br><span class="hljs-number">28</span>日 nt<br>反映 v<br>， wp<br>美 j<br>国防部 <span class="hljs-built_in">n</span><br>发言人 <span class="hljs-built_in">n</span><br>杰夫·莫莱尔 nh<br><span class="hljs-number">27</span>日 nt<br>表示 v<br>， wp<br>美 j<br>国防部长 <span class="hljs-built_in">n</span><br>盖茨 nh<br>将 d<br>于 p<br><span class="hljs-number">2011</span>年 nt<br><span class="hljs-number">1</span>月 nt<br><span class="hljs-number">14</span>日 nt<br>访问 v<br>韩国 ns<br>。 wp<br></code></pre></td></tr></table></figure><p>词性标注结果可参考网址：<ahref="https://ltp.readthedocs.io/zh_CN/latest/appendix.html">https://ltp.readthedocs.io/zh_CN/latest/appendix.html</a>。</p><h5 id="命名实体识别">命名实体识别</h5><p>命名实体识别（NER）指的是识别出一句话或一段话或一片文章中的命名实体，比如人名，地名，组织机构名。示例的Python代码ner.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor, Postagger<br><br><span class="hljs-comment"># 分词</span><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br><br>postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br>postags = postagger.postag(words)  <span class="hljs-comment"># 词性标注</span><br><br><br>ner_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/ner.model&#x27;</span>)   <span class="hljs-comment"># 命名实体识别模型路径，模型名称为`pos.model`</span><br><br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> NamedEntityRecognizer<br>recognizer = NamedEntityRecognizer() <span class="hljs-comment"># 初始化实例</span><br>recognizer.load(ner_model_path)  <span class="hljs-comment"># 加载模型</span><br><span class="hljs-comment"># netags = recognizer.recognize(words, postags)  # 命名实体识别</span><br><br><br><span class="hljs-comment"># 提取识别结果中的人名，地名，组织机构名</span><br><br>persons, places, orgs = <span class="hljs-built_in">set</span>(), <span class="hljs-built_in">set</span>(), <span class="hljs-built_in">set</span>()<br><br><br>netags = <span class="hljs-built_in">list</span>(recognizer.recognize(words, postags))  <span class="hljs-comment"># 命名实体识别</span><br><span class="hljs-built_in">print</span>(netags)<br><span class="hljs-comment"># print(netags)</span><br>i = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> tag, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(netags, words):<br>    j = i<br>    <span class="hljs-comment"># 人名</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;Nh&#x27;</span> <span class="hljs-keyword">in</span> tag:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;S&#x27;</span>):<br>            persons.add(word)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>            union_person = word<br>            <span class="hljs-keyword">while</span> netags[j] != <span class="hljs-string">&#x27;E-Nh&#x27;</span>:<br>                j += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> j &lt; <span class="hljs-built_in">len</span>(words):<br>                    union_person += words[j]<br>            persons.add(union_person)<br>    <span class="hljs-comment"># 地名</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;Ns&#x27;</span> <span class="hljs-keyword">in</span> tag:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;S&#x27;</span>):<br>            places.add(word)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>            union_place = word<br>            <span class="hljs-keyword">while</span> netags[j] != <span class="hljs-string">&#x27;E-Ns&#x27;</span>:<br>                j += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> j &lt; <span class="hljs-built_in">len</span>(words):<br>                    union_place += words[j]<br>            places.add(union_place)<br>    <span class="hljs-comment"># 机构名</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;Ni&#x27;</span> <span class="hljs-keyword">in</span> tag:<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;S&#x27;</span>):<br>            orgs.add(word)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">str</span>(tag).startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>            union_org = word<br>            <span class="hljs-keyword">while</span> netags[j] != <span class="hljs-string">&#x27;E-Ni&#x27;</span>:<br>                j += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> j &lt; <span class="hljs-built_in">len</span>(words):<br>                    union_org += words[j]<br>            orgs.add(union_org)<br><br>    i += <span class="hljs-number">1</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;人名：&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>.join(persons))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;地名：&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>.join(places))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;组织机构：&#x27;</span>, <span class="hljs-string">&#x27;，&#x27;</span>.join(orgs))<br><br><br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br>recognizer.release()<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Ni&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-Ni&#x27;</span>, <span class="hljs-string">&#x27;E-Ni&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Nh&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Ns&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Nh&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;S-Ns&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>人名： 杰夫·莫莱尔，盖茨<br>地名： 美，韩国<br>组织机构： 韩联社，美国防部<br></code></pre></td></tr></table></figure><p>命名实体识别结果可参考网址：<ahref="https://ltp.readthedocs.io/zh_CN/latest/appendix.html">https://ltp.readthedocs.io/zh_CN/latest/appendix.html</a>。</p><h4 id="依存句法分析">依存句法分析</h4><p>依存语法 (Dependency Parsing, DP)通过分析语言单位内成分之间的依存关系揭示其句法结构。直观来讲，依存句法分析识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关系。示例的Python代码parser.py代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor, Postagger, Parser<br><br><span class="hljs-comment"># 分词</span><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br><br>postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br>postags = postagger.postag(words)  <span class="hljs-comment"># 词性标注</span><br><br><br><span class="hljs-comment"># 依存句法分析</span><br>par_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/parser.model&#x27;</span>)  <span class="hljs-comment"># 模型路径，模型名称为`parser.model`</span><br><br>parser = Parser() <span class="hljs-comment"># 初始化实例</span><br>parser.load(par_model_path)  <span class="hljs-comment"># 加载模型</span><br>arcs = parser.parse(words, postags)  <span class="hljs-comment"># 句法分析</span><br><br>rely_id = [arc.head <span class="hljs-keyword">for</span> arc <span class="hljs-keyword">in</span> arcs]  <span class="hljs-comment"># 提取依存父节点id</span><br>relation = [arc.relation <span class="hljs-keyword">for</span> arc <span class="hljs-keyword">in</span> arcs]  <span class="hljs-comment"># 提取依存关系</span><br>heads = [<span class="hljs-string">&#x27;Root&#x27;</span> <span class="hljs-keyword">if</span> <span class="hljs-built_in">id</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> words[<span class="hljs-built_in">id</span>-<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> rely_id]  <span class="hljs-comment"># 匹配依存父节点词语</span><br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(words)):<br>    <span class="hljs-built_in">print</span>(relation[i] + <span class="hljs-string">&#x27;(&#x27;</span> + words[i] + <span class="hljs-string">&#x27;, &#x27;</span> + heads[i] + <span class="hljs-string">&#x27;)&#x27;</span>)<br><br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br>parser.release()<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(据, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">SBV</span><span class="hljs-params">(韩联社, 反映)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">12</span>月, <span class="hljs-number">28</span>日)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(<span class="hljs-number">28</span>日, 反映)</span></span><br><span class="hljs-function"><span class="hljs-title">POB</span><span class="hljs-params">(反映, 据)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(，, 据)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(美, 国防部)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(国防部, 发言人)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(发言人, 杰夫·莫莱尔)</span></span><br><span class="hljs-function"><span class="hljs-title">SBV</span><span class="hljs-params">(杰夫·莫莱尔, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(<span class="hljs-number">27</span>日, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">HED</span><span class="hljs-params">(表示, Root)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(，, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(美, 国防部长)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(国防部长, 盖茨)</span></span><br><span class="hljs-function"><span class="hljs-title">SBV</span><span class="hljs-params">(盖茨, 访问)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(将, 访问)</span></span><br><span class="hljs-function"><span class="hljs-title">ADV</span><span class="hljs-params">(于, 访问)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">2011</span>年, <span class="hljs-number">14</span>日)</span></span><br><span class="hljs-function"><span class="hljs-title">ATT</span><span class="hljs-params">(<span class="hljs-number">1</span>月, <span class="hljs-number">14</span>日)</span></span><br><span class="hljs-function"><span class="hljs-title">POB</span><span class="hljs-params">(<span class="hljs-number">14</span>日, 于)</span></span><br><span class="hljs-function"><span class="hljs-title">VOB</span><span class="hljs-params">(访问, 表示)</span></span><br><span class="hljs-function"><span class="hljs-title">VOB</span><span class="hljs-params">(韩国, 访问)</span></span><br><span class="hljs-function"><span class="hljs-title">WP</span><span class="hljs-params">(。, 表示)</span></span><br></code></pre></td></tr></table></figure><p>依存句法分析结果可参考网址：<ahref="https://ltp.readthedocs.io/zh_CN/latest/appendix.html">https://ltp.readthedocs.io/zh_CN/latest/appendix.html</a>。</p><h5 id="语义角色标注">语义角色标注</h5><p>语义角色标注是实现浅层语义分析的一种方式。在一个句子中，谓词是对主语的陈述或说明，指出“做什么”、“是什么”或“怎么样，代表了一个事件的核心，跟谓词搭配的名词称为论元。语义角色是指论元在动词所指事件中担任的角色。主要有：施事者（Agent）、受事者（Patient）、客体（Theme）、经验者（Experiencer）、受益者（Beneficiary）、工具（Instrument）、处所（Location）、目标（Goal）和来源（Source）等。示例的Python代码rolelabel.py如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> pyltp <span class="hljs-keyword">import</span> Segmentor, Postagger, Parser, SementicRoleLabeller<br><br><span class="hljs-comment"># 分词</span><br>cws_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/cws.model&#x27;</span>)  <span class="hljs-comment"># 分词模型路径，模型名称为`cws.model`</span><br>lexicon_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/lexicon.txt&#x27;</span>)  <span class="hljs-comment"># 参数lexicon是自定义词典的文件路径</span><br><br>segmentor = Segmentor()<br>segmentor.load_with_lexicon(cws_model_path, lexicon_path)<br><br>sent = <span class="hljs-string">&#x27;据韩联社12月28日反映，美国防部发言人杰夫·莫莱尔27日表示，美国防部长盖茨将于2011年1月14日访问韩国。&#x27;</span><br>words = segmentor.segment(sent)  <span class="hljs-comment"># 分词</span><br><br><span class="hljs-comment"># 词性标注</span><br>pos_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pos.model&#x27;</span>)  <span class="hljs-comment"># 词性标注模型路径，模型名称为`pos.model`</span><br><br>postagger = Postagger()  <span class="hljs-comment"># 初始化实例</span><br>postagger.load(pos_model_path)  <span class="hljs-comment"># 加载模型</span><br>postags = postagger.postag(words)  <span class="hljs-comment"># 词性标注</span><br><br><span class="hljs-comment"># 依存句法分析</span><br>par_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/parser.model&#x27;</span>)  <span class="hljs-comment"># 模型路径，模型名称为`parser.model`</span><br><br>parser = Parser() <span class="hljs-comment"># 初始化实例</span><br>parser.load(par_model_path)  <span class="hljs-comment"># 加载模型</span><br>arcs = parser.parse(words, postags)  <span class="hljs-comment"># 句法分析</span><br><br><span class="hljs-comment"># 语义角色标注</span><br>srl_model_path = os.path.join(os.path.dirname(__file__), <span class="hljs-string">&#x27;data/pisrl.model&#x27;</span>)  <span class="hljs-comment"># 语义角色标注模型目录路径</span><br>labeller = SementicRoleLabeller() <span class="hljs-comment"># 初始化实例</span><br>labeller.load(srl_model_path)  <span class="hljs-comment"># 加载模型</span><br>roles = labeller.label(words, postags, arcs)  <span class="hljs-comment"># 语义角色标注</span><br><br><span class="hljs-comment"># 打印结果</span><br><span class="hljs-keyword">for</span> role <span class="hljs-keyword">in</span> roles:<br>    <span class="hljs-built_in">print</span>(words[role.index], end=<span class="hljs-string">&#x27; &#x27;</span>)<br>    <span class="hljs-built_in">print</span>(role.index, <span class="hljs-string">&quot;&quot;</span>.join([<span class="hljs-string">&quot;%s:(%d,%d)&quot;</span> % (arg.name, arg.<span class="hljs-built_in">range</span>.start, arg.<span class="hljs-built_in">range</span>.end) <span class="hljs-keyword">for</span> arg <span class="hljs-keyword">in</span> role.arguments]))<br><br><span class="hljs-comment"># 释放模型</span><br>segmentor.release()<br>postagger.release()<br>parser.release()<br>labeller.release()<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">反映 <span class="hljs-number">4</span> <span class="hljs-built_in">A0</span>:(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<span class="hljs-built_in">A0</span>:(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)<br>表示 <span class="hljs-number">11</span> MNR:(<span class="hljs-number">0</span>,<span class="hljs-number">5</span>)<span class="hljs-built_in">A0</span>:(<span class="hljs-number">6</span>,<span class="hljs-number">9</span>)TMP:(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>)<span class="hljs-built_in">A1</span>:(<span class="hljs-number">13</span>,<span class="hljs-number">22</span>)<br>访问 <span class="hljs-number">21</span> <span class="hljs-built_in">A0</span>:(<span class="hljs-number">13</span>,<span class="hljs-number">15</span>)ADV:(<span class="hljs-number">16</span>,<span class="hljs-number">16</span>)TMP:(<span class="hljs-number">17</span>,<span class="hljs-number">20</span>)<span class="hljs-built_in">A1</span>:(<span class="hljs-number">22</span>,<span class="hljs-number">22</span>)<br></code></pre></td></tr></table></figure><h3 id="总结">总结</h3><p>本文介绍了中文NLP的一个杰出工具pyltp，并给出了该模块的各个功能的一个示例，希望能给读者一些思考与启示。本文到此结束，感谢大家阅读~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>NLP工具</tag>
      
      <tag>pyltp</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（五）用深度学习实现命名实体识别（NER）</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%94%EF%BC%89%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%94%EF%BC%89%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>在文章：<ahref="https://percent4.github.io/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/">NLP入门（四）命名实体识别（NER）</a>中，笔者介绍了两个实现命名实体识别的工具——NLTK和StanfordNLP。在本文中，我们将会学习到如何使用深度学习工具来自己一步步地实现NER，只要你坚持看完，就一定会很有收获的。</p><p>OK，话不多说，让我们进入正题。</p><p>几乎所有的NLP都依赖一个强大的语料库，本项目实现NER的语料库如下(文件名为train.txt，一共42000行，这里只展示前15行，可以在文章最后的Github地址下载该语料库)：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs bash">playedonMonday(hometeam<span class="hljs-keyword">in</span>CAPS):<br>VBDINNNP(NNNNINNNP):<br>OOOOOOOOOO<br>AmericanLeague<br>NNPNNP<br>B-MISCI-MISC<br>Cleveland2DETROIT1<br>NNPCDNNPCD<br>B-ORGOB-ORGO<br>BALTIMORE12Oakland11(10innings)<br>VBCDNNPCD(CDNN)<br>B-ORGOB-ORGOOOOO<br>TORONTO5Minnesota3<br>TOCDNNPCD<br>B-ORGOB-ORGO<br>......<br></code></pre></td></tr></table></figure><p>简单介绍下该语料库的结构：该语料库一共42000行，每三行为一组，其中，第一行为英语句子，第二行为每个句子的词性（关于英语单词的词性，可参考文章：<ahref="https://percent4.github.io/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F%EF%BC%88Lemmatization%EF%BC%89/">NLP入门（三）词形还原（Lemmatization）</a>），第三行为NER系统的标注，具体的含义会在之后介绍。</p><p>我们的NER项目的名称为DL_4_NER，结构如下：</p><figure><img src="/img/nlp5_1.png" alt="NER项目名称" /><figcaption aria-hidden="true">NER项目名称</figcaption></figure><p>项目中每个文件的功能如下：</p><ul><li><p>utils.py: 项目配置及数据导入</p></li><li><p>data_processing.py: 数据探索</p></li><li><p>Bi_LSTM_Model_training.py: 模型创建及训练</p></li><li><p>Bi_LSTM_Model_predict.py: 对新句子进行NER预测</p><p>接下来，笔者将结合代码文件，分部介绍该项目的步骤，当所有步骤介绍完毕后，我们的项目就结束了，而你，也就知道了如何用深度学习实现命名实体识别（NER）。</p><p>Let's begin!</p></li></ul><h3 id="项目配置">项目配置</h3><p>第一步，是项目的配置及数据导入，在utils.py文件中实现，完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><span class="hljs-comment"># basic settings for DL_4_NER Project</span><br>BASE_DIR = <span class="hljs-string">&quot;F://NERSystem&quot;</span><br>CORPUS_PATH = <span class="hljs-string">&quot;%s/train.txt&quot;</span> % BASE_DIR<br><br>KERAS_MODEL_SAVE_PATH = <span class="hljs-string">&#x27;%s/Bi-LSTM-4-NER.h5&#x27;</span> % BASE_DIR<br>WORD_DICTIONARY_PATH = <span class="hljs-string">&#x27;%s/word_dictionary.pk&#x27;</span> % BASE_DIR<br>InVERSE_WORD_DICTIONARY_PATH = <span class="hljs-string">&#x27;%s/inverse_word_dictionary.pk&#x27;</span> % BASE_DIR<br>LABEL_DICTIONARY_PATH = <span class="hljs-string">&#x27;%s/label_dictionary.pk&#x27;</span> % BASE_DIR<br>OUTPUT_DICTIONARY_PATH = <span class="hljs-string">&#x27;%s/output_dictionary.pk&#x27;</span> % BASE_DIR<br><br>CONSTANTS = [<br>             KERAS_MODEL_SAVE_PATH,<br>             InVERSE_WORD_DICTIONARY_PATH,<br>             WORD_DICTIONARY_PATH,<br>             LABEL_DICTIONARY_PATH,<br>             OUTPUT_DICTIONARY_PATH<br>             ]<br><br><span class="hljs-comment"># load data from corpus to from pandas DataFrame</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>():<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CORPUS_PATH, <span class="hljs-string">&#x27;r&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        text_data = [text.strip() <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> f.readlines()]<br>    text_data = [text_data[k].split(<span class="hljs-string">&#x27;\t&#x27;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(text_data))]<br>    index = <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(text_data), <span class="hljs-number">3</span>)<br><br>    <span class="hljs-comment"># Transforming data to matrix format for neural network</span><br>    input_data = <span class="hljs-built_in">list</span>()<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(index) - <span class="hljs-number">1</span>):<br>        rows = text_data[index[i-<span class="hljs-number">1</span>]:index[i]]<br>        sentence_no = np.array([i]*<span class="hljs-built_in">len</span>(rows[<span class="hljs-number">0</span>]), dtype=<span class="hljs-built_in">str</span>)<br>        rows.append(sentence_no)<br>        rows = np.array(rows).T<br>        input_data.append(rows)<br><br>    input_data = pd.DataFrame(np.concatenate([item <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> input_data]),\<br>                               columns=[<span class="hljs-string">&#x27;word&#x27;</span>, <span class="hljs-string">&#x27;pos&#x27;</span>, <span class="hljs-string">&#x27;tag&#x27;</span>, <span class="hljs-string">&#x27;sent_no&#x27;</span>])<br><br>    <span class="hljs-keyword">return</span> input_data<br></code></pre></td></tr></table></figure><p>在该代码中，先是设置了语料库文件的路径CORPUS_PATH，KERAS模型保存路径KERAS_MODEL_SAVE_PATH，以及在项目过程中会用到的三个字典的保存路径（以pickle文件形式保存）WORD_DICTIONARY_PATH，LABEL_DICTIONARY_PATH，OUTPUT_DICTIONARY_PATH。然后是load_data()函数，它将语料库中的文本以Pandas中的DataFrame结构展示出来，该数据框的前30行如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs apache">         <span class="hljs-attribute">word</span>  pos     tag sent_no<br><span class="hljs-attribute">0</span>      played  VBD       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">1</span>          <span class="hljs-literal">on</span>   IN       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">2</span>      Monday  NNP       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">3</span>           (    (       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">4</span>        home   NN       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">5</span>        team   NN       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">6</span>          in   IN       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">7</span>        CAPS  NNP       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">8</span>           )    )       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">9</span>           :    :       O       <span class="hljs-number">1</span><br><span class="hljs-attribute">10</span>   American  NNP  B-MISC       <span class="hljs-number">2</span><br><span class="hljs-attribute">11</span>     League  NNP  I-MISC       <span class="hljs-number">2</span><br><span class="hljs-attribute">12</span>  Cleveland  NNP   B-ORG       <span class="hljs-number">3</span><br><span class="hljs-attribute">13</span>          <span class="hljs-number">2</span>   CD       O       <span class="hljs-number">3</span><br><span class="hljs-attribute">14</span>    DETROIT  NNP   B-ORG       <span class="hljs-number">3</span><br><span class="hljs-attribute">15</span>          <span class="hljs-number">1</span>   CD       O       <span class="hljs-number">3</span><br><span class="hljs-attribute">16</span>  BALTIMORE   VB   B-ORG       <span class="hljs-number">4</span><br><span class="hljs-attribute">17</span>         <span class="hljs-number">12</span>   CD       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">18</span>    Oakland  NNP   B-ORG       <span class="hljs-number">4</span><br><span class="hljs-attribute">19</span>         <span class="hljs-number">11</span>   CD       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">20</span>          (    (       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">21</span>         <span class="hljs-number">10</span>   CD       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">22</span>    innings   NN       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">23</span>          )    )       O       <span class="hljs-number">4</span><br><span class="hljs-attribute">24</span>    TORONTO   TO   B-ORG       <span class="hljs-number">5</span><br><span class="hljs-attribute">25</span>          <span class="hljs-number">5</span>   CD       O       <span class="hljs-number">5</span><br><span class="hljs-attribute">26</span>  Minnesota  NNP   B-ORG       <span class="hljs-number">5</span><br><span class="hljs-attribute">27</span>          <span class="hljs-number">3</span>   CD       O       <span class="hljs-number">5</span><br><span class="hljs-attribute">28</span>  Milwaukee  NNP   B-ORG       <span class="hljs-number">6</span><br><span class="hljs-attribute">29</span>          <span class="hljs-number">3</span>   CD       O       <span class="hljs-number">6</span><br></code></pre></td></tr></table></figure><p>在该数据框中，word这一列表示文本语料库中的单词，pos这一列表示该单词的词性，tag这一列表示NER的标注，sent_no这一列表示该单词在第几个句子中。</p><h3 id="数据探索">数据探索</h3><p>接着，第二步是数据探索，即对输入的数据（input_data）进行一些数据review，完整的代码（data_processing.py）如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> accumulate<br><span class="hljs-keyword">from</span> operator <span class="hljs-keyword">import</span> itemgetter<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> matplotlib <span class="hljs-keyword">as</span> mpl<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> BASE_DIR, CONSTANTS, load_data<br><br><span class="hljs-comment"># 设置matplotlib绘图时的字体</span><br>mpl.rcParams[<span class="hljs-string">&#x27;font.sans-serif&#x27;</span>]=[<span class="hljs-string">&#x27;SimHei&#x27;</span>]<br><br><span class="hljs-comment"># 数据查看</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_review</span>():<br><br>    <span class="hljs-comment"># 数据导入</span><br>    input_data = load_data()<br><br>    <span class="hljs-comment"># 基本的数据review</span><br>    sent_num = input_data[<span class="hljs-string">&#x27;sent_no&#x27;</span>].astype(np.<span class="hljs-built_in">int</span>).<span class="hljs-built_in">max</span>()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;一共有%s个句子。\n&quot;</span>%sent_num)<br><br>    vocabulary = input_data[<span class="hljs-string">&#x27;word&#x27;</span>].unique()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;一共有%d个单词。&quot;</span>%<span class="hljs-built_in">len</span>(vocabulary))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;前10个单词为：%s.\n&quot;</span>%vocabulary[:<span class="hljs-number">11</span>])<br><br>    pos_arr = input_data[<span class="hljs-string">&#x27;pos&#x27;</span>].unique()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;单词的词性列表：%s.\n&quot;</span>%pos_arr)<br><br>    ner_tag_arr = input_data[<span class="hljs-string">&#x27;tag&#x27;</span>].unique()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;NER的标注列表：%s.\n&quot;</span> % ner_tag_arr)<br><br>    df = input_data[[<span class="hljs-string">&#x27;word&#x27;</span>, <span class="hljs-string">&#x27;sent_no&#x27;</span>]].groupby(<span class="hljs-string">&#x27;sent_no&#x27;</span>).count()<br>    sent_len_list = df[<span class="hljs-string">&#x27;word&#x27;</span>].tolist()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;句子长度及出现频数字典：\n%s.&quot;</span> % <span class="hljs-built_in">dict</span>(Counter(sent_len_list)))<br><br>    <span class="hljs-comment"># 绘制句子长度及出现频数统计图</span><br>    sort_sent_len_dist = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">dict</span>(Counter(sent_len_list)).items(), key=itemgetter(<span class="hljs-number">0</span>))<br>    sent_no_data = [item[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sort_sent_len_dist]<br>    sent_count_data = [item[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sort_sent_len_dist]<br>    plt.bar(sent_no_data, sent_count_data)<br>    plt.title(<span class="hljs-string">&quot;句子长度及出现频数统计图&quot;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;句子长度&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;句子长度出现的频数&quot;</span>)<br>    plt.savefig(<span class="hljs-string">&quot;%s/句子长度及出现频数统计图.png&quot;</span> % BASE_DIR)<br>    plt.close()<br><br>    <span class="hljs-comment"># 绘制句子长度累积分布函数(CDF)</span><br>    sent_pentage_list = [(count/sent_num) <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> accumulate(sent_count_data)]<br><br>    <span class="hljs-comment"># 寻找分位点为quantile的句子长度</span><br>    quantile = <span class="hljs-number">0.9992</span><br>    <span class="hljs-comment">#print(list(sent_pentage_list))</span><br>    <span class="hljs-keyword">for</span> length, per <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(sent_no_data, sent_pentage_list):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">round</span>(per, <span class="hljs-number">4</span>) == quantile:<br>            index = length<br>            <span class="hljs-keyword">break</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\n分位点为%s的句子长度:%d.&quot;</span> % (quantile, index))<br><br>    <span class="hljs-comment"># 绘制CDF</span><br>    plt.plot(sent_no_data, sent_pentage_list)<br>    plt.hlines(quantile, <span class="hljs-number">0</span>, index, colors=<span class="hljs-string">&quot;c&quot;</span>, linestyles=<span class="hljs-string">&quot;dashed&quot;</span>)<br>    plt.vlines(index, <span class="hljs-number">0</span>, quantile, colors=<span class="hljs-string">&quot;c&quot;</span>, linestyles=<span class="hljs-string">&quot;dashed&quot;</span>)<br>    plt.text(<span class="hljs-number">0</span>, quantile, <span class="hljs-built_in">str</span>(quantile))<br>    plt.text(index, <span class="hljs-number">0</span>, <span class="hljs-built_in">str</span>(index))<br>    plt.title(<span class="hljs-string">&quot;句子长度累积分布函数图&quot;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;句子长度&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;句子长度累积频率&quot;</span>)<br>    plt.savefig(<span class="hljs-string">&quot;%s/句子长度累积分布函数图.png&quot;</span> % BASE_DIR)<br>    plt.close()<br><br><span class="hljs-comment"># 数据处理</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_processing</span>():<br>    <span class="hljs-comment"># 数据导入</span><br>    input_data = load_data()<br><br>    <span class="hljs-comment"># 标签及词汇表</span><br>    labels, vocabulary = <span class="hljs-built_in">list</span>(input_data[<span class="hljs-string">&#x27;tag&#x27;</span>].unique()), <span class="hljs-built_in">list</span>(input_data[<span class="hljs-string">&#x27;word&#x27;</span>].unique())<br><br>    <span class="hljs-comment"># 字典列表</span><br>    word_dictionary = &#123;word: i+<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocabulary)&#125;<br>    inverse_word_dictionary = &#123;i+<span class="hljs-number">1</span>: word <span class="hljs-keyword">for</span> i, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(vocabulary)&#125;<br>    label_dictionary = &#123;label: i+<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels)&#125;<br>    output_dictionary = &#123;i+<span class="hljs-number">1</span>: labels <span class="hljs-keyword">for</span> i, labels <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels)&#125;<br><br>    dict_list = [word_dictionary, inverse_word_dictionary,label_dictionary, output_dictionary]<br><br>    <span class="hljs-comment"># 保存为pickle形式</span><br>    <span class="hljs-keyword">for</span> dict_item, path <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(dict_list, CONSTANTS[<span class="hljs-number">1</span>:]):<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(path, <span class="hljs-string">&#x27;wb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            pickle.dump(dict_item, f)<br><br><span class="hljs-comment">#data_review()</span><br></code></pre></td></tr></table></figure><p>调用data_review()函数，输出的结果如下：</p><figure class="highlight prolog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs prolog">一共有<span class="hljs-number">13998</span>个句子。<br><br>一共有<span class="hljs-number">24339</span>个单词。<br>前<span class="hljs-number">10</span>个单词为：[<span class="hljs-string">&#x27;played&#x27;</span> <span class="hljs-string">&#x27;on&#x27;</span> <span class="hljs-string">&#x27;Monday&#x27;</span> <span class="hljs-string">&#x27;(&#x27;</span> <span class="hljs-string">&#x27;home&#x27;</span> <span class="hljs-string">&#x27;team&#x27;</span> <span class="hljs-string">&#x27;in&#x27;</span> <span class="hljs-string">&#x27;CAPS&#x27;</span> <span class="hljs-string">&#x27;)&#x27;</span> <span class="hljs-string">&#x27;:&#x27;</span> <span class="hljs-string">&#x27;American&#x27;</span>].<br><br>单词的词性列表：[<span class="hljs-string">&#x27;VBD&#x27;</span> <span class="hljs-string">&#x27;IN&#x27;</span> <span class="hljs-string">&#x27;NNP&#x27;</span> <span class="hljs-string">&#x27;(&#x27;</span> <span class="hljs-string">&#x27;NN&#x27;</span> <span class="hljs-string">&#x27;)&#x27;</span> <span class="hljs-string">&#x27;:&#x27;</span> <span class="hljs-string">&#x27;CD&#x27;</span> <span class="hljs-string">&#x27;VB&#x27;</span> <span class="hljs-string">&#x27;TO&#x27;</span> <span class="hljs-string">&#x27;NNS&#x27;</span> <span class="hljs-string">&#x27;,&#x27;</span> <span class="hljs-string">&#x27;VBP&#x27;</span> <span class="hljs-string">&#x27;VBZ&#x27;</span><br> <span class="hljs-string">&#x27;.&#x27;</span> <span class="hljs-string">&#x27;VBG&#x27;</span> <span class="hljs-string">&#x27;PRP$&#x27;</span> <span class="hljs-string">&#x27;JJ&#x27;</span> <span class="hljs-string">&#x27;CC&#x27;</span> <span class="hljs-string">&#x27;JJS&#x27;</span> <span class="hljs-string">&#x27;RB&#x27;</span> <span class="hljs-string">&#x27;DT&#x27;</span> <span class="hljs-string">&#x27;VBN&#x27;</span> <span class="hljs-string">&#x27;&quot;&#x27;</span> <span class="hljs-string">&#x27;PRP&#x27;</span> <span class="hljs-string">&#x27;WDT&#x27;</span> <span class="hljs-string">&#x27;WRB&#x27;</span><br> <span class="hljs-string">&#x27;MD&#x27;</span> <span class="hljs-string">&#x27;WP&#x27;</span> <span class="hljs-string">&#x27;POS&#x27;</span> <span class="hljs-string">&#x27;JJR&#x27;</span> <span class="hljs-string">&#x27;WP$&#x27;</span> <span class="hljs-string">&#x27;RP&#x27;</span> <span class="hljs-string">&#x27;NNPS&#x27;</span> <span class="hljs-string">&#x27;RBS&#x27;</span> <span class="hljs-string">&#x27;FW&#x27;</span> <span class="hljs-string">&#x27;$&#x27;</span> <span class="hljs-string">&#x27;RBR&#x27;</span> <span class="hljs-string">&#x27;EX&#x27;</span> <span class="hljs-string">&quot;&#x27;&#x27;&quot;</span><br> <span class="hljs-string">&#x27;PDT&#x27;</span> <span class="hljs-string">&#x27;UH&#x27;</span> <span class="hljs-string">&#x27;SYM&#x27;</span> <span class="hljs-string">&#x27;LS&#x27;</span> <span class="hljs-string">&#x27;NN|SYM&#x27;</span>].<br><br><span class="hljs-symbol">NER</span>的标注列表：[<span class="hljs-string">&#x27;O&#x27;</span> <span class="hljs-string">&#x27;B-MISC&#x27;</span> <span class="hljs-string">&#x27;I-MISC&#x27;</span> <span class="hljs-string">&#x27;B-ORG&#x27;</span> <span class="hljs-string">&#x27;I-ORG&#x27;</span> <span class="hljs-string">&#x27;B-PER&#x27;</span> <span class="hljs-string">&#x27;B-LOC&#x27;</span> <span class="hljs-string">&#x27;I-PER&#x27;</span> <span class="hljs-string">&#x27;I-LOC&#x27;</span><br> <span class="hljs-string">&#x27;sO&#x27;</span>].<br><br>句子长度及出现频数字典：<br>&#123;<span class="hljs-number">1</span>: <span class="hljs-number">177</span>, <span class="hljs-number">2</span>: <span class="hljs-number">1141</span>, <span class="hljs-number">3</span>: <span class="hljs-number">620</span>, <span class="hljs-number">4</span>: <span class="hljs-number">794</span>, <span class="hljs-number">5</span>: <span class="hljs-number">769</span>, <span class="hljs-number">6</span>: <span class="hljs-number">639</span>, <span class="hljs-number">7</span>: <span class="hljs-number">999</span>, <span class="hljs-number">8</span>: <span class="hljs-number">977</span>, <span class="hljs-number">9</span>: <span class="hljs-number">841</span>, <span class="hljs-number">10</span>: <span class="hljs-number">501</span>, <span class="hljs-number">11</span>: <span class="hljs-number">395</span>, <span class="hljs-number">12</span>: <span class="hljs-number">316</span>, <span class="hljs-number">13</span>: <span class="hljs-number">339</span>, <span class="hljs-number">14</span>: <span class="hljs-number">291</span>, <span class="hljs-number">15</span>: <span class="hljs-number">275</span>, <span class="hljs-number">16</span>: <span class="hljs-number">225</span>, <span class="hljs-number">17</span>: <span class="hljs-number">229</span>, <span class="hljs-number">18</span>: <span class="hljs-number">212</span>, <span class="hljs-number">19</span>: <span class="hljs-number">197</span>, <span class="hljs-number">20</span>: <span class="hljs-number">221</span>, <span class="hljs-number">21</span>: <span class="hljs-number">228</span>, <span class="hljs-number">22</span>: <span class="hljs-number">221</span>, <span class="hljs-number">23</span>: <span class="hljs-number">230</span>, <span class="hljs-number">24</span>: <span class="hljs-number">210</span>, <span class="hljs-number">25</span>: <span class="hljs-number">207</span>, <span class="hljs-number">26</span>: <span class="hljs-number">224</span>, <span class="hljs-number">27</span>: <span class="hljs-number">188</span>, <span class="hljs-number">28</span>: <span class="hljs-number">199</span>, <span class="hljs-number">29</span>: <span class="hljs-number">214</span>, <span class="hljs-number">30</span>: <span class="hljs-number">183</span>, <span class="hljs-number">31</span>: <span class="hljs-number">202</span>, <span class="hljs-number">32</span>: <span class="hljs-number">167</span>, <span class="hljs-number">33</span>: <span class="hljs-number">167</span>, <span class="hljs-number">34</span>: <span class="hljs-number">141</span>, <span class="hljs-number">35</span>: <span class="hljs-number">130</span>, <span class="hljs-number">36</span>: <span class="hljs-number">119</span>, <span class="hljs-number">37</span>: <span class="hljs-number">105</span>, <span class="hljs-number">38</span>: <span class="hljs-number">112</span>, <span class="hljs-number">39</span>: <span class="hljs-number">98</span>, <span class="hljs-number">40</span>: <span class="hljs-number">78</span>, <span class="hljs-number">41</span>: <span class="hljs-number">74</span>, <span class="hljs-number">42</span>: <span class="hljs-number">63</span>, <span class="hljs-number">43</span>: <span class="hljs-number">51</span>, <span class="hljs-number">44</span>: <span class="hljs-number">42</span>, <span class="hljs-number">45</span>: <span class="hljs-number">39</span>, <span class="hljs-number">46</span>: <span class="hljs-number">19</span>, <span class="hljs-number">47</span>: <span class="hljs-number">22</span>, <span class="hljs-number">48</span>: <span class="hljs-number">19</span>, <span class="hljs-number">49</span>: <span class="hljs-number">15</span>, <span class="hljs-number">50</span>: <span class="hljs-number">16</span>, <span class="hljs-number">51</span>: <span class="hljs-number">8</span>, <span class="hljs-number">52</span>: <span class="hljs-number">9</span>, <span class="hljs-number">53</span>: <span class="hljs-number">5</span>, <span class="hljs-number">54</span>: <span class="hljs-number">4</span>, <span class="hljs-number">55</span>: <span class="hljs-number">9</span>, <span class="hljs-number">56</span>: <span class="hljs-number">2</span>, <span class="hljs-number">57</span>: <span class="hljs-number">2</span>, <span class="hljs-number">58</span>: <span class="hljs-number">2</span>, <span class="hljs-number">59</span>: <span class="hljs-number">2</span>, <span class="hljs-number">60</span>: <span class="hljs-number">3</span>, <span class="hljs-number">62</span>: <span class="hljs-number">2</span>, <span class="hljs-number">66</span>: <span class="hljs-number">1</span>, <span class="hljs-number">67</span>: <span class="hljs-number">1</span>, <span class="hljs-number">69</span>: <span class="hljs-number">1</span>, <span class="hljs-number">71</span>: <span class="hljs-number">1</span>, <span class="hljs-number">72</span>: <span class="hljs-number">1</span>, <span class="hljs-number">78</span>: <span class="hljs-number">1</span>, <span class="hljs-number">80</span>: <span class="hljs-number">1</span>, <span class="hljs-number">113</span>: <span class="hljs-number">1</span>, <span class="hljs-number">124</span>: <span class="hljs-number">1</span>&#125;.<br><br>分位点为<span class="hljs-number">0.9992</span>的句子长度:<span class="hljs-number">60.</span><br></code></pre></td></tr></table></figure><p>在该语料库中，一共有13998个句子，比预期的42000/3=14000个句子少两个。一个有24339个单词，单词量还是蛮大的，当然，这里对单词没有做任何处理，直接保留了语料库中的形式（后期可以继续优化）。单词的词性可以参考文章：<ahref="https://www.jianshu.com/p/79255fe0c5b5">NLP入门（三）词形还原（Lemmatization）</a>。我们需要注意的是，NER的标注列表为['O','B-MISC', 'I-MISC', 'B-ORG' ,'I-ORG', 'B-PER' ,'B-LOC' ,'I-PER','I-LOC','sO']，因此，本项目的NER一共分为四类：PER（人名），LOC（位置），ORG（组织）以及MISC，其中B表示开始，I表示中间，O表示单字词，不计入NER，sO表示特殊单字词。</p><p>接下来，让我们考虑下句子的长度，这对后面的建模时填充的句子长度有有参考作用。句子长度及出现频数的统计图如下：</p><figure><img src="/img/nlp5_2.png" alt="句子长度及出现频数统计图" /><figcaption aria-hidden="true">句子长度及出现频数统计图</figcaption></figure><p>可以看到，句子长度基本在60以下，当然，这也可以在输出的句子长度及出现频数字典中看到。那么，我们是否可以选在一个标准作为后面模型的句子填充的长度呢？答案是，利用出现频数的累计分布函数的分位点，在这里，我们选择分位点为0.9992,对应的句子长度为60，如下图：</p><figure><img src="/img/nlp5_3.png" alt="句子长度累积分布函数图" /><figcaption aria-hidden="true">句子长度累积分布函数图</figcaption></figure><p>接着是数据处理函数data_processing()，它的功能主要是实现单词、标签字典，并保存为pickle文件形式，便于后续直接调用。</p><h3 id="建模">建模</h3><p>在第三步中，我们建立Bi-LSTM模型来训练训练，完整的Python代码（Bi_LSTM_Model_training.py）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> BASE_DIR, CONSTANTS, load_data<br><span class="hljs-keyword">from</span> data_processing <span class="hljs-keyword">import</span> data_processing<br><span class="hljs-keyword">from</span> keras.utils <span class="hljs-keyword">import</span> np_utils, plot_model<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Bidirectional, LSTM, Dense, Embedding, TimeDistributed<br><br><br><span class="hljs-comment"># 模型输入数据</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">input_data_for_model</span>(<span class="hljs-params">input_shape</span>):<br><br>    <span class="hljs-comment"># 数据导入</span><br>    input_data = load_data()<br>    <span class="hljs-comment"># 数据处理</span><br>    data_processing()<br>    <span class="hljs-comment"># 导入字典</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">1</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        word_dictionary = pickle.load(f)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">2</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        inverse_word_dictionary = pickle.load(f)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">3</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        label_dictionary = pickle.load(f)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">4</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>        output_dictionary = pickle.load(f)<br>    vocab_size = <span class="hljs-built_in">len</span>(word_dictionary.keys())<br>    label_size = <span class="hljs-built_in">len</span>(label_dictionary.keys())<br><br>    <span class="hljs-comment"># 处理输入数据</span><br>    aggregate_function = <span class="hljs-keyword">lambda</span> <span class="hljs-built_in">input</span>: [(word, pos, label) <span class="hljs-keyword">for</span> word, pos, label <span class="hljs-keyword">in</span><br>                                            <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;word&#x27;</span>].values.tolist(),<br>                                                <span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;pos&#x27;</span>].values.tolist(),<br>                                                <span class="hljs-built_in">input</span>[<span class="hljs-string">&#x27;tag&#x27;</span>].values.tolist())]<br><br>    grouped_input_data = input_data.groupby(<span class="hljs-string">&#x27;sent_no&#x27;</span>).apply(aggregate_function)<br>    sentences = [sentence <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> grouped_input_data]<br><br>    x = [[word_dictionary[word[<span class="hljs-number">0</span>]] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sentences]<br>    x = pad_sequences(maxlen=input_shape, sequences=x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br>    y = [[label_dictionary[word[<span class="hljs-number">2</span>]] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sent] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sentences]<br>    y = pad_sequences(maxlen=input_shape, sequences=y, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br>    y = [np_utils.to_categorical(label, num_classes=label_size + <span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> y]<br><br>    <span class="hljs-keyword">return</span> x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary<br><br><br><span class="hljs-comment"># 定义深度学习模型：Bi-LSTM</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_Bi_LSTM</span>(<span class="hljs-params">vocab_size, label_size, input_shape, output_dim, n_units, out_act, activation</span>):<br>    model = Sequential()<br>    model.add(Embedding(input_dim=vocab_size + <span class="hljs-number">1</span>, output_dim=output_dim,<br>                        input_length=input_shape, mask_zero=<span class="hljs-literal">True</span>))<br>    model.add(Bidirectional(LSTM(units=n_units, activation=activation,<br>                                 return_sequences=<span class="hljs-literal">True</span>)))<br>    model.add(TimeDistributed(Dense(label_size + <span class="hljs-number">1</span>, activation=out_act)))<br>    model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">&#x27;adam&#x27;</span>, loss=<span class="hljs-string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="hljs-string">&#x27;accuracy&#x27;</span>])<br>    <span class="hljs-keyword">return</span> model<br><br><br><span class="hljs-comment"># 模型训练</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_train</span>():<br><br>    <span class="hljs-comment"># 将数据集分为训练集和测试集，占比为9:1</span><br>    input_shape = <span class="hljs-number">60</span><br>    x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary = input_data_for_model(input_shape)<br>    train_end = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(x)*<span class="hljs-number">0.9</span>)<br>    train_x, train_y = x[<span class="hljs-number">0</span>:train_end], np.array(y[<span class="hljs-number">0</span>:train_end])<br>    test_x, test_y = x[train_end:], np.array(y[train_end:])<br><br>    <span class="hljs-comment"># 模型输入参数</span><br>    activation = <span class="hljs-string">&#x27;selu&#x27;</span><br>    out_act = <span class="hljs-string">&#x27;softmax&#x27;</span><br>    n_units = <span class="hljs-number">100</span><br>    batch_size = <span class="hljs-number">32</span><br>    epochs = <span class="hljs-number">10</span><br>    output_dim = <span class="hljs-number">20</span><br><br>    <span class="hljs-comment"># 模型训练</span><br>    lstm_model = create_Bi_LSTM(vocab_size, label_size, input_shape, output_dim, n_units, out_act, activation)<br>    lstm_model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 模型保存</span><br>    model_save_path = CONSTANTS[<span class="hljs-number">0</span>]<br>    lstm_model.save(model_save_path)<br>    plot_model(lstm_model, to_file=<span class="hljs-string">&#x27;%s/LSTM_model.png&#x27;</span> % BASE_DIR)<br><br>    <span class="hljs-comment"># 在测试集上的效果</span><br>    N = test_x.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 测试的条数</span><br>    avg_accuracy = <span class="hljs-number">0</span>  <span class="hljs-comment"># 预测的平均准确率</span><br>    <span class="hljs-keyword">for</span> start, end <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, N, <span class="hljs-number">1</span>), <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, N+<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)):<br>        sentence = [inverse_word_dictionary[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> test_x[start] <span class="hljs-keyword">if</span> i != <span class="hljs-number">0</span>]<br>        y_predict = lstm_model.predict(test_x[start:end])<br>        input_sequences, output_sequences = [], []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(y_predict[<span class="hljs-number">0</span>])):<br>            output_sequences.append(np.argmax(y_predict[<span class="hljs-number">0</span>][i]))<br>            input_sequences.append(np.argmax(test_y[start][i]))<br><br>        <span class="hljs-built_in">eval</span> = lstm_model.evaluate(test_x[start:end], test_y[start:end])<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Test Accuracy: loss = %0.6f accuracy = %0.2f%%&#x27;</span> % (<span class="hljs-built_in">eval</span>[<span class="hljs-number">0</span>], <span class="hljs-built_in">eval</span>[<span class="hljs-number">1</span>] * <span class="hljs-number">100</span>))<br>        avg_accuracy += <span class="hljs-built_in">eval</span>[<span class="hljs-number">1</span>]<br>        output_sequences = <span class="hljs-string">&#x27; &#x27;</span>.join([output_dictionary[key] <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> output_sequences <span class="hljs-keyword">if</span> key != <span class="hljs-number">0</span>]).split()<br>        input_sequences = <span class="hljs-string">&#x27; &#x27;</span>.join([output_dictionary[key] <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> input_sequences <span class="hljs-keyword">if</span> key != <span class="hljs-number">0</span>]).split()<br>        output_input_comparison = pd.DataFrame([sentence, output_sequences, input_sequences]).T<br>        <span class="hljs-built_in">print</span>(output_input_comparison.dropna())<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;#&#x27;</span> * <span class="hljs-number">80</span>)<br><br>    avg_accuracy /= N<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;测试样本的平均预测准确率：%.2f%%.&quot;</span> % (avg_accuracy * <span class="hljs-number">100</span>))<br><br>model_train()<br></code></pre></td></tr></table></figure><p>在上面的代码中，先是通过input_data_for_model()函数来处理好进入模型的数据，其参数为input_shape，即填充句子时的长度。然后是创建Bi-LSTM模型create_Bi_LSTM()，模型的示意图如下：</p><figure><img src="/img/nlp5_4.png" alt="模型示意图" /><figcaption aria-hidden="true">模型示意图</figcaption></figure><p>最后，是在输入的数据上进行模型训练，将原始的数据分为训练集和测试集，占比为9:1，训练的周期为10次。</p><h3 id="模型训练">模型训练</h3><p>运行上述模型训练代码，一共训练10个周期，训练时间大概为500s，在训练集上的准确率达99%以上，在测试集上的平均准确率为95%以上。以下是最后几个测试集上的预测结果：</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs clean">......(前面的输出已忽略)<br>Test Accuracy: loss = <span class="hljs-number">0.000986</span> accuracy = <span class="hljs-number">100.00</span>%<br>          <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>   Cardiff  B-ORG  B-ORG<br><span class="hljs-number">1</span>         <span class="hljs-number">1</span>      O      O<br><span class="hljs-number">2</span>  Brighton  B-ORG  B-ORG<br><span class="hljs-number">3</span>         <span class="hljs-number">0</span>      O      O<br>################################################################################<br><br><span class="hljs-number">1</span>/<span class="hljs-number">1</span> [==============================] - <span class="hljs-number">0</span>s <span class="hljs-number">10</span>ms/step<br>Test Accuracy: loss = <span class="hljs-number">0.000274</span> accuracy = <span class="hljs-number">100.00</span>%<br>          <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>  Carlisle  B-ORG  B-ORG<br><span class="hljs-number">1</span>         <span class="hljs-number">0</span>      O      O<br><span class="hljs-number">2</span>      Hull  B-ORG  B-ORG<br><span class="hljs-number">3</span>         <span class="hljs-number">0</span>      O      O<br>################################################################################<br><br><span class="hljs-number">1</span>/<span class="hljs-number">1</span> [==============================] - <span class="hljs-number">0</span>s <span class="hljs-number">9</span>ms/step<br>Test Accuracy: loss = <span class="hljs-number">0.000479</span> accuracy = <span class="hljs-number">100.00</span>%<br>           <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>    Chester  B-ORG  B-ORG<br><span class="hljs-number">1</span>          <span class="hljs-number">1</span>      O      O<br><span class="hljs-number">2</span>  Cambridge  B-ORG  B-ORG<br><span class="hljs-number">3</span>          <span class="hljs-number">1</span>      O      O<br>################################################################################<br><br><span class="hljs-number">1</span>/<span class="hljs-number">1</span> [==============================] - <span class="hljs-number">0</span>s <span class="hljs-number">9</span>ms/step<br>Test Accuracy: loss = <span class="hljs-number">0.003092</span> accuracy = <span class="hljs-number">100.00</span>%<br>            <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>  Darlington  B-ORG  B-ORG<br><span class="hljs-number">1</span>           <span class="hljs-number">4</span>      O      O<br><span class="hljs-number">2</span>     Swansea  B-ORG  B-ORG<br><span class="hljs-number">3</span>           <span class="hljs-number">1</span>      O      O<br>################################################################################<br><br><span class="hljs-number">1</span>/<span class="hljs-number">1</span> [==============================] - <span class="hljs-number">0</span>s <span class="hljs-number">8</span>ms/step<br>Test Accuracy: loss = <span class="hljs-number">0.000705</span> accuracy = <span class="hljs-number">100.00</span>%<br>             <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">2</span><br><span class="hljs-number">0</span>       Exeter  B-ORG  B-ORG<br><span class="hljs-number">1</span>            <span class="hljs-number">2</span>      O      O<br><span class="hljs-number">2</span>  Scarborough  B-ORG  B-ORG<br><span class="hljs-number">3</span>            <span class="hljs-number">2</span>      O      O<br>################################################################################<br>测试样本的平均预测准确率：<span class="hljs-number">95.55</span>%.<br></code></pre></td></tr></table></figure><p>该模型在原始数据上的识别效果还是可以的。</p><p>训练完模型后，BASE_DIR中的所有文件如下：</p><figure><img src="/img/nlp5_5.png" alt="模型训练完后的所有文件截图" /><figcaption aria-hidden="true">模型训练完后的所有文件截图</figcaption></figure><h3 id="模型预测">模型预测</h3><p>最后，也许是整个项目最为激动人心的时刻，因为，我们要在新数据集上测试模型的识别效果。预测新数据的识别结果的完整Python代码（Bi_LSTM_Model_predict.py）如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-comment"># Name entity recognition for new data</span><br><br><span class="hljs-comment"># Import the necessary modules</span><br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> CONSTANTS<br><span class="hljs-keyword">from</span> keras.preprocessing.sequence <span class="hljs-keyword">import</span> pad_sequences<br><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> load_model<br><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize<br><br><span class="hljs-comment"># 导入字典</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">1</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    word_dictionary = pickle.load(f)<br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(CONSTANTS[<span class="hljs-number">4</span>], <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>    output_dictionary = pickle.load(f)<br><br><span class="hljs-keyword">try</span>:<br>    <span class="hljs-comment"># 数据预处理</span><br>    input_shape = <span class="hljs-number">60</span><br>    sent = <span class="hljs-string">&#x27;New York is the biggest city in America.&#x27;</span><br>    new_sent = word_tokenize(sent)<br>    new_x = [[word_dictionary[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> new_sent]]<br>    x = pad_sequences(maxlen=input_shape, sequences=new_x, padding=<span class="hljs-string">&#x27;post&#x27;</span>, value=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># 载入模型</span><br>    model_save_path = CONSTANTS[<span class="hljs-number">0</span>]<br>    lstm_model = load_model(model_save_path)<br><br>    <span class="hljs-comment"># 模型预测</span><br>    y_predict = lstm_model.predict(x)<br><br>    ner_tag = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(new_sent)):<br>        ner_tag.append(np.argmax(y_predict[<span class="hljs-number">0</span>][i]))<br><br>    ner = [output_dictionary[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ner_tag]<br>    <span class="hljs-built_in">print</span>(new_sent)<br>    <span class="hljs-built_in">print</span>(ner)<br><br>    <span class="hljs-comment"># 去掉NER标注为O的元素</span><br>    ner_reg_list = []<br>    <span class="hljs-keyword">for</span> word, tag <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(new_sent, ner):<br>        <span class="hljs-keyword">if</span> tag != <span class="hljs-string">&#x27;O&#x27;</span>:<br>            ner_reg_list.append((word, tag))<br><br>    <span class="hljs-comment"># 输出模型的NER识别结果</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;NER识别结果：&quot;</span>)<br>    <span class="hljs-keyword">if</span> ner_reg_list:<br>        <span class="hljs-keyword">for</span> i, item <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(ner_reg_list):<br>            <span class="hljs-keyword">if</span> item[<span class="hljs-number">1</span>].startswith(<span class="hljs-string">&#x27;B&#x27;</span>):<br>                end = i+<span class="hljs-number">1</span><br>                <span class="hljs-keyword">while</span> end &lt;= <span class="hljs-built_in">len</span>(ner_reg_list)-<span class="hljs-number">1</span> <span class="hljs-keyword">and</span> ner_reg_list[end][<span class="hljs-number">1</span>].startswith(<span class="hljs-string">&#x27;I&#x27;</span>):<br>                    end += <span class="hljs-number">1</span><br><br>                ner_type = item[<span class="hljs-number">1</span>].split(<span class="hljs-string">&#x27;-&#x27;</span>)[<span class="hljs-number">1</span>]<br>                ner_type_dict = &#123;<span class="hljs-string">&#x27;PER&#x27;</span>: <span class="hljs-string">&#x27;PERSON: &#x27;</span>,<br>                                <span class="hljs-string">&#x27;LOC&#x27;</span>: <span class="hljs-string">&#x27;LOCATION: &#x27;</span>,<br>                                <span class="hljs-string">&#x27;ORG&#x27;</span>: <span class="hljs-string">&#x27;ORGANIZATION: &#x27;</span>,<br>                                <span class="hljs-string">&#x27;MISC&#x27;</span>: <span class="hljs-string">&#x27;MISC: &#x27;</span><br>                                &#125;<br>                <span class="hljs-built_in">print</span>(ner_type_dict[ner_type],\<br>                    <span class="hljs-string">&#x27; &#x27;</span>.join([item[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> ner_reg_list[i:end]]))<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型并未识别任何有效命名实体。&quot;</span>)<br><br><span class="hljs-keyword">except</span> KeyError <span class="hljs-keyword">as</span> err:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;您输入的句子有单词不在词汇表中，请重新输入！&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;不在词汇表中的单词为：%s.&quot;</span> % err)<br></code></pre></td></tr></table></figure><p>输出结果为：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">[<span class="hljs-string">&#x27;New&#x27;</span>, <span class="hljs-string">&#x27;York&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;biggest&#x27;</span>, <span class="hljs-string">&#x27;city&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;America&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]<br>[<span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]<br>NER识别结果：<br><span class="hljs-keyword">LOCATION</span>:  <span class="hljs-built_in">New</span> York<br><span class="hljs-keyword">LOCATION</span>:  America<br></code></pre></td></tr></table></figure><p>接下来，再测试三个笔者自己想的句子：</p><p>输入为： <figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">sent = <span class="hljs-symbol">&#x27;James</span> <span class="hljs-keyword">is</span> a world famous actor, whose home <span class="hljs-keyword">is</span> <span class="hljs-keyword">in</span> London.&#x27;<br></code></pre></td></tr></table></figure> 输出结果为：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;James&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;world&#x27;</span>, <span class="hljs-string">&#x27;famous&#x27;</span>, <span class="hljs-string">&#x27;actor&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;whose&#x27;</span>, <span class="hljs-string">&#x27;home&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;London&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>PERSON:  James<br>LOCATION:  London<br></code></pre></td></tr></table></figure><p>输入为： <figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">sent = <span class="hljs-symbol">&#x27;Oxford</span> <span class="hljs-keyword">is</span> <span class="hljs-keyword">in</span> England, Jack <span class="hljs-keyword">is</span> from here.&#x27;<br></code></pre></td></tr></table></figure> 输出为： <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;Oxford&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>, <span class="hljs-string">&#x27;England&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;Jack&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;from&#x27;</span>, <span class="hljs-string">&#x27;here&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-PER&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>PERSON:  Oxford<br>LOCATION:  England<br>PERSON:  Jack<br></code></pre></td></tr></table></figure></p><p>输入为： <figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ini"><span class="hljs-attr">sent</span> = <span class="hljs-string">&#x27;I love Shanghai.&#x27;</span><br></code></pre></td></tr></table></figure> 输出为： <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;Shanghai&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>LOCATION:  Shanghai<br></code></pre></td></tr></table></figure></p><p>在上面的例子中，只有Oxford的识别效果不理想，模型将它识别为PERSON，其实应该是ORGANIZATION。</p><p>接下来是三个来自CNN和wikipedia的句子：</p><p>输入为： <figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">sent</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;the US runs the risk of a military defeat by China or Russia&quot;</span><br></code></pre></td></tr></table></figure> 输出为： <figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">[<span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;US&#x27;</span>, <span class="hljs-string">&#x27;runs&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;risk&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;military&#x27;</span>, <span class="hljs-string">&#x27;defeat&#x27;</span>, <span class="hljs-string">&#x27;by&#x27;</span>, <span class="hljs-string">&#x27;China&#x27;</span>, <span class="hljs-string">&#x27;or&#x27;</span>, <span class="hljs-string">&#x27;Russia&#x27;</span>]<br>[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>]<br>NER识别结果：<br><span class="hljs-keyword">LOCATION</span>:  US<br><span class="hljs-keyword">LOCATION</span>:  China<br><span class="hljs-keyword">LOCATION</span>:  Russia<br></code></pre></td></tr></table></figure> 输入为：<figure class="highlight smalltalk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs smalltalk">sent = <span class="hljs-comment">&quot;Home to the headquarters of the United Nations, New York is an important center for international diplomacy.&quot;</span><br></code></pre></td></tr></table></figure> 输出为： <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;Home&#x27;</span>, <span class="hljs-string">&#x27;to&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;headquarters&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;United&#x27;</span>, <span class="hljs-string">&#x27;Nations&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;New&#x27;</span>, <span class="hljs-string">&#x27;York&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;an&#x27;</span>, <span class="hljs-string">&#x27;important&#x27;</span>, <span class="hljs-string">&#x27;center&#x27;</span>, <span class="hljs-string">&#x27;for&#x27;</span>, <span class="hljs-string">&#x27;international&#x27;</span>, <span class="hljs-string">&#x27;diplomacy&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>ORGANIZATION:  United Nations<br>LOCATION:  New York<br></code></pre></td></tr></table></figure></p><p>输入为： <figure class="highlight abnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs abnf"><span class="hljs-attribute">sent</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;The United States is a founding member of the United Nations, World Bank, International Monetary Fund.&quot;</span><br></code></pre></td></tr></table></figure> 输出为: <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;The&#x27;</span>, <span class="hljs-string">&#x27;United&#x27;</span>, <span class="hljs-string">&#x27;States&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-string">&#x27;founding&#x27;</span>, <span class="hljs-string">&#x27;member&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;United&#x27;</span>, <span class="hljs-string">&#x27;Nations&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;World&#x27;</span>, <span class="hljs-string">&#x27;Bank&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;International&#x27;</span>, <span class="hljs-string">&#x27;Monetary&#x27;</span>, <span class="hljs-string">&#x27;Fund&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]</span><br><span class="hljs-selector-attr">[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-LOC&#x27;</span>, <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;B-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>]</span><br>NER识别结果：<br>LOCATION:  United States<br>ORGANIZATION:  United Nations<br>ORGANIZATION:  World Bank<br>ORGANIZATION:  International Monetary Fund<br></code></pre></td></tr></table></figure></p><p>这三个例子识别全部正确。</p><h3 id="总结">总结</h3><p>到这儿，笔者的这个项目就差不多了。我们有必要对这个项目做个总结。</p><p>首先是这个项目的优点。它的优点在于能够让你一步步地实现NER，而且除了语料库，你基本熟悉了如何创建一个识别NER系统的步骤，同时，对深度学习模型及其应用也有了深刻理解。因此，好处是显而易见的。当然，在实际工作中，语料库的整理才是最耗费时间的，能够占到90%或者更多的时间，因此，有一个好的语料库你才能展开工作。</p><p>接着讲讲这个项目的缺点。第一个，是语料库不够大，当然，约14000条句子也够了，但本项目没有对句子进行文本预处理，所以，有些单词的变形可能无法进入词汇表。第二个，缺少对新词的处理，一旦句子中出现一个新的单词，这个模型便无法处理，这是后期需要完善的地方。第三个，句子的填充长度为60，如果输入的句子长度大于60，则后面的部分将无法有效识别。</p><p>因此，后续还有更多的工作需要去做，当然，做一个中文NER也是可以考虑的。</p><p>本项目已上传Github,地址为 <ahref="https://github.com/percent4/DL_4_NER">https://github.com/percent4/DL_4_NER</a>。：欢迎大家参考~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p><h3 id="参考文献">参考文献</h3><ol type="1"><li>BOOK： Applied Natural Language Processing with Python， TawehBeysolow II</li><li>WEBSITE：https://github.com/Apress/applied-natural-language-processing-w-python</li><li>WEBSITE: NLP入门（四）命名实体识别（NER）:https://www.jianshu.com/p/16e1f6a7aaef</li></ol>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>NER</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（四）命名实体识别（NER）</title>
    <link href="/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/"/>
    <url>/2023/07/08/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E5%9B%9B%EF%BC%89%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%EF%BC%88NER%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>本文将会简单介绍自然语言处理（NLP）中的命名实体识别（NER）。</p><p>命名实体识别（Named EntityRecognition，简称NER）是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。</p><p>举个简单的例子，在句子“小明早上8点去学校上课。”中，对其进行命名实体识别，应该能提取信息</p><blockquote><p>人名：小明，时间：早上8点，地点：学校。</p></blockquote><p>本文将会介绍几个工具用来进行命名实体识别，后续有机会的话，我们将会尝试着用HMM、CRF或深度学习来实现命名实体识别。</p><p>首先我们来看一下NLTK和StanfordNLP中对命名实体识别的分类，如下图：</p><figure><img src="/img/nlp4_1.png"alt="NLTK和Stanford NLP中对命名实体识别的分类" /><figcaption aria-hidden="true">NLTK和StanfordNLP中对命名实体识别的分类</figcaption></figure><p>在上图中，LOCATION和GPE有重合。GPE通常表示地理—政治条目，比如城市，州，国家，洲等。LOCATION除了上述内容外，还能表示名山大川等。FACILITY通常表示知名的纪念碑或人工制品等。</p><p>下面介绍两个工具来进行NER的任务：NLTK和Stanford NLP。</p><p>首先是NLTK，我们的示例文档（介绍FIFA，来源于维基百科）如下：</p><blockquote><p>FIFA was founded in 1904 to oversee international competition amongthe national associations of Belgium, Denmark, France, Germany, theNetherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich,its membership now comprises 211 national associations. Member countriesmust each also be members of one of the six regional confederations intowhich the world is divided: Africa, Asia, Europe, North &amp; CentralAmerica and the Caribbean, Oceania, and South America.</p></blockquote><p>实现NER的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> nltk<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_document</span>(<span class="hljs-params">document</span>):<br>   document = re.sub(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, document)<br>   <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(document, <span class="hljs-built_in">str</span>):<br>       document = document<br>   <span class="hljs-keyword">else</span>:<br>       <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Document is not string!&#x27;</span>)<br>   document = document.strip()<br>   sentences = nltk.sent_tokenize(document)<br>   sentences = [sentence.strip() <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences]<br>   <span class="hljs-keyword">return</span> sentences<br><br><span class="hljs-comment"># sample document</span><br>text = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">FIFA was founded in 1904 to oversee international competition among the national associations of Belgium, </span><br><span class="hljs-string">Denmark, France, Germany, the Netherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich, its </span><br><span class="hljs-string">membership now comprises 211 national associations. Member countries must each also be members of one of </span><br><span class="hljs-string">the six regional confederations into which the world is divided: Africa, Asia, Europe, North &amp; Central America </span><br><span class="hljs-string">and the Caribbean, Oceania, and South America.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># tokenize sentences</span><br>sentences = parse_document(text)<br>tokenized_sentences = [nltk.word_tokenize(sentence) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences]<br><span class="hljs-comment"># tag sentences and use nltk&#x27;s Named Entity Chunker</span><br>tagged_sentences = [nltk.pos_tag(sentence) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> tokenized_sentences]<br>ne_chunked_sents = [nltk.ne_chunk(tagged) <span class="hljs-keyword">for</span> tagged <span class="hljs-keyword">in</span> tagged_sentences]<br><span class="hljs-comment"># extract all named entities</span><br>named_entities = []<br><span class="hljs-keyword">for</span> ne_tagged_sentence <span class="hljs-keyword">in</span> ne_chunked_sents:<br>   <span class="hljs-keyword">for</span> tagged_tree <span class="hljs-keyword">in</span> ne_tagged_sentence:<br>       <span class="hljs-comment"># extract only chunks having NE labels</span><br>       <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(tagged_tree, <span class="hljs-string">&#x27;label&#x27;</span>):<br>           entity_name = <span class="hljs-string">&#x27; &#x27;</span>.join(c[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> tagged_tree.leaves()) <span class="hljs-comment">#get NE name</span><br>           entity_type = tagged_tree.label() <span class="hljs-comment"># get NE category</span><br>           named_entities.append((entity_name, entity_type))<br>           <span class="hljs-comment"># get unique named entities</span><br>           named_entities = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(named_entities))<br><br><span class="hljs-comment"># store named entities in a data frame</span><br>entity_frame = pd.DataFrame(named_entities, columns=[<span class="hljs-string">&#x27;Entity Name&#x27;</span>, <span class="hljs-string">&#x27;Entity Type&#x27;</span>])<br><span class="hljs-comment"># display results</span><br><span class="hljs-built_in">print</span>(entity_frame)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">        Entity Name   Entity Type<br><span class="hljs-number">0</span>              FIFA  <span class="hljs-keyword">ORGANIZATION</span><br><span class="hljs-keyword"></span><span class="hljs-number">1</span>   Central America  <span class="hljs-keyword">ORGANIZATION</span><br><span class="hljs-keyword"></span><span class="hljs-number">2</span>           <span class="hljs-keyword">Belgium </span>          GPE<br><span class="hljs-number">3</span>         Caribbean      LOCATION<br><span class="hljs-number">4</span>              Asia           GPE<br><span class="hljs-number">5</span>            France           GPE<br><span class="hljs-number">6</span>           Oceania           GPE<br><span class="hljs-number">7</span>           Germany           GPE<br><span class="hljs-number">8</span>     South America           GPE<br><span class="hljs-number">9</span>           Denmark           GPE<br><span class="hljs-number">10</span>           Zürich           GPE<br><span class="hljs-number">11</span>           Africa        PERSON<br><span class="hljs-number">12</span>           <span class="hljs-keyword">Sweden </span>          GPE<br><span class="hljs-number">13</span>      Netherlands           GPE<br><span class="hljs-number">14</span>            Spain           GPE<br><span class="hljs-number">15</span>      <span class="hljs-keyword">Switzerland </span>          GPE<br><span class="hljs-number">16</span>            <span class="hljs-keyword">North </span>          GPE<br><span class="hljs-number">17</span>           Europe           GPE<br></code></pre></td></tr></table></figure><p>可以看到，NLTK中的NER任务大体上完成得还是不错的，能够识别FIFA为组织（ORGANIZATION），Belgium,Asia为GPE,但是也有一些不太如人意的地方，比如，它将CentralAmerica识别为ORGANIZATION，而实际上它应该为GPE；将Africa识别为PERSON，实际上应该为GPE。</p><p>接下来，我们尝试着用StanfordNLP工具。关于该工具，我们主要使用Stanford NER标注工具。在使用这个工具之前，你需要在自己的电脑上安装Java（一般是JDK），并将Java添加到系统路径中，同时下载英语NER的文件包：stanford-ner-2018-10-16.zip（大小为172MB），下载地址为：https://nlp.stanford.edu/software/CRF-NER.shtml。以笔者的电脑为例，Java所在的路径为：C:Files.0_161.exe， 下载StanfordNER的zip文件解压后的文件夹的路径为：E://stanford-ner-2018-10-16，如下图所示：</p><p><img src="/img/nlp4_2.png" /></p><p>在classifer文件夹中有如下文件：</p><p><img src="/img/nlp4_3.png" /></p><p>它们代表的含义如下：</p><blockquote><p>3 class: Location, Person, Organization 4 class: Location, Person,Organization, Misc 7 class: Location, Person, Organization, Money,Percent, Date, Time</p></blockquote><p>可以使用Python实现Stanford NER，完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><span class="hljs-keyword">from</span> nltk.tag <span class="hljs-keyword">import</span> StanfordNERTagger<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> nltk<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">parse_document</span>(<span class="hljs-params">document</span>):<br>   document = re.sub(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27; &#x27;</span>, document)<br>   <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(document, <span class="hljs-built_in">str</span>):<br>       document = document<br>   <span class="hljs-keyword">else</span>:<br>       <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&#x27;Document is not string!&#x27;</span>)<br>   document = document.strip()<br>   sentences = nltk.sent_tokenize(document)<br>   sentences = [sentence.strip() <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences]<br>   <span class="hljs-keyword">return</span> sentences<br><br><span class="hljs-comment"># sample document</span><br>text = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">FIFA was founded in 1904 to oversee international competition among the national associations of Belgium, </span><br><span class="hljs-string">Denmark, France, Germany, the Netherlands, Spain, Sweden, and Switzerland. Headquartered in Zürich, its </span><br><span class="hljs-string">membership now comprises 211 national associations. Member countries must each also be members of one of </span><br><span class="hljs-string">the six regional confederations into which the world is divided: Africa, Asia, Europe, North &amp; Central America </span><br><span class="hljs-string">and the Caribbean, Oceania, and South America.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>sentences = parse_document(text)<br>tokenized_sentences = [nltk.word_tokenize(sentence) <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences]<br><br><span class="hljs-comment"># set java path in environment variables</span><br>java_path = <span class="hljs-string">r&#x27;C:\Program Files\Java\jdk1.8.0_161\bin\java.exe&#x27;</span><br>os.environ[<span class="hljs-string">&#x27;JAVAHOME&#x27;</span>] = java_path<br><span class="hljs-comment"># load stanford NER</span><br>sn = StanfordNERTagger(<span class="hljs-string">&#x27;E://stanford-ner-2018-10-16/classifiers/english.muc.7class.distsim.crf.ser.gz&#x27;</span>,<br>                       path_to_jar=<span class="hljs-string">&#x27;E://stanford-ner-2018-10-16/stanford-ner.jar&#x27;</span>)<br><br><span class="hljs-comment"># tag sentences</span><br>ne_annotated_sentences = [sn.tag(sent) <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> tokenized_sentences]<br><span class="hljs-comment"># extract named entities</span><br>named_entities = []<br><span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> ne_annotated_sentences:<br>   temp_entity_name = <span class="hljs-string">&#x27;&#x27;</span><br>   temp_named_entity = <span class="hljs-literal">None</span><br>   <span class="hljs-keyword">for</span> term, tag <span class="hljs-keyword">in</span> sentence:<br>       <span class="hljs-comment"># get terms with NE tags</span><br>       <span class="hljs-keyword">if</span> tag != <span class="hljs-string">&#x27;O&#x27;</span>:<br>           temp_entity_name = <span class="hljs-string">&#x27; &#x27;</span>.join([temp_entity_name, term]).strip() <span class="hljs-comment">#get NE name</span><br>           temp_named_entity = (temp_entity_name, tag) <span class="hljs-comment"># get NE and its category</span><br>       <span class="hljs-keyword">else</span>:<br>           <span class="hljs-keyword">if</span> temp_named_entity:<br>               named_entities.append(temp_named_entity)<br>               temp_entity_name = <span class="hljs-string">&#x27;&#x27;</span><br>               temp_named_entity = <span class="hljs-literal">None</span><br><br><span class="hljs-comment"># get unique named entities</span><br>named_entities = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(named_entities))<br><span class="hljs-comment"># store named entities in a data frame</span><br>entity_frame = pd.DataFrame(named_entities, columns=[<span class="hljs-string">&#x27;Entity Name&#x27;</span>, <span class="hljs-string">&#x27;Entity Type&#x27;</span>])<br><span class="hljs-comment"># display results</span><br><span class="hljs-built_in">print</span>(entity_frame)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs crmsh">                Entity Name   Entity <span class="hljs-keyword">Type</span><br><span class="hljs-number">0</span>                      <span class="hljs-number">1904</span>          <span class="hljs-keyword">DATE</span><br><span class="hljs-number">1</span>                   Denmark      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">2</span>                     Spain      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">3</span>   North &amp; Central America  ORGANIZATION<br><span class="hljs-number">4</span>             South America      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">5</span>                   Belgium      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">6</span>                    Zürich      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">7</span>           the Netherlands      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">8</span>                    France      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">9</span>                 Caribbean      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">10</span>                   Sweden      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">11</span>                  Oceania      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">12</span>                     Asia      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">13</span>                     FIFA  ORGANIZATION<br><span class="hljs-number">14</span>                   Europe      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">15</span>                   Africa      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">16</span>              Switzerland      <span class="hljs-keyword">LOCATION</span><br><span class="hljs-title">17</span>                  Germany      LOCATION<br></code></pre></td></tr></table></figure><p>可以看到，在StanfordNER的帮助下，NER的实现效果较好，将Africa识别为LOCATION，将1904识别为时间（这在NLTK中没有识别出来），但还是对North&amp; Central America识别有误，将其识别为ORGANIZATION。</p><p>值得注意的是，并不是说Stanford NER一定会比NLTKNER的效果好，两者针对的对象，预料，算法可能有差异，因此，需要根据自己的需求决定使用什么工具。</p><p>本次分享到此结束，以后有机会的话，将会尝试着用HMM、CRF或深度学习来实现命名实体识别。</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>NER</tag>
      
      <tag>NLP工具</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（三）词形还原（Lemmatization）</title>
    <link href="/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F%EF%BC%88Lemmatization%EF%BC%89/"/>
    <url>/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%89%EF%BC%89%E8%AF%8D%E5%BD%A2%E8%BF%98%E5%8E%9F%EF%BC%88Lemmatization%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>词形还原（Lemmatization）是文本预处理中的重要部分，与词干提取（stemming）很相似。</p><p>简单说来，词形还原就是去掉单词的词缀，提取单词的主干部分，通常提取后的单词会是字典中的单词，不同于词干提取（stemming），提取后的单词不一定会出现在单词中。比如，单词“cars”词形还原后的单词为“car”，单词“ate”词形还原后的单词为“eat”。</p><p>在Python的nltk模块中，使用WordNet为我们提供了稳健的词形还原的函数。如以下示例Python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.stem <span class="hljs-keyword">import</span> WordNetLemmatizer<br><br>wnl = WordNetLemmatizer()<br><span class="hljs-comment"># lemmatize nouns</span><br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;cars&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>))<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;men&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>))<br><br><span class="hljs-comment"># lemmatize verbs</span><br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;running&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>))<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;ate&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>))<br><br><span class="hljs-comment"># lemmatize adjectives</span><br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;saddest&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>))<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;fancier&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>car men run eat sad fancy</p></blockquote><p>在以上代码中，wnl.lemmatize()函数可以进行词形还原，第一个参数为单词，第二个参数为该单词的词性，如名词，动词，形容词等，返回的结果为输入单词的词形还原后的结果。</p><p>词形还原一般是简单的，但具体我们在使用时，指定单词的词性很重要，不然词形还原可能效果不好，如以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.stem <span class="hljs-keyword">import</span> WordNetLemmatizer<br><br>wnl = WordNetLemmatizer()<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;ate&#x27;</span>, <span class="hljs-string">&#x27;n&#x27;</span>))<br><span class="hljs-built_in">print</span>(wnl.lemmatize(<span class="hljs-string">&#x27;fancier&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>ate fancier</p></blockquote><p>那么，如何获取单词的词性呢？在NLP中，使用Parts ofspeech（POS）技术实现。在nltk中，可以使用nltk.pos_tag()获取单词在句子中的词性，如以下Python代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">sentence = <span class="hljs-string">&#x27;The brown fox is quick and he is jumping over the lazy dog&#x27;</span><br><span class="hljs-keyword">import</span> nltk<br>tokens = nltk.word_tokenize(sentence)<br>tagged_sent = nltk.pos_tag(tokens)<br><span class="hljs-built_in">print</span>(tagged_sent)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('is', 'VBZ'),('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('is', 'VBZ'),('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'),('dog', 'NN')]</p></blockquote><p>关于上述词性的说明，可以参考下表：</p><figure><img src="/img/nlp3_1.webp" alt="词性说明表1" /><figcaption aria-hidden="true">词性说明表1</figcaption></figure><figure><img src="/img/nlp3_2.webp" alt="词性说明表2" /><figcaption aria-hidden="true">词性说明表2</figcaption></figure><p>OK，知道了获取单词在句子中的词性，再结合词形还原，就能很好地完成词形还原功能。示例的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize, pos_tag<br><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> wordnet<br><span class="hljs-keyword">from</span> nltk.stem <span class="hljs-keyword">import</span> WordNetLemmatizer<br><br><span class="hljs-comment"># 获取单词的词性</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_wordnet_pos</span>(<span class="hljs-params">tag</span>):<br>    <span class="hljs-keyword">if</span> tag.startswith(<span class="hljs-string">&#x27;J&#x27;</span>):<br>        <span class="hljs-keyword">return</span> wordnet.ADJ<br>    <span class="hljs-keyword">elif</span> tag.startswith(<span class="hljs-string">&#x27;V&#x27;</span>):<br>        <span class="hljs-keyword">return</span> wordnet.VERB<br>    <span class="hljs-keyword">elif</span> tag.startswith(<span class="hljs-string">&#x27;N&#x27;</span>):<br>        <span class="hljs-keyword">return</span> wordnet.NOUN<br>    <span class="hljs-keyword">elif</span> tag.startswith(<span class="hljs-string">&#x27;R&#x27;</span>):<br>        <span class="hljs-keyword">return</span> wordnet.ADV<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>sentence = <span class="hljs-string">&#x27;football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal.&#x27;</span><br>tokens = word_tokenize(sentence)  <span class="hljs-comment"># 分词</span><br>tagged_sent = pos_tag(tokens)     <span class="hljs-comment"># 获取单词词性</span><br><br>wnl = WordNetLemmatizer()<br>lemmas_sent = []<br><span class="hljs-keyword">for</span> tag <span class="hljs-keyword">in</span> tagged_sent:<br>    wordnet_pos = get_wordnet_pos(tag[<span class="hljs-number">1</span>]) <span class="hljs-keyword">or</span> wordnet.NOUN<br>    lemmas_sent.append(wnl.lemmatize(tag[<span class="hljs-number">0</span>], pos=wordnet_pos)) <span class="hljs-comment"># 词形还原</span><br><br><span class="hljs-built_in">print</span>(lemmas_sent)<br><br></code></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>['football', 'be', 'a', 'family', 'of', 'team', 'sport', 'that','involve', ',', 'to', 'vary', 'degree', ',', 'kick', 'a', 'ball', 'to','score', 'a', 'goal', '.']</p></blockquote><p>输出的结果就是对句子中的单词进行词形还原后的结果。本次分享到此结束，欢迎大家交流~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>词形还原</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（二）探究TF-IDF的原理</title>
    <link href="/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%8C%EF%BC%89%E6%8E%A2%E7%A9%B6TF-IDF%E7%9A%84%E5%8E%9F%E7%90%86/"/>
    <url>/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%BA%8C%EF%BC%89%E6%8E%A2%E7%A9%B6TF-IDF%E7%9A%84%E5%8E%9F%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h3 id="tf-idf介绍">TF-IDF介绍</h3><p>TF-IDF是NLP中一种常用的统计方法，用以评估一个字词对于一个文件集或一个语料库中的其中一份文件的重要程度，通常用于提取文本的特征，即关键词。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。</p><p>在NLP中，TF-IDF的计算公式如下：</p><p><span class="math display">\[tfidf = tf*idf.\]</span></p><p>其中，tf是词频(Term Frequency)，idf为逆向文件频率(Inverse DocumentFrequency)。</p><p>tf为词频，即一个词语在文档中的出现频率，假设一个词语在整个文档中出现了i次，而整个文档有N个词语，则tf的值为i/N.</p><p>idf为逆向文件频率，假设整个文档有n篇文章，而一个词语在k篇文章中出现，则idf值为</p><p><span class="math display">\[idf=\log_{2}(\frac{n}{k}).\]</span></p><p>当然，不同地方的idf值计算公式会有稍微的不同。比如有些地方会在分母的k上加1，防止分母为0，还有些地方会让分子，分母都加上1，这是smoothing技巧。在本文中，还是采用最原始的idf值计算公式，因为这与gensim里面的计算公式一致。</p><p>假设整个文档有D篇文章，则单词i在第j篇文章中的tfidf值为</p><figure><img src="/img/nlp2_1.webp" alt="gensim中tfidf的计算公式" /><figcaption aria-hidden="true">gensim中tfidf的计算公式</figcaption></figure><p>以上就是TF-IDF的计算方法。</p><h3 id="文本介绍及预处理">文本介绍及预处理</h3><p>我们将采用以下三个示例文本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">text1 =<span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal. </span><br><span class="hljs-string">Unqualified, the word football is understood to refer to whichever form of football is the most popular </span><br><span class="hljs-string">in the regional context in which the word appears. Sports commonly called football in certain places </span><br><span class="hljs-string">include association football (known as soccer in some countries); gridiron football (specifically American </span><br><span class="hljs-string">football or Canadian football); Australian rules football; rugby football (either rugby league or rugby union); </span><br><span class="hljs-string">and Gaelic football. These different variations of football are known as football codes.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>text2 = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Basketball is a team sport in which two teams of five players, opposing one another on a rectangular court, </span><br><span class="hljs-string">compete with the primary objective of shooting a basketball (approximately 9.4 inches (24 cm) in diameter) </span><br><span class="hljs-string">through the defender&#x27;s hoop (a basket 18 inches (46 cm) in diameter mounted 10 feet (3.048 m) high to a backboard </span><br><span class="hljs-string">at each end of the court) while preventing the opposing team from shooting through their own hoop. A field goal is </span><br><span class="hljs-string">worth two points, unless made from behind the three-point line, when it is worth three. After a foul, timed play stops </span><br><span class="hljs-string">and the player fouled or designated to shoot a technical foul is given one or more one-point free throws. The team with </span><br><span class="hljs-string">the most points at the end of the game wins, but if regulation play expires with the score tied, an additional period </span><br><span class="hljs-string">of play (overtime) is mandated.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>text3 = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Volleyball, game played by two teams, usually of six players on a side, in which the players use their hands to bat a </span><br><span class="hljs-string">ball back and forth over a high net, trying to make the ball touch the court within the opponents’ playing area before </span><br><span class="hljs-string">it can be returned. To prevent this a player on the opposing team bats the ball up and toward a teammate before it touches </span><br><span class="hljs-string">the court surface—that teammate may then volley it back across the net or bat it to a third teammate who volleys it across </span><br><span class="hljs-string">the net. A team is allowed only three touches of the ball before it must be returned over the net.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure><p>这三篇文章分别是关于足球，篮球，排球的介绍，它们组成一篇文档。</p><p>接下来是文本的预处理部分。</p><p>首先是对文本去掉换行符，然后是分句，分词，再去掉其中的标点，完整的Python代码如下，输入的参数为文章text:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> nltk<br><span class="hljs-keyword">import</span> string<br><br><span class="hljs-comment"># 文本预处理</span><br><span class="hljs-comment"># 函数：text文件分句，分词，并去掉标点</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_tokens</span>(<span class="hljs-params">text</span>):<br>    text = text.replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>    sents = nltk.sent_tokenize(text)  <span class="hljs-comment"># 分句</span><br>    tokens = []<br>    <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> nltk.word_tokenize(sent):  <span class="hljs-comment"># 分词</span><br>            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> string.punctuation: <span class="hljs-comment"># 去掉标点</span><br>                tokens.append(word)<br>    <span class="hljs-keyword">return</span> tokens<br></code></pre></td></tr></table></figure><p>接着，去掉文章中的通用词（stopwords），然后统计每个单词的出现次数，完整的Python代码如下，输入的参数为文章text:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords     <span class="hljs-comment">#停用词</span><br><br><span class="hljs-comment"># 对原始的text文件去掉停用词</span><br><span class="hljs-comment"># 生成count字典，即每个单词的出现次数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_count</span>(<span class="hljs-params">text</span>):<br>    tokens = get_tokens(text)<br>    filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> w <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)]    <span class="hljs-comment">#去掉停用词</span><br>    count = Counter(filtered)<br>    <span class="hljs-keyword">return</span> count<br></code></pre></td></tr></table></figure><p>以text3为例，生成的count字典如下：</p><blockquote><p>Counter({'ball': 4, 'net': 4, 'teammate': 3, 'returned': 2, 'bat': 2,'court': 2, 'team': 2, 'across': 2, 'touches': 2, 'back': 2, 'players':2, 'touch': 1, 'must': 1, 'usually': 1, 'side': 1, 'player': 1, 'area':1, 'Volleyball': 1, 'hands': 1, 'may': 1, 'toward': 1, 'A': 1, 'third':1, 'two': 1, 'six': 1, 'opposing': 1, 'within': 1, 'prevent': 1,'allowed': 1, '’': 1, 'playing': 1, 'played': 1, 'volley': 1,'surface—that': 1, 'volleys': 1, 'opponents': 1, 'use': 1, 'high': 1,'teams': 1, 'bats': 1, 'To': 1, 'game': 1, 'make': 1, 'forth': 1,'three': 1, 'trying': 1})</p></blockquote><h3 id="gensim中的tf-idf">Gensim中的TF-IDF</h3><p>对文本进行预处理后，对于以上三个示例文本，我们都会得到一个count字典，里面是每个文本中单词的出现次数。下面，我们将用gensim中的已实现的TF-IDF模型，来输出每篇文章中TF-IDF排名前三的单词及它们的tfidf值，完整的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords     <span class="hljs-comment">#停用词</span><br><span class="hljs-keyword">from</span> gensim <span class="hljs-keyword">import</span> corpora, models, matutils<br><br><span class="hljs-comment">#training by gensim&#x27;s Ifidf Model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_words</span>(<span class="hljs-params">text</span>):<br>    tokens = get_tokens(text)<br>    filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> w <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)]<br>    <span class="hljs-keyword">return</span> filtered<br><br><span class="hljs-comment"># get text</span><br>count1, count2, count3 = get_words(text1), get_words(text2), get_words(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-comment"># training by TfidfModel in gensim</span><br>dictionary = corpora.Dictionary(countlist)<br>new_dict = &#123;v:k <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> dictionary.token2id.items()&#125;<br>corpus2 = [dictionary.doc2bow(count) <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> countlist]<br>tfidf2 = models.TfidfModel(corpus2)<br>corpus_tfidf = tfidf2[corpus2]<br><br><span class="hljs-comment"># output</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nTraining by gensim Tfidf Model.......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, doc <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(corpus_tfidf):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    sorted_words = <span class="hljs-built_in">sorted</span>(doc, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    <span class="hljs-keyword">for</span> num, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(new_dict[num], <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Training</span> by gensim Tfidf Model.......<br><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">1</span><br>    <span class="hljs-attribute">Word</span>: football, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">84766</span><br>    <span class="hljs-attribute">Word</span>: rugby, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">21192</span><br>    <span class="hljs-attribute">Word</span>: known, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">14128</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">2</span><br>    <span class="hljs-attribute">Word</span>: play, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">29872</span><br>    <span class="hljs-attribute">Word</span>: cm, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">19915</span><br>    <span class="hljs-attribute">Word</span>: diameter, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">19915</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">3</span><br>    <span class="hljs-attribute">Word</span>: net, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">45775</span><br>    <span class="hljs-attribute">Word</span>: teammate, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">34331</span><br>    <span class="hljs-attribute">Word</span>: across, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">22888</span><br></code></pre></td></tr></table></figure><p>输出的结果还是比较符合我们的预期的，比如关于足球的文章中提取了football,rugby关键词，关于篮球的文章中提取了plat,cm关键词，关于排球的文章中提取了net, teammate关键词。</p><h3 id="自己动手实践tf-idf模型">自己动手实践TF-IDF模型</h3><p>有了以上我们对TF-IDF模型的理解，其实我们自己也可以动手实践一把，这是学习算法的最佳方式！</p><p>以下是笔者实践TF-IDF的代码（接文本预处理代码）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><br><span class="hljs-comment"># 计算tf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tf</span>(<span class="hljs-params">word, count</span>):<br>    <span class="hljs-keyword">return</span> count[word] / <span class="hljs-built_in">sum</span>(count.values())<br><span class="hljs-comment"># 计算count_list有多少个文件包含word</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">n_containing</span>(<span class="hljs-params">word, count_list</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> count_list <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> count)<br><br><span class="hljs-comment"># 计算idf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">idf</span>(<span class="hljs-params">word, count_list</span>):<br>    <span class="hljs-keyword">return</span> math.log2(<span class="hljs-built_in">len</span>(count_list) / (n_containing(word, count_list)))    <span class="hljs-comment">#对数以2为底</span><br><span class="hljs-comment"># 计算tf-idf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tfidf</span>(<span class="hljs-params">word, count, count_list</span>):<br>    <span class="hljs-keyword">return</span> tf(word, count) * idf(word, count_list)<br><br><span class="hljs-comment"># TF-IDF测试</span><br>count1, count2, count3 = make_count(text1), make_count(text2), make_count(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training by original algorithm......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, count <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(countlist):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    scores = &#123;word: tfidf(word, count, countlist) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> count&#125;<br>    sorted_words = <span class="hljs-built_in">sorted</span>(scores.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    <span class="hljs-comment"># sorted_words = matutils.unitvec(sorted_words)</span><br>    <span class="hljs-keyword">for</span> word, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(word, <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Training</span> by original algorithm......<br><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">1</span><br>    <span class="hljs-attribute">Word</span>: football, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">30677</span><br>    <span class="hljs-attribute">Word</span>: rugby, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">07669</span><br>    <span class="hljs-attribute">Word</span>: known, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">05113</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">2</span><br>    <span class="hljs-attribute">Word</span>: play, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">05283</span><br>    <span class="hljs-attribute">Word</span>: inches, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">03522</span><br>    <span class="hljs-attribute">Word</span>: worth, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">03522</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">3</span><br>    <span class="hljs-attribute">Word</span>: net, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">10226</span><br>    <span class="hljs-attribute">Word</span>: teammate, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">07669</span><br>    <span class="hljs-attribute">Word</span>: across, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">05113</span><br></code></pre></td></tr></table></figure><p>可以看到，笔者自己动手实践的TF-IDF模型提取的关键词与gensim一致，至于篮球中为什么后两个单词不一致，是因为这些单词的tfidf一样，随机选择的结果不同而已。但是有一个问题，那就是计算得到的tfidf值不一样，这是什么原因呢？</p><p>查阅gensim中计算tf-idf值的源代码（https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/tfidfmodel.py）：</p><figure><img src="/img/nlp2_2.webp" alt="TfidfModel类的参数" /><figcaption aria-hidden="true">TfidfModel类的参数</figcaption></figure><figure><img src="/img/nlp2_3.webp" alt="normalize参数的说明" /><figcaption aria-hidden="true">normalize参数的说明</figcaption></figure><p>也就是说，gensim对得到的tf-idf向量做了规范化（normalize），将其转化为单位向量。因此，我们需要在刚才的代码中加入规范化这一步，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 对向量做规范化, normalize</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">unitvec</span>(<span class="hljs-params">sorted_words</span>):<br>    lst = [item[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sorted_words]<br>    L2Norm = math.sqrt(<span class="hljs-built_in">sum</span>(np.array(lst)*np.array(lst)))<br>    unit_vector = [(item[<span class="hljs-number">0</span>], item[<span class="hljs-number">1</span>]/L2Norm) <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sorted_words]<br>    <span class="hljs-keyword">return</span> unit_vector<br><br><span class="hljs-comment"># TF-IDF测试</span><br>count1, count2, count3 = make_count(text1), make_count(text2), make_count(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training by original algorithm......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, count <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(countlist):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    scores = &#123;word: tfidf(word, count, countlist) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> count&#125;<br>    sorted_words = <span class="hljs-built_in">sorted</span>(scores.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    sorted_words = unitvec(sorted_words)   <span class="hljs-comment"># normalize</span><br>    <span class="hljs-keyword">for</span> word, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(word, <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Training</span> by original algorithm......<br><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">1</span><br>    <span class="hljs-attribute">Word</span>: football, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">84766</span><br>    <span class="hljs-attribute">Word</span>: rugby, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">21192</span><br>    <span class="hljs-attribute">Word</span>: known, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">14128</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">2</span><br>    <span class="hljs-attribute">Word</span>: play, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">29872</span><br>    <span class="hljs-attribute">Word</span>: shooting, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">19915</span><br>    <span class="hljs-attribute">Word</span>: diameter, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">19915</span><br><span class="hljs-attribute">Top</span> words in document <span class="hljs-number">3</span><br>    <span class="hljs-attribute">Word</span>: net, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">45775</span><br>    <span class="hljs-attribute">Word</span>: teammate, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">34331</span><br>    <span class="hljs-attribute">Word</span>: back, TF-IDF: <span class="hljs-number">0</span>.<span class="hljs-number">22888</span><br></code></pre></td></tr></table></figure><p>现在的输出结果与gensim得到的结果一致！</p><h3 id="总结">总结</h3><p>Gensim是Python做NLP时鼎鼎大名的模块，有空还是多读读源码吧！以后，我们还会继续介绍TF-IDF在其它方面的应用，欢迎大家交流~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape）， 欢迎大家关注哦~~</p><p>本文的完整代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> nltk<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> string<br><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> stopwords     <span class="hljs-comment">#停用词</span><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter       <span class="hljs-comment">#计数</span><br><span class="hljs-keyword">from</span> gensim <span class="hljs-keyword">import</span> corpora, models, matutils<br><br>text1 =<span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal. </span><br><span class="hljs-string">Unqualified, the word football is understood to refer to whichever form of football is the most popular </span><br><span class="hljs-string">in the regional context in which the word appears. Sports commonly called football in certain places </span><br><span class="hljs-string">include association football (known as soccer in some countries); gridiron football (specifically American </span><br><span class="hljs-string">football or Canadian football); Australian rules football; rugby football (either rugby league or rugby union); </span><br><span class="hljs-string">and Gaelic football. These different variations of football are known as football codes.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>text2 = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Basketball is a team sport in which two teams of five players, opposing one another on a rectangular court, </span><br><span class="hljs-string">compete with the primary objective of shooting a basketball (approximately 9.4 inches (24 cm) in diameter) </span><br><span class="hljs-string">through the defender&#x27;s hoop (a basket 18 inches (46 cm) in diameter mounted 10 feet (3.048 m) high to a backboard </span><br><span class="hljs-string">at each end of the court) while preventing the opposing team from shooting through their own hoop. A field goal is </span><br><span class="hljs-string">worth two points, unless made from behind the three-point line, when it is worth three. After a foul, timed play stops </span><br><span class="hljs-string">and the player fouled or designated to shoot a technical foul is given one or more one-point free throws. The team with </span><br><span class="hljs-string">the most points at the end of the game wins, but if regulation play expires with the score tied, an additional period </span><br><span class="hljs-string">of play (overtime) is mandated.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br>text3 = <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">Volleyball, game played by two teams, usually of six players on a side, in which the players use their hands to bat a </span><br><span class="hljs-string">ball back and forth over a high net, trying to make the ball touch the court within the opponents’ playing area before </span><br><span class="hljs-string">it can be returned. To prevent this a player on the opposing team bats the ball up and toward a teammate before it touches </span><br><span class="hljs-string">the court surface—that teammate may then volley it back across the net or bat it to a third teammate who volleys it across </span><br><span class="hljs-string">the net. A team is allowed only three touches of the ball before it must be returned over the net.</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># 文本预处理</span><br><span class="hljs-comment"># 函数：text文件分句，分词，并去掉标点</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_tokens</span>(<span class="hljs-params">text</span>):<br>    text = text.replace(<span class="hljs-string">&#x27;\n&#x27;</span>, <span class="hljs-string">&#x27;&#x27;</span>)<br>    sents = nltk.sent_tokenize(text)  <span class="hljs-comment"># 分句</span><br>    tokens = []<br>    <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents:<br>        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> nltk.word_tokenize(sent):  <span class="hljs-comment"># 分词</span><br>            <span class="hljs-keyword">if</span> word <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> string.punctuation: <span class="hljs-comment"># 去掉标点</span><br>                tokens.append(word)<br>    <span class="hljs-keyword">return</span> tokens<br><br><span class="hljs-comment"># 对原始的text文件去掉停用词</span><br><span class="hljs-comment"># 生成count字典，即每个单词的出现次数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_count</span>(<span class="hljs-params">text</span>):<br>    tokens = get_tokens(text)<br>    filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> w <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)]    <span class="hljs-comment">#去掉停用词</span><br>    count = Counter(filtered)<br>    <span class="hljs-keyword">return</span> count<br><br><span class="hljs-comment"># 计算tf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tf</span>(<span class="hljs-params">word, count</span>):<br>    <span class="hljs-keyword">return</span> count[word] / <span class="hljs-built_in">sum</span>(count.values())<br><span class="hljs-comment"># 计算count_list有多少个文件包含word</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">n_containing</span>(<span class="hljs-params">word, count_list</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> count_list <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> count)<br><br><span class="hljs-comment"># 计算idf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">idf</span>(<span class="hljs-params">word, count_list</span>):<br>    <span class="hljs-keyword">return</span> math.log2(<span class="hljs-built_in">len</span>(count_list) / (n_containing(word, count_list)))    <span class="hljs-comment">#对数以2为底</span><br><span class="hljs-comment"># 计算tf-idf</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tfidf</span>(<span class="hljs-params">word, count, count_list</span>):<br>    <span class="hljs-keyword">return</span> tf(word, count) * idf(word, count_list)<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 对向量做规范化, normalize</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">unitvec</span>(<span class="hljs-params">sorted_words</span>):<br>    lst = [item[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sorted_words]<br>    L2Norm = math.sqrt(<span class="hljs-built_in">sum</span>(np.array(lst)*np.array(lst)))<br>    unit_vector = [(item[<span class="hljs-number">0</span>], item[<span class="hljs-number">1</span>]/L2Norm) <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> sorted_words]<br>    <span class="hljs-keyword">return</span> unit_vector<br><br><span class="hljs-comment"># TF-IDF测试</span><br>count1, count2, count3 = make_count(text1), make_count(text2), make_count(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Training by original algorithm......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, count <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(countlist):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    scores = &#123;word: tfidf(word, count, countlist) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> count&#125;<br>    sorted_words = <span class="hljs-built_in">sorted</span>(scores.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    sorted_words = unitvec(sorted_words)   <span class="hljs-comment"># normalize</span><br>    <span class="hljs-keyword">for</span> word, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(word, <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br><br><span class="hljs-comment">#training by gensim&#x27;s Ifidf Model</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_words</span>(<span class="hljs-params">text</span>):<br>    tokens = get_tokens(text)<br>    filtered = [w <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> tokens <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> w <span class="hljs-keyword">in</span> stopwords.words(<span class="hljs-string">&#x27;english&#x27;</span>)]<br>    <span class="hljs-keyword">return</span> filtered<br><br><span class="hljs-comment"># get text</span><br>count1, count2, count3 = get_words(text1), get_words(text2), get_words(text3)<br>countlist = [count1, count2, count3]<br><span class="hljs-comment"># training by TfidfModel in gensim</span><br>dictionary = corpora.Dictionary(countlist)<br>new_dict = &#123;v:k <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> dictionary.token2id.items()&#125;<br>corpus2 = [dictionary.doc2bow(count) <span class="hljs-keyword">for</span> count <span class="hljs-keyword">in</span> countlist]<br>tfidf2 = models.TfidfModel(corpus2)<br>corpus_tfidf = tfidf2[corpus2]<br><br><span class="hljs-comment"># output</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nTraining by gensim Tfidf Model.......\n&quot;</span>)<br><span class="hljs-keyword">for</span> i, doc <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(corpus_tfidf):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Top words in document %d&quot;</span>%(i + <span class="hljs-number">1</span>))<br>    sorted_words = <span class="hljs-built_in">sorted</span>(doc, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)    <span class="hljs-comment">#type=list</span><br>    <span class="hljs-keyword">for</span> num, score <span class="hljs-keyword">in</span> sorted_words[:<span class="hljs-number">3</span>]:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;    Word: %s, TF-IDF: %s&quot;</span>%(new_dict[num], <span class="hljs-built_in">round</span>(score, <span class="hljs-number">5</span>)))<br>        <br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">输出结果：</span><br><span class="hljs-string"></span><br><span class="hljs-string">Training by original algorithm......</span><br><span class="hljs-string"></span><br><span class="hljs-string">Top words in document 1</span><br><span class="hljs-string">    Word: football, TF-IDF: 0.84766</span><br><span class="hljs-string">    Word: rugby, TF-IDF: 0.21192</span><br><span class="hljs-string">    Word: word, TF-IDF: 0.14128</span><br><span class="hljs-string">Top words in document 2</span><br><span class="hljs-string">    Word: play, TF-IDF: 0.29872</span><br><span class="hljs-string">    Word: inches, TF-IDF: 0.19915</span><br><span class="hljs-string">    Word: points, TF-IDF: 0.19915</span><br><span class="hljs-string">Top words in document 3</span><br><span class="hljs-string">    Word: net, TF-IDF: 0.45775</span><br><span class="hljs-string">    Word: teammate, TF-IDF: 0.34331</span><br><span class="hljs-string">    Word: bat, TF-IDF: 0.22888</span><br><span class="hljs-string"></span><br><span class="hljs-string">Training by gensim Tfidf Model.......</span><br><span class="hljs-string"></span><br><span class="hljs-string">Top words in document 1</span><br><span class="hljs-string">    Word: football, TF-IDF: 0.84766</span><br><span class="hljs-string">    Word: rugby, TF-IDF: 0.21192</span><br><span class="hljs-string">    Word: known, TF-IDF: 0.14128</span><br><span class="hljs-string">Top words in document 2</span><br><span class="hljs-string">    Word: play, TF-IDF: 0.29872</span><br><span class="hljs-string">    Word: cm, TF-IDF: 0.19915</span><br><span class="hljs-string">    Word: diameter, TF-IDF: 0.19915</span><br><span class="hljs-string">Top words in document 3</span><br><span class="hljs-string">    Word: net, TF-IDF: 0.45775</span><br><span class="hljs-string">    Word: teammate, TF-IDF: 0.34331</span><br><span class="hljs-string">    Word: across, TF-IDF: 0.22888</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>TF-IDF</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>NLP入门（一）词袋模型及句子相似度</title>
    <link href="/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6/"/>
    <url>/2023/07/06/NLP%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%80%EF%BC%89%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6/</url>
    
    <content type="html"><![CDATA[<p>本文作为笔者NLP入门系列文章第一篇，以后我们就要步入NLP时代。</p><p>本文将会介绍NLP中常见的词袋模型（Bag ofWords）以及如何利用词袋模型来计算句子间的相似度（余弦相似度，cosinesimilarity）。</p><p>首先，让我们来看一下，什么是词袋模型。我们以下面两个简单句子为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">sent1 = <span class="hljs-string">&quot;I love sky, I love sea.&quot;</span><br>sent2 = <span class="hljs-string">&quot;I like running, I love reading.&quot;</span><br></code></pre></td></tr></table></figure><p>通常，NLP无法一下子处理完整的段落或句子，因此，第一步往往是分句和分词。这里只有句子，因此我们只需要分词即可。对于英语句子，可以使用NLTK中的word_tokenize函数，对于中文句子，则可使用jieba模块。故第一步为分词，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize<br>sents = [sent1, sent2]<br>texts = [[word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_tokenize(sent)] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents]<br></code></pre></td></tr></table></figure><p>输出的结果如下：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scheme">[[<span class="hljs-symbol">&#x27;I</span>&#x27;, <span class="hljs-symbol">&#x27;love</span>&#x27;, <span class="hljs-symbol">&#x27;sky</span>&#x27;, &#x27;,&#x27;, <span class="hljs-symbol">&#x27;I</span>&#x27;, <span class="hljs-symbol">&#x27;love</span>&#x27;, <span class="hljs-symbol">&#x27;sea</span>&#x27;, <span class="hljs-symbol">&#x27;.</span>&#x27;], [<span class="hljs-symbol">&#x27;I</span>&#x27;, <span class="hljs-symbol">&#x27;like</span>&#x27;, <span class="hljs-symbol">&#x27;running</span>&#x27;, &#x27;,&#x27;, <span class="hljs-symbol">&#x27;I</span>&#x27;, <span class="hljs-symbol">&#x27;love</span>&#x27;, <span class="hljs-symbol">&#x27;reading</span>&#x27;, <span class="hljs-symbol">&#x27;.</span>&#x27;]]<br></code></pre></td></tr></table></figure><p>分词完毕。下一步是构建语料库，即所有句子中出现的单词及标点。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">all_list = []<br><span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:<br>    all_list += text<br>corpus = <span class="hljs-built_in">set</span>(all_list)<br><span class="hljs-built_in">print</span>(corpus)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">&#123;&#x27;love&#x27;, &#x27;running&#x27;, &#x27;reading&#x27;, &#x27;sky&#x27;, &#x27;.&#x27;, &#x27;I&#x27;, &#x27;like&#x27;, &#x27;sea&#x27;, &#x27;,&#x27;&#125;<br></code></pre></td></tr></table></figure><p>可以看到，语料库中一共是8个单词及标点。接下来，对语料库中的单词及标点建立数字映射，便于后续的句子的向量表示。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">corpus_dict = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(corpus, <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(corpus))))<br><span class="hljs-built_in">print</span>(corpus_dict)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">&#123;&#x27;running&#x27;: <span class="hljs-number">1</span>, &#x27;reading&#x27;: <span class="hljs-number">2</span>, &#x27;love&#x27;: <span class="hljs-number">0</span>, &#x27;sky&#x27;: <span class="hljs-number">3</span>, &#x27;.&#x27;: <span class="hljs-number">4</span>, &#x27;I&#x27;: <span class="hljs-number">5</span>, &#x27;like&#x27;: <span class="hljs-number">6</span>, &#x27;sea&#x27;: <span class="hljs-number">7</span>, &#x27;,&#x27;: <span class="hljs-number">8</span>&#125;<br></code></pre></td></tr></table></figure><p>虽然单词及标点并没有按照它们出现的顺序来建立数字映射，不过这并不会影响句子的向量表示及后续的句子间的相似度。</p><p>下一步，也就是词袋模型的关键一步，就是建立句子的向量表示。这个表示向量并不是简单地以单词或标点出现与否来选择0，1数字，而是把单词或标点的出现频数作为其对应的数字表示，结合刚才的语料库字典，句子的向量表示的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 建立句子的向量表示</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">vector_rep</span>(<span class="hljs-params">text, corpus_dict</span>):<br>    vec = []<br>    <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> corpus_dict.keys():<br>        <span class="hljs-keyword">if</span> key <span class="hljs-keyword">in</span> text:<br>            vec.append((corpus_dict[key], text.count(key)))<br>        <span class="hljs-keyword">else</span>:<br>            vec.append((corpus_dict[key], <span class="hljs-number">0</span>))<br><br>    vec = <span class="hljs-built_in">sorted</span>(vec, key= <span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">0</span>])<br><br>    <span class="hljs-keyword">return</span> vec<br><br>vec1 = vector_rep(texts[<span class="hljs-number">0</span>], corpus_dict)<br>vec2 = vector_rep(texts[<span class="hljs-number">1</span>], corpus_dict)<br><span class="hljs-built_in">print</span>(vec1)<br><span class="hljs-built_in">print</span>(vec2)<br></code></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight scheme"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scheme">[(<span class="hljs-name">0</span>, <span class="hljs-number">2</span>), (<span class="hljs-name">1</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">2</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">3</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">4</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">5</span>, <span class="hljs-number">2</span>), (<span class="hljs-name">6</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">7</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">8</span>, <span class="hljs-number">1</span>)]<br>[(<span class="hljs-name">0</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">1</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">2</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">3</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">4</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">5</span>, <span class="hljs-number">2</span>), (<span class="hljs-name">6</span>, <span class="hljs-number">1</span>), (<span class="hljs-name">7</span>, <span class="hljs-number">0</span>), (<span class="hljs-name">8</span>, <span class="hljs-number">1</span>)]<br></code></pre></td></tr></table></figure><p>让我们稍微逗留一会儿，来看看这个向量。在第一句中I出现了两次，在预料库字典中，I对应的数字为5，因此在第一句中5出现2次，在列表中的元组即为(5,2)，代表单词I在第一句中出现了2次。以上的输出可能并不那么直观，真实的两个句子的代表向量应为：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs accesslog"><span class="hljs-string">[2, 0, 0, 1, 1, 2, 0, 1, 1]</span><br><span class="hljs-string">[1, 1, 1, 0, 1, 2, 1, 0, 1]</span><br></code></pre></td></tr></table></figure><p>OK，词袋模型到此结束。接下来，我们会利用刚才得到的词袋模型，即两个句子的向量表示，来计算相似度。</p><p>在NLP中，如果得到了两个句子的向量表示，那么，一般会选择用余弦相似度作为它们的相似度，而向量的余弦相似度即为两个向量的夹角的余弦值。其计算的Python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> sqrt<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">similarity_with_2_sents</span>(<span class="hljs-params">vec1, vec2</span>):<br>    inner_product = <span class="hljs-number">0</span><br>    square_length_vec1 = <span class="hljs-number">0</span><br>    square_length_vec2 = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> tup1, tup2 <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(vec1, vec2):<br>        inner_product += tup1[<span class="hljs-number">1</span>]*tup2[<span class="hljs-number">1</span>]<br>        square_length_vec1 += tup1[<span class="hljs-number">1</span>]**<span class="hljs-number">2</span><br>        square_length_vec2 += tup2[<span class="hljs-number">1</span>]**<span class="hljs-number">2</span><br><br>    <span class="hljs-keyword">return</span> (inner_product/sqrt(square_length_vec1*square_length_vec2))<br><br><br>cosine_sim = similarity_with_2_sents(vec1, vec2)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;两个句子的余弦相似度为： %.4f。&#x27;</span>%cosine_sim)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">两个句子的余弦相似度为： 0.7303。<br></code></pre></td></tr></table></figure><p>这样，我们就通过句子的词袋模型，得到了它们间的句子相似度。</p><p>当然，在实际的NLP项目中，如果需要计算两个句子的相似度，我们只需调用gensim模块即可，它是NLP的利器，能够帮助我们处理很多NLP任务。下面为用gensim计算两个句子的相似度的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">sent1 = <span class="hljs-string">&quot;I love sky, I love sea.&quot;</span><br>sent2 = <span class="hljs-string">&quot;I like running, I love reading.&quot;</span><br><br><span class="hljs-keyword">from</span> nltk <span class="hljs-keyword">import</span> word_tokenize<br>sents = [sent1, sent2]<br>texts = [[word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> word_tokenize(sent)] <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> sents]<br><span class="hljs-built_in">print</span>(texts)<br><br><span class="hljs-keyword">from</span> gensim <span class="hljs-keyword">import</span> corpora<br><span class="hljs-keyword">from</span> gensim.similarities <span class="hljs-keyword">import</span> Similarity<br><br><span class="hljs-comment">#  语料库</span><br>dictionary = corpora.Dictionary(texts)<br><br><span class="hljs-comment"># 利用doc2bow作为词袋模型</span><br>corpus = [dictionary.doc2bow(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts]<br>similarity = Similarity(<span class="hljs-string">&#x27;-Similarity-index&#x27;</span>, corpus, num_features=<span class="hljs-built_in">len</span>(dictionary))<br><span class="hljs-built_in">print</span>(similarity)<br><span class="hljs-comment"># 获取句子的相似度</span><br>new_sensence = sent1<br>test_corpus_1 = dictionary.doc2bow(word_tokenize(new_sensence))<br><br>cosine_sim = similarity[test_corpus_1][<span class="hljs-number">1</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;利用gensim计算得到两个句子的相似度： %.4f。&quot;</span>%cosine_sim)<br></code></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs delphi">[[<span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;sky&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;sea&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>], [<span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;like&#x27;</span>, <span class="hljs-string">&#x27;running&#x27;</span>, <span class="hljs-string">&#x27;,&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;reading&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]]<br>Similarity <span class="hljs-keyword">index</span> <span class="hljs-keyword">with</span> <span class="hljs-number">2</span> documents <span class="hljs-keyword">in</span> <span class="hljs-number">0</span> shards (<span class="hljs-keyword">stored</span> under -Similarity-<span class="hljs-keyword">index</span>)<br>利用gensim计算得到两个句子的相似度： <span class="hljs-number">0.7303</span>。<br></code></pre></td></tr></table></figure><p>注意，如果在运行代码时出现以下warning:</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">gensim\utils.py:<span class="hljs-number">1209</span>: UserWarning: detected Windows; aliasing chunkize <span class="hljs-keyword">to</span> chunkize_serial<br>  warnings.warn(&quot;detected Windows; aliasing chunkize to chunkize_serial&quot;)<br><br>gensim\matutils.py:<span class="hljs-number">737</span>: FutureWarning: <span class="hljs-keyword">Conversion</span> <span class="hljs-keyword">of</span> the second argument <span class="hljs-keyword">of</span> issubdtype <span class="hljs-keyword">from</span> `<span class="hljs-type">int</span>` <span class="hljs-keyword">to</span> `np.signedinteger` <span class="hljs-keyword">is</span> deprecated. <span class="hljs-keyword">In</span> future, it will be treated <span class="hljs-keyword">as</span> `np.int32 == np.dtype(<span class="hljs-type">int</span>).<span class="hljs-keyword">type</span>`.<br>  <span class="hljs-keyword">if</span> np.issubdtype(vec.dtype, np.int):<br></code></pre></td></tr></table></figure><p>如果想要去掉这些warning，则在导入gensim模块的代码前添加以下代码即可：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> warnings<br>warnings.filterwarnings(action=<span class="hljs-string">&#x27;ignore&#x27;</span>,category=UserWarning,module=<span class="hljs-string">&#x27;gensim&#x27;</span>)<br>warnings.filterwarnings(action=<span class="hljs-string">&#x27;ignore&#x27;</span>,category=FutureWarning,module=<span class="hljs-string">&#x27;gensim&#x27;</span>)<br></code></pre></td></tr></table></figure><p>本文到此结束，感谢阅读！如果不当之处，请速联系笔者，欢迎大家交流！祝您好运~</p><p>注意：本人现已开通微信公众号：NLP奇幻之旅（微信号为：easy_web_scrape），欢迎大家关注哦~~</p>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
      <tag>词袋模型</tag>
      
      <tag>句子相似度</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>技术文章写作计划</title>
    <link href="/2023/07/06/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0%E5%86%99%E4%BD%9C%E8%AE%A1%E5%88%92/"/>
    <url>/2023/07/06/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0%E5%86%99%E4%BD%9C%E8%AE%A1%E5%88%92/</url>
    
    <content type="html"><![CDATA[<ul class="task-list"><li><label><input type="checkbox"checked="" />滑动验证码的识别</label></li><li><label><input type="checkbox"checked="" />滑动验证码的获取</label></li><li><label><input type="checkbox" />点选验证码的识别</label></li><li><label><input type="checkbox" />ELK简单搭建的demo</label></li><li><label><input type="checkbox" />文本聚类</label></li><li><label><input type="checkbox" />智能问答</label></li><li><label><input type="checkbox" />车牌的识别</label></li><li><label><input type="checkbox"checked="" />个人足迹地图（WEB服务）</label></li><li><label><input type="checkbox" checked="" />别名发现系统</label></li><li><label><input type="checkbox" />读取doc和docx文档</label></li><li><label><input type="checkbox"checked="" />利用celery实现定时任务</label></li><li><label><input type="checkbox"checked="" />文本标注工具Doccano</label></li><li><label><input type="checkbox"checked="" />利用Conda创建Python虚拟环境</label></li><li><label><input type="checkbox"checked="" />利用SFTP连接Linux服务器并上传、下载文件</label></li><li><label><input type="checkbox" checked="" />Flask学习之RESTfulAPI</label></li><li><label><input type="checkbox" />Flask学习之JWT认证</label></li><li><label><input type="checkbox" checked="" />BSON文件读取</label></li><li><label><inputtype="checkbox" />Flask学习之Flask-SQLALCHEMY</label></li><li><label><input type="checkbox"checked="" />设计模式（完成三篇：单例模式、工厂模式、监听模式）</label></li><li><label><input type="checkbox" />Redis</label></li><li><label><input type="checkbox" />supervisor使用</label></li><li><label><input type="checkbox"checked="" />tornado之文件下载（包含中文文件下载）</label></li><li><label><input type="checkbox"checked="" />利用CRF实现中文分词</label></li><li><label><input type="checkbox"checked="" />利用CRF实现模型预测</label></li><li><label><input type="checkbox"checked="" />protobuf的初次使用</label></li><li><label><inputtype="checkbox" />更新tensorflow/serving中的models.config文件中的model_version_policy</label></li><li><label><input type="checkbox"checked="" />tensorflow同时使用多个session</label></li><li><label><input type="checkbox"checked="" />如何离线安装tensorflow模块</label></li><li><label><input type="checkbox"checked="" />tensorboard查看ckpt和pb文件模型</label></li><li><label><input type="checkbox"checked="" />将ckpt转化为pb文件</label></li><li><label><input type="checkbox"checked="" />tensorflow/serving之BERT模型部署和预测</label></li><li><label><input type="checkbox"checked="" />tensorflow/serving实现模型部署</label></li><li><label><input type="checkbox" /><span class="citation"data-cites="property">@property</span></label></li><li><label><input type="checkbox" />tf_record</label></li><li><label><input type="checkbox" />指代关系抽取</label></li><li><label><inputtype="checkbox" />实体链接（百度实体链接比赛、武器装备知识图谱）</label></li><li><label><input type="checkbox"checked="" />文本多分类BERT微调</label></li><li><label><input type="checkbox"checked="" />文本多标签分类BERT微调</label></li><li><label><input type="checkbox"checked="" />文本序列标注BERT微调</label></li><li><label><input type="checkbox" checked="" />keras-bertEnglish系列（3个模型稍微调整即可）</label></li><li><label><input type="checkbox"checked="" />keras-bert调用ALBERT</label></li><li><label><input type="checkbox"checked="" />keras-bert模型部署</label></li><li><label><input type="checkbox"checked="" />h5文件转化为pb文件进行部署</label></li><li><label><input type="checkbox"checked="" />tensorflow/serving高效调用</label></li><li><label><inputtype="checkbox" />tensorflow_hub实现英文文本二分类</label></li><li><label><inputtype="checkbox" />tensorflow2.0和transformers实现文本多分类</label></li><li><label><input type="checkbox" />抽取式问答</label></li><li><label><input type="checkbox"checked="" />完形填空与文本纠错</label></li><li><label><inputtype="checkbox" />transformers实现中文序列标注</label></li><li><label><inputtype="checkbox" />tokenizers中的token使用方法</label></li><li><label><input type="checkbox" />BPE token 算法</label></li><li><label><input type="checkbox" checked="" />Keras:K折交叉验证</label></li><li><label><input type="checkbox"checked="" />使用Prothemus对tensorflow/serving进行服务监控</label></li><li><label><input type="checkbox"checked="" />seqeval获取序列标注实体识别结果</label></li><li><label><input type="checkbox" />ES进阶</label></li><li><label><input type="checkbox"checked="" />从荷兰国旗问题到快速排序</label></li><li><label><input type="checkbox" />中英文大模型调研</label></li><li><label><input type="checkbox" />LLaMA模型的介绍及其使用</label></li><li><label><input type="checkbox" />Fine-tune LLaMA模型</label></li><li><label><input type="checkbox"checked="" />OpenAI的tokenizer调研</label></li><li><label><input type="checkbox" checked="" />Gitlab CI/CD入门</label></li><li><label><input type="checkbox"checked="" />LangChain使用</label></li><li><label><input type="checkbox" checked="" />Flask部署</label></li><li><label><input type="checkbox" />LangChain构建阅读助手</label></li><li><label><inputtype="checkbox" />使用LoRA训练Flan-T5-XXL模型</label></li><li><label><inputtype="checkbox" />VSCode连接远程服务器进行开发</label></li></ul>]]></content>
    
    
    <categories>
      
      <category>写作计划</category>
      
    </categories>
    
    
    <tags>
      
      <tag>写作计划</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用Hexo+Github搭建个人博客网站</title>
    <link href="/2023/07/06/%E4%BD%BF%E7%94%A8Hexo-Github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/"/>
    <url>/2023/07/06/%E4%BD%BF%E7%94%A8Hexo-Github%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99/</url>
    
    <content type="html"><![CDATA[<p>曾几何时，笔者也幻想过写个项目来搭建属于自己的个人博客。但是，写程序以及维护的成本，不禁让我犹豫再三，最后还是选择了CSDN等博客网站。将近六年的博客生涯，我尝试了不同的博客网站，各有各的利和弊，不变的是广告，这让人很不爽。直到今天，我看到了别人写的利用Hexo+Github来搭建个人博客网站，如获至宝。折腾了一阵以后，轻松完成了个人博客的搭建，这种清爽的界面风格，让人耳目一新，同时它又是免费的，功能繁多的，便于维护的。下面，我将会介绍如何来使用Hexo+Github搭建个人博客网站。</p><h3 id="准备工作">准备工作</h3><p>为了顺利地完成个人博客网站的搭建，需要做以下准备工作：</p><ul><li>安装Git和NodeJs（版本为18.16.1）；</li><li>安装Hexo（命令为<code>npm i -g hexo</code>）;</li><li>Github账号</li></ul><h3 id="搭建博客">搭建博客</h3><p>下面将分步来介绍如何使用Hexo和Github来搭建个人博客网站。</p><h4 id="创建github仓库">创建Github仓库</h4><p>在Github中新建一个名为username.github.io的空仓库，其中username是你在GitHub上的用户名，比如笔者的仓库名为percent.github.io。</p><h4 id="配置ssh">配置SSH</h4><p>如果想要使用远程从你的电脑上传文件至你的github仓库，那么，你就需要配置SSH。点击你个人Github上的Settings选项，在<code>SSH and GPG keys</code>中配置SSH的公钥，一般公钥位于<code>.ssh/id_rsa.pub</code>中，如下图：<img src="/img/hexo1.png" alt="配置SSH" /></p><h4 id="博客初始化">博客初始化</h4><p>新建一个空的文件夹，比如笔者新建了文件夹<code>github_blog</code>，使用<code>hexo init</code>命令初始化博客。初始化后的文件夹结构如下：<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs 1c">.<br>├── _config.yml<br>├── package.json<br>├── scaffolds<br>├── source<br><span class="hljs-string">|   ├── _drafts</span><br><span class="hljs-string">|   └── _posts</span><br>└── themes<br></code></pre></td></tr></table></figure> 上述文件说明如下：</p><ul><li>_config.yml 网站的 配置 信息，您可以在此配置大部分的参数。</li><li>package.json：应用程序的信息。EJS, Stylus 和 Markdown renderer已默认安装，您可以自由移除。</li><li>scaffolds：模版文件夹。当您新建文章时，Hexo会根据 scaffold来建立文件。</li><li>source：资源文件夹是存放用户资源的地方。</li><li>themes：主题文件夹。Hexo 会根据主题来生成静态页面。</li></ul><h4 id="生成个人博客网站">生成个人博客网站</h4><p>配置_config.yml文件，配置信息如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># Deployment</span><br><span class="hljs-comment">## Docs: https://hexo.io/docs/deployment.html</span><br><span class="hljs-attr">deploy:</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">git</span><br>  <span class="hljs-attr">repo:</span> <span class="hljs-string">https://github.com/percent4/percent4.github.io.git(第一步创建的Github仓库)</span><br>  <span class="hljs-attr">branch:</span> <span class="hljs-string">master</span><br></code></pre></td></tr></table></figure><p>安装插件<code>npm install hexo-deployer-git --save</code>后，运行如下命令：<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">hexo</span> clean<span class="hljs-comment"># 清除数据</span><br>hexo d -g<span class="hljs-comment"># 生成博客</span><br></code></pre></td></tr></table></figure>这时候，你会看到博客数据会提交至Github的信息，而第一步创建的空仓库也有了提交内容，当然，你的个人博客也搭建搭建完毕，访问网址为：https://username.github.io/，其中username是你在GitHub上的用户名。界面如下： <imgsrc="/img/hexo2.png" alt="Hexo界面" /></p><h3 id="博客维护">博客维护</h3><p>Hexo提供了一套维护博客的优雅的办法。笔者在此仅介绍如何新建一篇博客。新建博客格式为markdown格式，比如我想创建一篇名为<code>利用Tornado搭建文档预览系统</code>的博客，可以使用以下命令：</p><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs haxe">hexo <span class="hljs-keyword">new</span> <span class="hljs-type"></span>利用Tornado搭建文档预览系统<br></code></pre></td></tr></table></figure><p>这时候会在你当前目录下的source/_posts文件夹下生成<code>利用Tornado搭建文档预览系统.md</code>,其中内容如下：<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-meta">---</span><br><span class="hljs-attr">title:</span> <span class="hljs-string">利用Tornado搭建文档预览系统</span><br><span class="hljs-attr">date:</span> <span class="hljs-number">2020-06-09 18:32:29</span><br><span class="hljs-attr">tags:</span><br><span class="hljs-meta">---</span><br></code></pre></td></tr></table></figure>其中title为博客标题，date为博客时间，tags为博客标签。在<code>---</code>后面可以写博客正文的内容。写完博客后，使用命令 <figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nginx"><span class="hljs-attribute">hexo</span> clean<span class="hljs-comment"># 清除数据</span><br>hexo d -g<span class="hljs-comment"># 生成博客</span><br></code></pre></td></tr></table></figure> 就会更新个人博客。</p><h3 id="更换主题">更换主题</h3><p>Hexo提供的默认主题为landscape,我们想替换主题为Fluid.Hexo替换主题为Fluid的步骤如下：</p><ol type="1"><li>通过<code>npm</code>直接安装，进入博客目录执行命令：<code>npm install --save hexo-theme-fluid</code></li><li>将node_modules文件夹下的hexo-theme-fluid复制到themes文件夹，并重名为fluid</li><li>在博客目录下创建_config.fluid.yml，将主题的_config.yml内容复制进去，并将<code>theme:</code>后面的主题修改为fluid</li><li>使用<code>hexo s</code>进行本地部署，如无问题，则使用命令<code>hexo d -g</code>进行远程部署</li></ol><h3 id="总结">总结</h3><p>当然，Hexo还提供了许多丰富的功能，比如theme（主题）的个性化定制等，这会使得你的博客内容更加丰富，功能更加完善。</p><p>笔者大家的个人博客网站为：<ahref="https://percent4.github.io/">https://percent4.github.io/</a>，欢迎大家访问。以后，笔者将会逐渐往个人博客网站倾斜，而减少使用公开的博客社区。</p>]]></content>
    
    
    <categories>
      
      <category>Hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
      <tag>Github</tag>
      
      <tag>个人博客</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何使用Hexo？</title>
    <link href="/2023/07/06/hello-world/"/>
    <url>/2023/07/06/hello-world/</url>
    
    <content type="html"><![CDATA[<p>欢迎来到 <a href="https://hexo.io/">Hexo</a>!这是我的第一篇博客。查阅 <ahref="https://hexo.io/docs/">documentation</a> 获取更多信息。如果你在使用Hexo遇到问题，你可以在这里找到答案 <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a>，或者你可以在这上面提问：<ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="快速开始">快速开始</h2><h3 id="创建一篇新的博客">创建一篇新的博客</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>更新信息: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="运行服务">运行服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>更新信息: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="产生静态文件">产生静态文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>更新信息: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="远程部署">远程部署</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>更新信息: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
